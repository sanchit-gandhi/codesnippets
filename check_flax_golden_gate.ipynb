{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03f1ec8-3ca3-4edc-9c99-c6bae9177387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAX platform to CPU for highest matmul precision\n",
    "import os\n",
    "# os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d71739-61a0-470d-9b17-2e4a60c21530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlaxGoldenGateForCausalLM, GoldenGateForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import jax\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b4f29-96da-4932-8732-6edab9cfef41",
   "metadata": {},
   "source": [
    "## 1. Load pre-trained PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9738731-61f3-49fc-86e5-0b36ee6249a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.76it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load the model for 'gg-hf/golden-gate-2b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gg-hf/golden-gate-2b' is the correct path to a directory containing a file named flax_model.msgpack or pytorch_model.bin.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/new-model-addition-golden-gate/src/transformers/modeling_flax_utils.py:830\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, dtype, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m     is_sharded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupport for sharded checkpoints using safetensors is coming soon!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Support for sharded checkpoints using safetensors is coming soon!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgg-hf/golden-gate-2b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m pt_model \u001b[38;5;241m=\u001b[39m GoldenGateForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m----> 6\u001b[0m flax_model \u001b[38;5;241m=\u001b[39m \u001b[43mFlaxGoldenGateForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/new-model-addition-golden-gate/src/transformers/modeling_flax_utils.py:856\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, dtype, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the model for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m same name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory containing a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_local:\n\u001b[1;32m    864\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading weights file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load the model for 'gg-hf/golden-gate-2b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gg-hf/golden-gate-2b' is the correct path to a directory containing a file named flax_model.msgpack or pytorch_model.bin."
     ]
    }
   ],
   "source": [
    "model_id = \"gg-hf/golden-gate-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gg-hf/golden-gate-2b\")\n",
    "pt_model = GoldenGateForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8243ce9-c295-4efb-affb-bb5f63ffb04f",
   "metadata": {},
   "source": [
    "## 2. Convert PyTorch weights to Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "092b1f1a-ea41-4db1-a23b-dd42aa43cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at /tmp/tmpspc7r7v9 were not used when initializing FlaxGoldenGateForCausalLM: {('lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxGoldenGateForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxGoldenGateForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    pt_model.save_pretrained(tmpdirname, safe_serialization=False)\n",
    "    flax_model = FlaxGoldenGateForCausalLM.from_pretrained(tmpdirname, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80f18a5b-2b5f-4487-b8c0-ae6302557d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "input_str = [10*\"hello this string is definitely longer\", \"Hey you\"]\n",
    "\n",
    "inputs_pt = tokenizer(input_str, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs_np = tokenizer(input_str, return_tensors=\"np\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c07d646a-ae15-4057-ab23-8f94f40b0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits_pt = pt_model(**inputs_pt).logits\n",
    "    logits_pt_single = pt_model(inputs_pt.input_ids[:1]).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d780c61e-696b-45e4-b47b-f85bff466710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched padded pt vs padded flax\n",
      "68.04801\n",
      "batched full pt vs full flax\n",
      "1.4137669\n",
      "single pt vs flax\n",
      "1.4139137\n",
      "single flax vs flax\n",
      "0.938324\n"
     ]
    }
   ],
   "source": [
    "# default matmul precision (bfloat16)\n",
    "logits_fx = flax_model(**inputs_np).logits\n",
    "logits_fx_single = flax_model(inputs_np.input_ids[:1]).logits\n",
    "\n",
    "print(\"batched padded pt vs padded flax\")\n",
    "print(np.max(np.abs(logits_pt[1, :2].numpy() - np.array(logits_fx[1, :2]))))\n",
    "\n",
    "print(\"batched full pt vs full flax\")\n",
    "print(np.max(np.abs(logits_pt[0].numpy() - np.array(logits_fx[0]))))\n",
    "\n",
    "print(\"single pt vs flax\")\n",
    "print(np.max(np.abs(logits_pt_single[0].numpy() - np.array(logits_fx_single[0]))))\n",
    "\n",
    "print(\"single flax vs flax\")\n",
    "print(np.max(np.abs(np.array(logits_fx[0]) - np.array(logits_fx_single[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b30af3-6948-453d-8810-7982c858d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched padded pt vs padded flax\n",
      "66.512764\n",
      "batched full pt vs full flax\n",
      "0.0005722046\n",
      "single pt vs flax\n",
      "0.00086021423\n",
      "single flax vs flax\n",
      "0.00028800964\n"
     ]
    }
   ],
   "source": [
    "# highest matmul precision (float32)\n",
    "with jax.default_matmul_precision('float32'):\n",
    "    logits_fx = flax_model(**inputs_np).logits\n",
    "    logits_fx_single = flax_model(inputs_np.input_ids[:1]).logits\n",
    "    \n",
    "print(\"batched padded pt vs padded flax\")\n",
    "print(np.max(np.abs(logits_pt[1, :2].numpy() - np.array(logits_fx[1, :2]))))\n",
    "\n",
    "print(\"batched full pt vs full flax\")\n",
    "print(np.max(np.abs(logits_pt[0].numpy() - np.array(logits_fx[0]))))\n",
    "\n",
    "print(\"single pt vs flax\")\n",
    "print(np.max(np.abs(logits_pt_single[0].numpy() - np.array(logits_fx_single[0]))))\n",
    "\n",
    "print(\"single flax vs flax\")\n",
    "print(np.max(np.abs(np.array(logits_fx[0]) - np.array(logits_fx_single[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769a6da-d641-4367-9f65-2e0852c88c2b",
   "metadata": {},
   "source": [
    "## JIT the fprop and watch the magic happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80292b8d-734a-4d17-8a6c-3e3a3fd18aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def flax_model_jitted(input_ids, attention_mask=None, **kwargs):\n",
    "    return flax_model(input_ids, attention_mask=attention_mask, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb44ed-08b1-45c0-b52a-5975465da054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2237095936 bytes == 0x29d86a000 @  0x7fbbe9d99680 0x7fbbe9dba824 0x58f8b8 0x586650 0x5869d4 0x619464 0x6194c2 0x6213b0 0x62177a 0x5c47d0 0x5f6517 0x7fba62077bf1 0x7fba618dab27 0x7fba61ab56a6 0x7fba619d1c46 0x7fba619d1f77 0x7fba619d833d 0x7fba619d8e80 0x7fba61a500b0 0x7fba619d9197 0x7fba619d9a5b 0x7fba619da21b 0x7fba61a3b5e6 0x7fba618ebfab 0x7fba618ec2a6 0x7fba619d9197 0x7fba619d9a5b 0x7fba619da21b 0x7fba61a1096d 0x7fba61a109c6 0x7fba619d9197\n",
      "tcmalloc: large alloc 2237431808 bytes == 0x1e5272000 @  0x7fbbe9d99680 0x7fbbe9dba824 0x58f8b8 0x586650 0x5869d4 0x619464 0x6195b6 0x6217b3 0x5042cb 0x56b1da 0x5f6836 0x570035 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836 0x56b1da 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x5f3547\n",
      "tcmalloc: large alloc 2237431808 bytes == 0x29d86a000 @  0x7fbbe9d99680 0x7fbbe9db9ff4 0x7fba794cd1de 0x7fba794cf979 0x7fba79505533 0x7fba794e4991 0x5f3989 0x5f3e1e 0x50b183 0x56c28c 0x5f6836 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x56b0ae 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836 0x56b1da 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x5f3547\n",
      "tcmalloc: large alloc 2751438848 bytes == 0x594452000 @  0x7fbbe9d99680 0x7fbbe9db9ff4 0x7fba7d2b78ca 0x7fba7c588cb7 0x7fba7c57ee17 0x7fba7c578249 0x7fba7b6bf611 0x7fba7b6cdad0 0x7fba7971feaa 0x7fba79504f56 0x7fba79505597 0x7fba794e4991 0x5f3989 0x5f3e1e 0x50b183 0x56c28c 0x5f6836 0x5f3547 0x56c8cd 0x56939a 0x5f6a13 0x56b0ae 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x5f3547 0x56c8cd 0x5f6836 0x56b1da 0x56939a\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/message_lite.cc:484] xla.HloModuleProto exceeded maximum protobuf size of 2GB: 2751431778\n",
      "2022-07-06 13:25:19.876073: F external/org_tensorflow/tensorflow/stream_executor/tpu/proto_helper.h:44] Check failed: proto.SerializeToArray(bytes, size) \n",
      "https://symbolize.stripped_domain/r/?trace=7fbbe9bca03b,7fbbe9bca0bf,7fba793a41ec,7fba793a45cf,7fba7c588cb6,7fba7c57ee16,7fba7c578248,7fba7b6bf610,7fba7b6cdacf,7fba7971fea9,7fba79504f55,7fba79505596,7fba794e4990,5f3988,903aff&map= \n",
      "*** SIGABRT received by PID 656516 (TID 656516) on cpu 53 from PID 656516; stack trace: ***\n",
      "PC: @     0x7fbbe9bca03b  (unknown)  raise\n",
      "    @     0x7fba78125c73        992  (unknown)\n",
      "    @     0x7fbbe9bca0c0  (unknown)  (unknown)\n",
      "    @     0x7fba793a41ed        432  stream_executor::tpu::SerializeProto<>()\n",
      "    @     0x7fba793a45d0       3440  xla::(anonymous namespace)::TpuCompiler::RunHloPasses()\n",
      "    @     0x7fba7c588cb7        608  xla::Service::BuildExecutable()\n",
      "    @     0x7fba7c57ee17       1152  xla::LocalService::CompileExecutables()\n",
      "    @     0x7fba7c578249       2672  xla::LocalClient::Compile()\n",
      "    @     0x7fba7b6bf611        896  xla::PjRtStreamExecutorClient::Compile()\n",
      "    @     0x7fba7b6cdad0       1216  xla::PjRtStreamExecutorClient::Compile()\n",
      "    @     0x7fba7971feaa       1168  xla::PyClient::CompileMlir()\n",
      "    @     0x7fba79504f56       1888  pybind11::detail::argument_loader<>::call_impl<>()\n",
      "    @     0x7fba79505597        208  pybind11::cpp_function::initialize<>()::{lambda()#3}::_FUN()\n",
      "    @     0x7fba794e4991        768  pybind11::cpp_function::dispatcher()\n",
      "    @           0x5f3989  (unknown)  PyCFunction_Call\n",
      "    @           0x903b00  (unknown)  (unknown)\n",
      "https://symbolize.stripped_domain/r/?trace=7fbbe9bca03b,7fba78125c72,7fbbe9bca0bf,7fba793a41ec,7fba793a45cf,7fba7c588cb6,7fba7c57ee16,7fba7c578248,7fba7b6bf610,7fba7b6cdacf,7fba7971fea9,7fba79504f55,7fba79505596,7fba794e4990,5f3988,903aff&map=abc33f1bfca16f4e7d925d4248b4beb3:7fba639f7000-7fba784a6b70 \n",
      "E0706 13:25:19.962431  656516 coredump_hook.cc:366] RAW: Remote crash data gathering hook invoked.\n",
      "E0706 13:25:19.962446  656516 coredump_hook.cc:412] RAW: Skipping coredump since rlimit was 0 at process start.\n",
      "E0706 13:25:19.962453  656516 client.cc:234] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.\n",
      "E0706 13:25:19.962456  656516 coredump_hook.cc:473] RAW: Sending fingerprint to remote end.\n",
      "E0706 13:25:19.962463  656516 coredump_socket.cc:118] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket\n",
      "E0706 13:25:19.962470  656516 coredump_hook.cc:477] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?\n",
      "E0706 13:25:19.962474  656516 coredump_hook.cc:551] RAW: Discarding core.\n"
     ]
    }
   ],
   "source": [
    "# microbench jit compile time for batch\n",
    "%time logits_fx = flax_model_jitted(**inputs_np).logits.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45ce7a-625b-47a8-acf5-a678170020eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# microbench compiled fprop -> should be ~ms, if on the order of seconds inidicates a recompilation\n",
    "%time logits_fx = flax_model_jitted(**inputs_np).logits.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd67dc-868c-42f9-88b9-814e3b06cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# microbench jit compile time for single input\n",
    "%time logits_fx_single = flax_model_jitted(inputs_np.input_ids[:1]).logits.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d244938-3d66-4a5f-8bfb-e6ed6328fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# microbench compiled fprop for single input -> should be ~ms, if on the order of seconds inidicates a recompilation\n",
    "%time logits_fx_single = flax_model_jitted(inputs_np.input_ids[:1]).logits.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57134e64-16c4-4539-a4cd-5b18b86a70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify correctness of jit-compiled fprop\n",
    "print(\"batched padded pt vs padded flax\")\n",
    "print(np.max(np.abs(logits_pt[1, :2].numpy() - np.array(logits_fx[1, :2]))))\n",
    "\n",
    "print(\"batched full pt vs full flax\")\n",
    "print(np.max(np.abs(logits_pt[0].numpy() - np.array(logits_fx[0]))))\n",
    "\n",
    "print(\"single pt vs flax\")\n",
    "print(np.max(np.abs(logits_pt_single[0].numpy() - np.array(logits_fx_single[0]))))\n",
    "\n",
    "print(\"single flax vs flax\")\n",
    "print(np.max(np.abs(np.array(logits_fx[0]) - np.array(logits_fx_single[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ccb1b-4ed0-4cd9-abbb-0040c2e2cf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
