{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0fa350-9797-4f52-a46d-d5624917d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/hf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import jax_utils\n",
    "from flax.training.common_utils import shard\n",
    "\n",
    "from transformers import FlaxGemmaForCausalLM, GemmaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2fba06-0a31-4519-8f41-a0ac07492ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-7b-it\"\n",
    "max_input_tokens = 1024\n",
    "max_new_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56988603-c4be-4090-8dda-ede4b463c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 4894842880 bytes == 0x11df08000 @  0x7fb8842d3680 0x7fb8842f4824 0x5d93d1 0x634ae1 0x5a23da 0x4c8bce 0x63afe8 0x4db8d3 0x547447 0x4e1a5e 0x54c8a9 0x54552a 0x4e1bd0 0x5483b6 0x54552a 0x684327 0x5e1514 0x5a27d0 0x547265 0x4d71f8 0x548c6b 0x4d71f8 0x548c6b 0x4d71f8 0x4daf8a 0x547447 0x5d5846 0x547265 0x5d5846 0x547447 0x54552a\n",
      "tcmalloc: large alloc 1572864000 bytes == 0x369b44000 @  0x7fb8842d3680 0x7fb8842f4824 0x7fb8728c1994 0x7fb8728c212f 0x7fb8729208f5 0x7fb8729c4329 0x7fb8729c4a77 0x7fb8729c4bcc 0x6af68d 0x7fb872909854 0x5d553a 0x5d6066 0x54ca58 0x54552a 0x5d5a23 0x54c8a9 0x5d5846 0x547265 0x5d5846 0x547265 0x5d5846 0x547265 0x5d5846 0x547265 0x5d5846 0x547265 0x5d5846 0x547265 0x4e1a5e 0x54c8a9 0x54552a\n",
      "tcmalloc: large alloc 4982947840 bytes == 0x3c7744000 @  0x7fb8842d3680 0x7fb8842f4824 0x5d93d1 0x634ae1 0x5a23da 0x4c8bce 0x63afe8 0x4db8d3 0x547447 0x4e1a5e 0x54c8a9 0x54552a 0x4e1bd0 0x5483b6 0x54552a 0x684327 0x5e1514 0x5a27d0 0x547265 0x4d71f8 0x548c6b 0x4d71f8 0x548c6b 0x4d71f8 0x4daf8a 0x547447 0x5d5846 0x547265 0x5d5846 0x547447 0x54552a\n",
      "tcmalloc: large alloc 4982947840 bytes == 0x3c7744000 @  0x7fb8842d3680 0x7fb8842f4824 0x5d93d1 0x634ae1 0x5a23da 0x4c8bce 0x63afe8 0x4db8d3 0x547447 0x4e1a5e 0x54c8a9 0x54552a 0x4e1bd0 0x5483b6 0x54552a 0x684327 0x5e1514 0x5a27d0 0x547265 0x4d71f8 0x548c6b 0x4d71f8 0x548c6b 0x4d71f8 0x4daf8a 0x547447 0x5d5846 0x547265 0x5d5846 0x547447 0x54552a\n",
      "tcmalloc: large alloc 2214649856 bytes == 0x3c7744000 @  0x7fb8842d3680 0x7fb8842f4824 0x5d93d1 0x634ae1 0x5a23da 0x4c8bce 0x63afe8 0x4db8d3 0x547447 0x4e1a5e 0x54c8a9 0x54552a 0x4e1bd0 0x5483b6 0x54552a 0x684327 0x5e1514 0x5a27d0 0x547265 0x4d71f8 0x548c6b 0x4d71f8 0x548c6b 0x4d71f8 0x4daf8a 0x547447 0x5d5846 0x547265 0x5d5846 0x547447 0x54552a\n",
      "Some of the weights of FlaxGemmaForCausalLM were initialized in bfloat16 precision from the model checkpoint at google/gemma-7b-it:\n",
      "[('model', 'embed_tokens', 'embedding'), ('model', 'layers', '0', 'input_layernorm', 'weight'), ('model', 'layers', '0', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '0', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '0', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '0', 'post_attention_layernorm', 'weight'), ('model', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '1', 'input_layernorm', 'weight'), ('model', 'layers', '1', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '1', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '1', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '1', 'post_attention_layernorm', 'weight'), ('model', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '10', 'input_layernorm', 'weight'), ('model', 'layers', '10', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '10', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '10', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '10', 'post_attention_layernorm', 'weight'), ('model', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '11', 'input_layernorm', 'weight'), ('model', 'layers', '11', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '11', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '11', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '11', 'post_attention_layernorm', 'weight'), ('model', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '12', 'input_layernorm', 'weight'), ('model', 'layers', '12', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '12', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '12', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '12', 'post_attention_layernorm', 'weight'), ('model', 'layers', '12', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '13', 'input_layernorm', 'weight'), ('model', 'layers', '13', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '13', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '13', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '13', 'post_attention_layernorm', 'weight'), ('model', 'layers', '13', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '14', 'input_layernorm', 'weight'), ('model', 'layers', '14', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '14', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '14', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '14', 'post_attention_layernorm', 'weight'), ('model', 'layers', '14', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '15', 'input_layernorm', 'weight'), ('model', 'layers', '15', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '15', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '15', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '15', 'post_attention_layernorm', 'weight'), ('model', 'layers', '15', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '16', 'input_layernorm', 'weight'), ('model', 'layers', '16', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '16', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '16', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '16', 'post_attention_layernorm', 'weight'), ('model', 'layers', '16', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '17', 'input_layernorm', 'weight'), ('model', 'layers', '17', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '17', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '17', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '17', 'post_attention_layernorm', 'weight'), ('model', 'layers', '17', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '18', 'input_layernorm', 'weight'), ('model', 'layers', '18', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '18', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '18', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '18', 'post_attention_layernorm', 'weight'), ('model', 'layers', '18', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '18', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '18', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '18', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '19', 'input_layernorm', 'weight'), ('model', 'layers', '19', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '19', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '19', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '19', 'post_attention_layernorm', 'weight'), ('model', 'layers', '19', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '19', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '19', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '19', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '2', 'input_layernorm', 'weight'), ('model', 'layers', '2', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '2', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '2', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '2', 'post_attention_layernorm', 'weight'), ('model', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '20', 'input_layernorm', 'weight'), ('model', 'layers', '20', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '20', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '20', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '20', 'post_attention_layernorm', 'weight'), ('model', 'layers', '20', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '20', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '20', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '20', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '21', 'input_layernorm', 'weight'), ('model', 'layers', '21', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '21', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '21', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '21', 'post_attention_layernorm', 'weight'), ('model', 'layers', '21', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '21', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '21', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '21', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '22', 'input_layernorm', 'weight'), ('model', 'layers', '22', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '22', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '22', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '22', 'post_attention_layernorm', 'weight'), ('model', 'layers', '22', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '22', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '22', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '22', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '23', 'input_layernorm', 'weight'), ('model', 'layers', '23', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '23', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '23', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '23', 'post_attention_layernorm', 'weight'), ('model', 'layers', '23', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '23', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '23', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '23', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '24', 'input_layernorm', 'weight'), ('model', 'layers', '24', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '24', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '24', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '24', 'post_attention_layernorm', 'weight'), ('model', 'layers', '24', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '24', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '24', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '24', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '25', 'input_layernorm', 'weight'), ('model', 'layers', '25', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '25', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '25', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '25', 'post_attention_layernorm', 'weight'), ('model', 'layers', '25', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '25', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '25', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '25', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '26', 'input_layernorm', 'weight'), ('model', 'layers', '26', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '26', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '26', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '26', 'post_attention_layernorm', 'weight'), ('model', 'layers', '26', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '26', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '26', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '26', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '27', 'input_layernorm', 'weight'), ('model', 'layers', '27', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '27', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '27', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '27', 'post_attention_layernorm', 'weight'), ('model', 'layers', '27', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '27', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '27', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '27', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '3', 'input_layernorm', 'weight'), ('model', 'layers', '3', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '3', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '3', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '3', 'post_attention_layernorm', 'weight'), ('model', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '4', 'input_layernorm', 'weight'), ('model', 'layers', '4', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '4', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '4', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '4', 'post_attention_layernorm', 'weight'), ('model', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '5', 'input_layernorm', 'weight'), ('model', 'layers', '5', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '5', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '5', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '5', 'post_attention_layernorm', 'weight'), ('model', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '6', 'input_layernorm', 'weight'), ('model', 'layers', '6', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '6', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '6', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '6', 'post_attention_layernorm', 'weight'), ('model', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '7', 'input_layernorm', 'weight'), ('model', 'layers', '7', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '7', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '7', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '7', 'post_attention_layernorm', 'weight'), ('model', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '8', 'input_layernorm', 'weight'), ('model', 'layers', '8', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '8', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '8', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '8', 'post_attention_layernorm', 'weight'), ('model', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '9', 'input_layernorm', 'weight'), ('model', 'layers', '9', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '9', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '9', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '9', 'post_attention_layernorm', 'weight'), ('model', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('model', 'norm', 'weight')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model, params = FlaxGemmaForCausalLM.from_pretrained(model_id, revision=\"flax\", _do_init=False, dtype=jnp.bfloat16)\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e2fc3f-dcf1-4b20-9bff-4fb423f70ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 4 * [\"The capital of France is\"]\n",
    "input_ids = tokenizer(input_text, return_tensors=\"np\", padding=\"max_length\", max_length=max_input_tokens).input_ids\n",
    "\n",
    "params = jax_utils.replicate(params)\n",
    "input_ids = shard(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42446a96-eef7-4a74-aea4-15d2be0b6591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/hf/lib/python3.8/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=bfloat16 to dtype=float32. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "/home/sanchitgandhi/hf/lib/python3.8/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=bfloat16 to dtype=float32. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n"
     ]
    }
   ],
   "source": [
    "def generate(input_ids, params, max_new_tokens):\n",
    "    generated_ids = model.generate(input_ids, params=params, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "    return generated_ids.sequences\n",
    "\n",
    "p_generate = jax.pmap(generate, \"input_ids\", in_axes=(0, 0, None,), out_axes=0, static_broadcasted_argnums=(2,))\n",
    "_ = p_generate(input_ids, params, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b7ad674-6b4e-460b-9cc5-9d13231f4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tok_per_s(input_ids, generated_ids, runtime):\n",
    "    total_inputs = np.prod(input_ids.shape)\n",
    "    total_outputs = np.prod(generated_ids.shape)\n",
    "    tokens_generated = total_outputs - total_inputs\n",
    "    tokens_per_s = tokens_generated / runtime\n",
    "    return tokens_per_s\n",
    "\n",
    "def chat_function(message, chat_history):\n",
    "    conversation = [{\"role\": \"assistant\", \"content\": \"You are a helpful assistant. Reply to the user in full paragraphs and be polite.\"}]\n",
    "    conversation = []\n",
    "    for user, assistant in chat_history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"np\")\n",
    "    # in-case our inputs exceed the maximum length, we might need to cut them\n",
    "    if input_ids.shape[1] > max_input_tokens:\n",
    "        input_ids = input_ids[:, -max_input_tokens:]\n",
    "        gr.Warning(f\"Trimmed input from conversation as it was longer than {max_input_tokens} tokens.\")\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"np\", padding=\"max_length\", max_length=max_input_tokens)\n",
    "    input_ids = np.vstack(4 * [input_ids])\n",
    "    input_ids = shard(input_ids)\n",
    "\n",
    "    start = time.time()\n",
    "    pred_ids = p_generate(input_ids, params, max_new_tokens)\n",
    "    runtime = time.time() - start\n",
    "    \n",
    "    pred_ids = jax.device_get(pred_ids.reshape(-1, pred_ids.shape[-1]))\n",
    "    pred_text = tokenizer.decode(np.array(pred_ids[0])[input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    tok_per_s = compute_tok_per_s(input_ids, pred_ids, runtime)\n",
    "    gr.Info(f\"Tok/s: {round(tok_per_s, 2)}\")\n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b84ad1d6-4200-45d8-b861-a838f6dbf35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://cde34eac7b5cc9a554.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cde34eac7b5cc9a554.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_interface = gr.ChatInterface(chat_function)\n",
    "chat_interface.queue().launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6727f13-ac11-48fa-8817-339821af8bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
