{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa376bc-93a0-4537-88a3-882d82948b7e",
   "metadata": {},
   "source": [
    "### 1. Set the JAX platform (CPU/TPU) and matmul precision (if on TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f906ec-50c5-4bf4-9c56-e9353d753b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "#os.environ[\"JAX_DEFAULT_MATMUL_PRECISION\"]=\"float32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee887206-b2d3-4888-acd2-cf7c36820aef",
   "metadata": {},
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42600b40-8ed3-425f-a4ce-2bb8d45b2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoConfig, AutoModelForSpeechSeq2Seq, FlaxAutoModelForSpeechSeq2Seq, AutoFeatureExtractor, AutoTokenizer, AutoProcessor, FlaxSpeechEncoderDecoderModel, SpeechEncoderDecoderModel\n",
    "import flax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from numpy.random import default_rng\n",
    "import tempfile\n",
    "from flax.traverse_util import flatten_dict\n",
    "import torch\n",
    "from flax.training.common_utils import onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078854b-c20a-4a31-85db-25a315825cb1",
   "metadata": {},
   "source": [
    "### 3. Set model,training and data args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd4be40-8f1d-4da5-85ce-3cd1f82866ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model args\n",
    "encoder_id = \"hf-internal-testing/tiny-random-wav2vec2\"\n",
    "decoder_id = \"hf-internal-testing/tiny-random-bart\"\n",
    "\n",
    "encoder_id = \"facebook/wav2vec2-large-lv60\"\n",
    "decoder_id = \"facebook/bart-large\"\n",
    "\n",
    "# training args\n",
    "batch_size_per_update = 2\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# data args\n",
    "dataset_name = \"librispeech_asr\"\n",
    "dataset_config_name = \"clean\"\n",
    "train_split_name = \"train.100[:5%]\"\n",
    "eval_split_name = \"validation[:5%]\"\n",
    "dataset_cache_dir = \"/home/sanchitgandhi/cache/huggingface/datasets\"\n",
    "audio_column_name = \"audio\"\n",
    "text_column_name = \"text\"\n",
    "do_lower_case = True\n",
    "\n",
    "max_duration_in_seconds = 5\n",
    "min_duration_in_seconds = 0\n",
    "max_target_length = 32\n",
    "min_target_length = 0\n",
    "pad_input_to_multiple_of = 32000\n",
    "pad_target_to_multiple_of = None\n",
    "max_train_samples = max_eval_samples = None\n",
    "preprocessing_num_workers = num_workers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b603b3-0d21-4923-8eac-5faa9c0338a0",
   "metadata": {},
   "source": [
    "### 4. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2c1bc3-1393-4105-92b9-9b84a5bab4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "raw_datasets[\"train\"] = load_dataset(\n",
    "            dataset_name,\n",
    "            dataset_config_name,\n",
    "            split=train_split_name,\n",
    "            cache_dir=dataset_cache_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505badb8-229e-498f-ab45-537e47e3119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets[\"eval\"] = load_dataset(\n",
    "            dataset_name,\n",
    "            dataset_config_name,\n",
    "            split=eval_split_name,\n",
    "            cache_dir=dataset_cache_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bfd05-c645-4871-b881-9c80ec47e83b",
   "metadata": {},
   "source": [
    "### 5. Load pretrained model, tokenizer, and feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61719803-6c0a-4001-b46e-96633e201fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-lv60 were not used when initializing FlaxWav2Vec2Model: {('project_hid', 'bias'), ('project_q', 'kernel'), ('quantizer', 'codevectors'), ('quantizer', 'weight_proj', 'kernel'), ('project_q', 'bias'), ('project_hid', 'kernel'), ('quantizer', 'weight_proj', 'bias')}\n",
      "- This IS expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large were not used when initializing FlaxBartForCausalLM: {('encoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '10', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '4', 'fc2', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '8', 'fc2', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '7', 'fc1', 'kernel'), ('encoder', 'layers', '11', 'fc1', 'kernel'), ('encoder', 'layers', '9', 'fc1', 'bias'), ('encoder', 'layers', '6', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '7', 'fc1', 'bias'), ('encoder', 'layers', '11', 'fc1', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '2', 'fc1', 'bias'), ('encoder', 'layers', '7', 'fc2', 'bias'), ('encoder', 'layers', '11', 'fc2', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '3', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '11', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'fc1', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '11', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '1', 'fc2', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '5', 'fc2', 'kernel'), ('encoder', 'layers', '9', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '8', 'fc1', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '5', 'fc2', 'bias'), ('encoder', 'layers', '9', 'final_layer_norm', 'bias'), ('encoder', 'layers', '2', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '10', 'final_layer_norm', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'final_layer_norm', 'bias'), ('encoder', 'layers', '8', 'fc2', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '5', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '5', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '6', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '6', 'fc2', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '3', 'final_layer_norm', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '4', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '3', 'fc2', 'kernel'), ('encoder', 'layers', '1', 'final_layer_norm', 'bias'), ('encoder', 'layers', '1', 'fc1', 'kernel'), ('encoder', 'layers', '4', 'final_layer_norm', 'bias'), ('encoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '5', 'fc1', 'kernel'), ('encoder', 'layers', '11', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '1', 'fc2', 'bias'), ('encoder', 'layers', '5', 'fc1', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '7', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '9', 'fc2', 'kernel'), ('encoder', 'layers', '7', 'final_layer_norm', 'bias'), ('encoder', 'layers', '8', 'fc1', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '10', 'fc2', 'kernel'), ('encoder', 'layers', '2', 'fc2', 'kernel'), ('encoder', 'embed_positions', 'kernel'), ('encoder', 'layers', '3', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '10', 'fc2', 'bias'), ('encoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('shared', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '6', 'fc1', 'kernel'), ('encoder', 'layers', '0', 'fc2', 'kernel'), ('encoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '3', 'fc1', 'kernel'), ('encoder', 'layers', '5', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '10', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '6', 'final_layer_norm', 'bias'), ('encoder', 'layers', '6', 'fc2', 'bias'), ('encoder', 'layers', '2', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '3', 'fc2', 'bias'), ('encoder', 'layers', '10', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '4', 'fc2', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '1', 'fc1', 'bias'), ('encoder', 'layers', '8', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'layers', '8', 'final_layer_norm', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '11', 'final_layer_norm', 'bias'), ('encoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '9', 'fc1', 'kernel'), ('encoder', 'layers', '10', 'fc1', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '2', 'fc1', 'kernel'), ('encoder', 'layers', '7', 'fc2', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '10', 'fc1', 'bias'), ('encoder', 'layers', '11', 'fc2', 'kernel'), ('encoder', 'layers', '9', 'fc2', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '2', 'fc2', 'bias'), ('encoder', 'layers', '3', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '4', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '0', 'final_layer_norm', 'kernel'), ('encoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '4', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '0', 'final_layer_norm', 'bias'), ('encoder', 'layers', '9', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '0', 'fc1', 'kernel'), ('encoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layers', '9', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('encoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('encoder', 'embed_tokens', 'kernel'), ('encoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('encoder', 'layernorm_embedding', 'kernel'), ('encoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('encoder', 'layers', '6', 'fc1', 'bias'), ('encoder', 'layers', '0', 'fc2', 'bias'), ('encoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('encoder', 'layernorm_embedding', 'bias'), ('encoder', 'layers', '7', 'self_attn_layer_norm', 'kernel'), ('encoder', 'layers', '3', 'fc1', 'bias'), ('encoder', 'layers', '4', 'fc1', 'kernel'), ('encoder', 'layers', '7', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '5', 'final_layer_norm', 'bias'), ('encoder', 'layers', '4', 'fc1', 'bias'), ('encoder', 'layers', '2', 'self_attn_layer_norm', 'bias'), ('encoder', 'layers', '11', 'self_attn', 'q_proj', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-04-05 09:41:20.982544: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-lv60 were not used when initializing Wav2Vec2Model: ['quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_q.weight', 'quantizer.codevectors']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large were not used when initializing BartForCausalLM: ['encoder.layers.10.fc2.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.7.fc1.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.3.fc1.bias', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.11.fc1.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.11.fc1.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.6.fc2.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.8.fc1.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.10.fc1.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.7.fc2.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.2.fc2.weight', 'encoder.embed_tokens.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.4.fc2.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.6.fc2.weight', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.9.fc1.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.6.fc1.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.9.fc2.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.11.fc2.weight', 'encoder.layers.5.fc2.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.5.fc2.bias', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.6.fc1.weight', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.0.fc2.weight', 'encoder.layers.9.fc1.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.10.fc2.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.8.fc1.weight', 'encoder.layers.8.fc2.weight', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layernorm_embedding.weight', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.9.fc2.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.1.fc2.bias', 'encoder.layers.9.self_attn_layer_norm.weight', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'shared.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.11.fc2.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.7.fc1.weight', 'encoder.layers.10.fc1.bias', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.2.fc1.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.5.fc1.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.7.fc2.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "fx_model = FlaxSpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_from_pt=True, decoder_from_pt=True)\n",
    "\n",
    "fx_model.config.decoder_start_token_id = fx_model.config.decoder.bos_token_id\n",
    "fx_model.config.pad_token_id = fx_model.config.decoder.pad_token_id\n",
    "fx_model.config.eos_token_id = fx_model.config.decoder.eos_token_id\n",
    "fx_model.config.processor_class = \"Wav2Vec2Processor\"\n",
    "\n",
    "pt_model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n",
    "pt_model.config.decoder_start_token_id = pt_model.config.decoder.bos_token_id\n",
    "pt_model.config.pad_token_id = pt_model.config.decoder.pad_token_id\n",
    "pt_model.config.eos_token_id = pt_model.config.decoder.eos_token_id\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n",
    "processor = AutoProcessor.from_pretrained(encoder_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n",
    "\n",
    "if fx_model.config.decoder_start_token_id or pt_model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14cd33a-7446-47f5-8864-799165f3bec3",
   "metadata": {},
   "source": [
    "### 6. Check that the PT and FX weights are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02e0d09-6d9e-4b25-985d-a4dc4ee87a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /tmp/tmpkaz8fvgn were not used when initializing FlaxSpeechEncoderDecoderModel: {('decoder', 'lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Convert the PT model to FX to enable comparison of param dicts (PT state dict -> FX param dict)\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    pt_model.save_pretrained(tmpdirname)\n",
    "    pt_model_to_fx = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(tmpdirname, from_pt=True)\n",
    "    \n",
    "# It's easier to view the PyTree param dict when flattened\n",
    "fx_params = flatten_dict(fx_model.params)\n",
    "pt_params_to_fx = flatten_dict(pt_model_to_fx.params)\n",
    "\n",
    "# Check that the keys match\n",
    "assert fx_params.keys() == pt_params_to_fx.keys()\n",
    "\n",
    "# Check that the parameters are precisely equal\n",
    "for param in fx_params:\n",
    "    assert (fx_params[param] == pt_params_to_fx[param]).all(), f\"{param} weights are not equal between Flax and PyTorch\"\n",
    "    \n",
    "# Free CPU memory\n",
    "del fx_params, pt_params_to_fx, pt_model_to_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab52e9c-fc1a-475d-a79b-3fdac410a053",
   "metadata": {},
   "source": [
    "### 7. Resample speech dataset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35edc92d-7124-4fea-b658-5ecfba1f4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Torch audio in this resampling step for convinience\n",
    "dataset_sampling_rate = next(iter(raw_datasets.values())).features[audio_column_name].sampling_rate\n",
    "if dataset_sampling_rate != feature_extractor.sampling_rate:\n",
    "    raw_datasets = raw_datasets.cast_column(\n",
    "        audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e50f5-b37e-42e7-b34c-b29614686c69",
   "metadata": {},
   "source": [
    "### 8. Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460a0da7-fb93-43e5-8d86-7797ac6839e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "max_input_length = int(max_duration_in_seconds * feature_extractor.sampling_rate)\n",
    "min_input_length = int(min_duration_in_seconds * feature_extractor.sampling_rate)\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49af1f99-a8e7-4615-9779-dd176cd5bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate data to max_samples\n",
    "if max_train_samples is not None:\n",
    "        raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(max_train_samples))\n",
    "\n",
    "if max_eval_samples is not None:\n",
    "    raw_datasets[\"eval\"] = raw_datasets[\"eval\"].select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a8563a-70b2-45cb-91b6-69c9d78c6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # process audio\n",
    "    sample = batch[audio_column_name]\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    # process audio length\n",
    "    batch[model_input_name] = inputs.input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    # process targets\n",
    "    input_str = batch[text_column_name].lower() if do_lower_case else batch[text_column_name]\n",
    "    batch[\"labels\"] = tokenizer(input_str).input_ids\n",
    "    batch[\"labels_length\"] = len(batch[\"labels\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df4d09b-b6a8-4cdc-83d9-accb2108ad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-db5d2c9f36ccc8cd.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-2c4e7cfc87703302.arrow\n"
     ]
    }
   ],
   "source": [
    "vectorized_datasets = raw_datasets.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=next(iter(raw_datasets.values())).column_names,\n",
    "            num_proc=preprocessing_num_workers,\n",
    "            desc=\"preprocess train dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e0dcd3f-2e3f-4f65-8111-b5682a7b0f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-dc1cbfa3e7304ac2.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-228e1ef514f5c637.arrow\n"
     ]
    }
   ],
   "source": [
    "# filter data with inputs shorter than min_input_length or longer than max_input_length\n",
    "def is_audio_in_length_range(length):\n",
    "    return length > min_input_length and length < max_input_length\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_audio_in_length_range,\n",
    "    num_proc=num_workers,\n",
    "    input_columns=[\"input_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1acf9a7f-2dad-4131-9c42-b6361ace1aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-1f73bf8419e71173.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/sanchitgandhi/cache/huggingface/datasets/librispeech_asr/clean/2.1.0/1f4602f6b5fed8d3ab3e3382783173f2e12d9877e98775e34d7780881175096c/cache-5940edcb91dd459a.arrow\n"
     ]
    }
   ],
   "source": [
    "# filter data with targets shorter than min_target_length or longer than max_target_length\n",
    "def is_labels_in_length_range(length):\n",
    "    return length > min_target_length and length < max_target_length\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_labels_in_length_range,\n",
    "    num_proc=num_workers,\n",
    "    input_columns=[\"labels_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4113029-dd51-4d4c-8f71-74b52c42689f",
   "metadata": {},
   "source": [
    "### 9. Define DataCollators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325456af-0560-4c13-bbff-8b67ba5407d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch DataCollator\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec65219-9e94-42d9-9018-5bb57bb04549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax DataCollator\n",
    "@flax.struct.dataclass\n",
    "class FlaxDataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    input_padding: Union[bool, str] = \"longest\"\n",
    "    target_padding: Union[bool, str] = \"max_length\"\n",
    "    max_input_length: Optional[float] = None\n",
    "    max_target_length: Optional[int] = None\n",
    "    pad_input_to_multiple_of: Optional[int] = None\n",
    "    pad_target_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=self.input_padding,\n",
    "            pad_to_multiple_of=self.pad_input_to_multiple_of,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=self.target_padding,\n",
    "            pad_to_multiple_of=self.pad_target_to_multiple_of,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().item():\n",
    "            labels = labels[:, 1:]\n",
    "            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]\n",
    "\n",
    "        decoder_input_ids = shift_tokens_right(labels, self.decoder_start_token_id)\n",
    "\n",
    "        # replace padding with -100 to ignore correctly when computing the loss\n",
    "        labels = np.ma.array(labels, mask=np.not_equal(labels_batch.attention_mask, 1))\n",
    "        labels = labels.filled(fill_value=-100)\n",
    "\n",
    "        batch[\"inputs\"] = batch.pop(\"input_values\")\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return batch\n",
    "    \n",
    "def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift label ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_label_ids = np.zeros_like(label_ids)\n",
    "    shifted_label_ids[:, 1:] = label_ids[:, :-1]\n",
    "    shifted_label_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    return shifted_label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb6699-2e55-459d-8b68-4c8f46c16874",
   "metadata": {},
   "source": [
    "### 10. Define length grouped sampler (PT and FX compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcacdc57-d168-4be5-ba2a-738031aa4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_indices(\n",
    "    dataset, batch_size: int, rng: Optional[List[int]] = None, mega_batch_mult: Optional[int] = None\n",
    ") -> np.array:\n",
    "    lengths = dataset[\"input_length\"]\n",
    "\n",
    "    # Default for mega_batch_mult: 50 or the number to get 4 megabatches, whichever is smaller.\n",
    "    if mega_batch_mult is None:\n",
    "        mega_batch_mult = min(len(lengths) // (batch_size * 4), 50)\n",
    "        # Just in case, for tiny datasets\n",
    "        if mega_batch_mult == 0:\n",
    "            mega_batch_mult = 1\n",
    "\n",
    "    # We need to use JAX for the random permutation as the PRNG key will be set based on the seed outside of the sampler.\n",
    "    num_samples = len(lengths)\n",
    "    indices = jax.random.permutation(rng, np.arange(num_samples)) if rng is not None else np.arange(num_samples)\n",
    "\n",
    "    megabatch_size = mega_batch_mult * batch_size\n",
    "    megabatches = [indices[i : i + megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]\n",
    "    megabatches = [list(sorted(megabatch, key=lambda i: lengths[i], reverse=True)) for megabatch in megabatches]\n",
    "\n",
    "    # The rest is to get the biggest batch first.\n",
    "    # Since each megabatch is sorted by descending length, the longest element is the first\n",
    "    megabatch_maximums = [lengths[megabatch[0]] for megabatch in megabatches]\n",
    "    max_idx = np.argmax(megabatch_maximums).item()\n",
    "    # Switch to put the longest batch in first position\n",
    "    # (note that this is different to the PT grouped sampler in which we only put the longest element in the first position, and not its batch)\n",
    "    megabatches[0], megabatches[max_idx] = megabatches[max_idx], megabatches[0]\n",
    "\n",
    "    megabatches = np.array([i for megabatch in megabatches for i in megabatch])\n",
    "\n",
    "    return megabatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d20628c0-cca6-4bd2-b153-bb38e0a6a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group samples into batch splits\n",
    "def generate_batch_splits(samples_idx: jnp.ndarray, batch_size: int) -> jnp.ndarray:\n",
    "    num_samples = len(samples_idx)\n",
    "    samples_to_remove = num_samples % batch_size\n",
    "\n",
    "    if samples_to_remove != 0:\n",
    "        samples_idx = samples_idx[:-samples_to_remove]\n",
    "    sections_split = num_samples // batch_size\n",
    "    batch_idx = np.split(samples_idx, sections_split)\n",
    "    return batch_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c5fd4-2d9b-4627-9624-9044a940fbc8",
   "metadata": {},
   "source": [
    "### 11. Helper funcitons for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b19eae5e-c72d-420c-9ea0-80e0fb786d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_almost_equals(a: np.ndarray, b: np.ndarray, tol: float = 1e-2):\n",
    "    \"\"\"Assert whether the maximum absolute difference between two NumPy arrays a and b is within a given tolerance tol. \n",
    "    Due to the pad_to_multiple_of nature of the FlaxDataCollator, the length of the Flax array a will always be greater than \n",
    "    or equal to the length of the PyTorch array b. If a and b are of different lengths, array a (Flax, padded) will be \n",
    "    reshaped to the shape of b (PyTorch).\"\"\"\n",
    "    if a.shape != b.shape:\n",
    "        a = a[:, :b.shape[1]]\n",
    "    \n",
    "    diff = np.abs((a - b))\n",
    "    if diff.max() < tol:\n",
    "        print(f\"✅ Difference between Flax and PyTorch is {diff.max()} (< {tol}), avg is {diff.mean()}\")\n",
    "    else:\n",
    "        print(f\"❌ Difference between Flax and PyTorch is {diff.max()} (>= {tol}), avg is {diff.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "189b0366-34dc-409f-95e3-8010842543bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_dict_equal(a: dict, b: dict, tol: float = 1e-2):\n",
    "    if a.keys() != b.keys():\n",
    "        print(\"❌ Dictionary keys for PyTorch and Flax do not match\")\n",
    "    results_fail = []\n",
    "    results_correct = []\n",
    "\n",
    "    results_fail_rel = []\n",
    "    results_correct_rel = []\n",
    "    for k in a:\n",
    "        ak_norm = np.linalg.norm(a[k])\n",
    "        bk_norm = np.linalg.norm(b[k])\n",
    "        diff = np.abs(ak_norm - bk_norm)\n",
    "        diff_rel = np.abs(ak_norm - bk_norm) / np.abs(ak_norm)\n",
    "        if diff < tol:\n",
    "            results_correct.append(f\"✅ Layer {k} diff is {diff} < {tol}).\")\n",
    "        else:\n",
    "            results_fail.append(f\"❌ Layer {k} has PT grad norm {bk_norm} and flax grad norm {ak_norm}.\")\n",
    "        if diff_rel < tol:\n",
    "            results_correct_rel.append(f\"✅ Layer {k} rel diff is {diff} < {tol}).\")\n",
    "        else:\n",
    "            results_fail_rel.append(f\"❌ Layer {k} has PT grad norm {bk_norm} and flax grad norm {ak_norm}.\")\n",
    "    return results_fail_rel, results_correct_rel, results_fail, results_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "869586c7-1bf5-4c8a-a80c-c920376007bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def kl_divergence(a: np.ndarray, b:np.ndarray, epsilon=1e-6, tol: float = 1e-2):\n",
    "    \"\"\"Epsilon is used here to avoid conditional code for checking that neither p(a) nor p(b) is equal to 0.\"\"\"\n",
    "    if a.shape[1] != b.shape[1]:\n",
    "        a = a[:, :b.shape[1], :]\n",
    "        \n",
    "    p_a = softmax(a) + epsilon\n",
    "    p_b = softmax(b) + epsilon\n",
    "    divergence = np.sum(p_b * np.log(p_b / p_a))\n",
    "    if divergence < tol:\n",
    "        print(f\"✅ KL divergence between Flax and PyTorch is {divergence} (< {tol})\")\n",
    "    else:\n",
    "        print(f\"❌ KL divergence between Flax and PyTorch is {divergence} (>= {tol})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9a12e-08a5-4d3d-af93-5575847d7e3e",
   "metadata": {},
   "source": [
    "### 12. Instantiate data collators and generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53cc0adc-e7d1-4b94-ae56-bd29b1908994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PT and FX DataCollators\n",
    "pt_data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=pt_model.config.decoder_start_token_id,\n",
    "    )\n",
    "\n",
    "fx_data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=fx_model.config.decoder_start_token_id,\n",
    "        input_padding=\"longest\",\n",
    "        target_padding=\"max_length\",\n",
    "        max_target_length=max_target_length,\n",
    "        pad_input_to_multiple_of=pad_input_to_multiple_of,\n",
    "        pad_target_to_multiple_of=pad_target_to_multiple_of,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c87eb0-e120-46df-ba9d-31f0862fb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAX seed and generate PRNG for stochasic operations\n",
    "seed = 0\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "rng, input_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2383df6f-e2e2-481e-8f7d-ec75b92baaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll naively create our batches through random shuffling and no grouping by length\n",
    "num_train_samples = len(vectorized_datasets[\"train\"])\n",
    "train_samples_idx = jax.random.permutation(input_rng, np.arange(num_train_samples))\n",
    "train_batch_idx = generate_batch_splits(train_samples_idx, batch_size_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b67470de-1374-410a-9144-6c2b5ff58874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt: we'll use the grouped sampler\n",
    "train_samples_idx = get_grouped_indices(vectorized_datasets[\"train\"], batch_size_per_update, input_rng)\n",
    "train_batch_idx = generate_batch_splits(train_samples_idx, batch_size_per_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd00ffbf-b613-46cf-a7ba-03597f99ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the first training batch\n",
    "batch_idx = train_batch_idx[1]\n",
    "samples = [vectorized_datasets[\"train\"][int(idx)] for idx in batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85377ea3-3616-4ba4-8f14-52efe488f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_batch = fx_data_collator(samples)\n",
    "pt_batch = pt_data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d0be611-5b08-43cb-a10a-5adb05c6297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Flax inputs to PyTorch (optional)\n",
    "#pt_batch = {k: torch.tensor(v.tolist()) for k, v in fx_batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949c45a-75e7-40a2-b55e-efa60c476517",
   "metadata": {},
   "source": [
    "### 13. Check that the inputs are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c47d8814-dc72-4c5b-b5d2-bf2c2f2af3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['attention_mask', 'inputs', 'labels', 'decoder_input_ids']),\n",
       " dict_keys(['input_values', 'attention_mask', 'labels']))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_fx_keys = [\"inputs\", \"labels\", \"decoder_input_ids\"]\n",
    "expected_pt_keys = [\"input_values\", \"labels\"]\n",
    "\n",
    "for expected_fx_key in expected_fx_keys:\n",
    "    assert expected_fx_key in fx_batch, f\"{expected_fx_key} not in Flax batched inputs\"\n",
    "\n",
    "for expected_pt_key in expected_pt_keys:\n",
    "    assert expected_pt_key in pt_batch, f\"{expected_pt_key} not in PyTorch batched inputs\"    \n",
    "\n",
    "# Expect the keys between Flax and PyTorch to be different, this is just for observation\n",
    "fx_batch.keys(), pt_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9acf4d02-e5e7-4452-b3c0-8e6fb4873a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Difference between Flax and PyTorch is 0.0 (< 0.01), avg is 0.0\n",
      "✅ Difference between Flax and PyTorch is 0 (< 0.01), avg is 0.0\n",
      "✅ Difference between Flax and PyTorch is 0 (< 0.01), avg is 0.0\n"
     ]
    }
   ],
   "source": [
    "assert_almost_equals(fx_batch['inputs'], pt_batch['input_values'].numpy())\n",
    "assert_almost_equals(fx_batch['labels'], pt_batch['labels'].numpy())\n",
    "if 'attention_mask' in fx_batch.keys() and pt_batch.keys():\n",
    "    assert_almost_equals(fx_batch['attention_mask'], pt_batch['attention_mask'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a599272-1f9a-4711-bc1b-908ae0b6006b",
   "metadata": {},
   "source": [
    "### 14. Run a training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c33f27f5-8387-41b2-9b09-91b15b1423da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch, output_hidden_states=True)\n",
    "pt_logits = pt_outputs.logits\n",
    "pt_loss = pt_outputs.loss\n",
    "pt_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9843812b-d3f2-4f3b-bcaa-5176ced950cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax cross entropy loss\n",
    "def loss_fn(logits, labels):\n",
    "    vocab_size = logits.shape[-1]\n",
    "    loss = optax.softmax_cross_entropy(logits, onehot(labels, vocab_size))\n",
    "    # ignore padded tokens from loss, i.e. where labels are not set to -100\n",
    "    padding = labels >= 0\n",
    "    loss = loss * padding\n",
    "    loss = loss.sum() / padding.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5db6a35b-d2cc-42d4-98b1-cbaac62d7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax training step (single device)\n",
    "def fx_train_step(fx_model, fx_batch, output_hidden_states=True):\n",
    "    def compute_loss(params):\n",
    "        labels = fx_batch.pop(\"labels\")\n",
    "        outputs = fx_model(**fx_batch, params=params, output_hidden_states=output_hidden_states)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        return loss, outputs\n",
    "\n",
    "    grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
    "    (loss, outputs), grad = grad_fn(fx_model.params)\n",
    "    \n",
    "    return loss, outputs, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd82d9ef-2dc1-41dc-9855-45ccfdd02413",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_loss, fx_outputs, fx_grad = fx_train_step(fx_model, fx_batch, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a30c5b-ca1c-44a1-9fb1-255e908c8e5b",
   "metadata": {},
   "source": [
    "### 15. Compare outputs for the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6674278-d69f-4ab3-9bf2-f80706b9af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking encoder hidden states match--------------------------\n",
      "❌ Difference between Flax and PyTorch is 1.394815444946289 (>= 0.01), avg is 0.03192371875047684\n",
      "❌ Difference between Flax and PyTorch is 2.4984569549560547 (>= 0.01), avg is 0.04434331879019737\n",
      "❌ Difference between Flax and PyTorch is 2.673779010772705 (>= 0.01), avg is 0.05054352805018425\n",
      "❌ Difference between Flax and PyTorch is 2.7711181640625 (>= 0.01), avg is 0.056128352880477905\n",
      "❌ Difference between Flax and PyTorch is 3.0873947143554688 (>= 0.01), avg is 0.061000000685453415\n",
      "❌ Difference between Flax and PyTorch is 3.1466064453125 (>= 0.01), avg is 0.06460364162921906\n",
      "❌ Difference between Flax and PyTorch is 6.617366790771484 (>= 0.01), avg is 0.06648571044206619\n",
      "❌ Difference between Flax and PyTorch is 150.392822265625 (>= 0.01), avg is 0.07012273371219635\n",
      "❌ Difference between Flax and PyTorch is 154.6243438720703 (>= 0.01), avg is 0.07368026673793793\n",
      "❌ Difference between Flax and PyTorch is 156.58375549316406 (>= 0.01), avg is 0.07746029645204544\n",
      "❌ Difference between Flax and PyTorch is 156.1802215576172 (>= 0.01), avg is 0.08235222101211548\n",
      "❌ Difference between Flax and PyTorch is 159.75428771972656 (>= 0.01), avg is 0.08933980017900467\n",
      "❌ Difference between Flax and PyTorch is 157.0019073486328 (>= 0.01), avg is 0.09810765087604523\n",
      "❌ Difference between Flax and PyTorch is 158.51901245117188 (>= 0.01), avg is 0.11121237277984619\n",
      "❌ Difference between Flax and PyTorch is 159.49014282226562 (>= 0.01), avg is 0.12522055208683014\n",
      "❌ Difference between Flax and PyTorch is 152.04678344726562 (>= 0.01), avg is 0.14311520755290985\n",
      "❌ Difference between Flax and PyTorch is 131.97470092773438 (>= 0.01), avg is 0.17541538178920746\n",
      "❌ Difference between Flax and PyTorch is 123.85238647460938 (>= 0.01), avg is 0.20147424936294556\n",
      "❌ Difference between Flax and PyTorch is 115.40216064453125 (>= 0.01), avg is 0.23707512021064758\n",
      "❌ Difference between Flax and PyTorch is 97.30654907226562 (>= 0.01), avg is 0.28712138533592224\n",
      "❌ Difference between Flax and PyTorch is 72.67626953125 (>= 0.01), avg is 0.3357849419116974\n",
      "❌ Difference between Flax and PyTorch is 76.23779296875 (>= 0.01), avg is 0.3861161470413208\n",
      "❌ Difference between Flax and PyTorch is 2082.6396484375 (>= 0.01), avg is 4.148151397705078\n",
      "❌ Difference between Flax and PyTorch is 2040.3076171875 (>= 0.01), avg is 5.571300983428955\n",
      "❌ Difference between Flax and PyTorch is 0.41159939765930176 (>= 0.01), avg is 0.00025369832292199135\n",
      "--------------------------Checking encoder last hidden states match--------------------------\n",
      "❌ Difference between Flax and PyTorch is 0.41159939765930176 (>= 0.01), avg is 0.00025369832292199135\n",
      "--------------------------Checking decoder hidden states match--------------------------\n",
      "❌ Difference between Flax and PyTorch is 3.37753963470459 (>= 0.01), avg is 0.023428764194250107\n",
      "❌ Difference between Flax and PyTorch is 2.4751734733581543 (>= 0.01), avg is 0.01842828467488289\n",
      "❌ Difference between Flax and PyTorch is 2.2446417808532715 (>= 0.01), avg is 0.012076234444975853\n",
      "❌ Difference between Flax and PyTorch is 1.324981451034546 (>= 0.01), avg is 0.006769894156605005\n",
      "❌ Difference between Flax and PyTorch is 0.15849795937538147 (>= 0.01), avg is 0.0010029284749180079\n",
      "❌ Difference between Flax and PyTorch is 0.014220401644706726 (>= 0.01), avg is 0.0004946206463500857\n",
      "✅ Difference between Flax and PyTorch is 0.0011734217405319214 (< 0.01), avg is 7.471351273125038e-05\n",
      "✅ Difference between Flax and PyTorch is 0.0013426542282104492 (< 0.01), avg is 7.233302312670276e-05\n",
      "✅ Difference between Flax and PyTorch is 0.0002247244119644165 (< 0.01), avg is 2.8627157007576898e-05\n",
      "✅ Difference between Flax and PyTorch is 0.00040738843381404877 (< 0.01), avg is 5.745477392338216e-05\n",
      "✅ Difference between Flax and PyTorch is 0.0010096728801727295 (< 0.01), avg is 0.0001213980867760256\n",
      "✅ Difference between Flax and PyTorch is 0.0018175840377807617 (< 0.01), avg is 0.00018452036601956934\n",
      "❌ Difference between Flax and PyTorch is 0.02248215675354004 (>= 0.01), avg is 0.004517388995736837\n",
      "--------------------------Checking logits match--------------------------\n",
      "Flax logits shape: (2, 31, 50265), PyTorch logits shape: torch.Size([2, 21, 50265])\n",
      "❌ Difference between Flax and PyTorch is 0.16919326782226562 (>= 0.01), avg is 0.01640697941184044\n",
      "❌ KL divergence between Flax and PyTorch is 45.524024963378906 (>= 0.01)\n",
      "--------------------------Checking losses match--------------------------\n",
      "Flax loss: 25.68524742126465, PyTorch loss: 25.756454467773438\n",
      "❌ Difference between Flax and PyTorch is 0.07120704650878906 (>= 0.01), avg is 0.07120704650878906\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking encoder hidden states match--------------------------\")\n",
    "for fx_state, pt_state in zip(fx_outputs.encoder_hidden_states, pt_outputs.encoder_hidden_states):\n",
    "    assert_almost_equals(fx_state, pt_state.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking encoder last hidden states match--------------------------\")\n",
    "assert_almost_equals(fx_outputs.encoder_last_hidden_state, pt_outputs.encoder_last_hidden_state.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking decoder hidden states match--------------------------\")\n",
    "for fx_state, pt_state in zip(fx_outputs.decoder_hidden_states, pt_outputs.decoder_hidden_states):\n",
    "    assert_almost_equals(fx_state, pt_state.detach().numpy())\n",
    "    \n",
    "print(\"--------------------------Checking logits match--------------------------\")\n",
    "print(f\"Flax logits shape: {fx_outputs.logits.shape}, PyTorch logits shape: {pt_logits.shape}\")\n",
    "assert_almost_equals(fx_outputs.logits, pt_logits.detach().numpy())\n",
    "kl_divergence(fx_outputs.logits, pt_logits.detach().numpy())\n",
    "\n",
    "print(\"--------------------------Checking losses match--------------------------\")\n",
    "print(f\"Flax loss: {fx_loss}, PyTorch loss: {pt_loss}\")\n",
    "assert_almost_equals(fx_loss, pt_loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8665ed-8f9b-44c3-bb27-e0c4f233726e",
   "metadata": {},
   "source": [
    "### 16. Compare outputs for the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "128835da-5a10-4dc3-bb3a-ff530be70651",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_grad_dict = {k: v.grad if v.grad is not None else torch.zeros(v.shape) for k, v in pt_model.named_parameters()}\n",
    "missing_grads = [k for k in pt_model.state_dict().keys() if k not in pt_grad_dict]\n",
    "\n",
    "missing_keys, unexpected_keys = pt_model.load_state_dict(pt_grad_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7298d88f-8d06-4f6c-be85-ae7c4ffb8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert missing_grads == missing_keys, f\"Error with either grads {missing_keys} or keys {unexpected_keys}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c52fe36-304f-41b6-ac45-b250183e74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /tmp/tmpedj5kw1y were not used when initializing FlaxSpeechEncoderDecoderModel: {('decoder', 'lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxSpeechEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    pt_model.save_pretrained(tmpdirname)\n",
    "    pt_grad_model_to_fx = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(tmpdirname, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd037b4a-609d-44b6-9e4a-9de0525f6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_grad_to_fx = pt_grad_model_to_fx.params\n",
    "fx_grad = flatten_dict(fx_grad)\n",
    "pt_grad_to_fx = flatten_dict(pt_grad_to_fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6022185e-71de-4338-8150-ffb819bce349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1313943/2310070863.py:13: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  diff_rel = np.abs(ak_norm - bk_norm) / np.abs(ak_norm)\n",
      "/tmp/ipykernel_1313943/2310070863.py:13: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  diff_rel = np.abs(ak_norm - bk_norm) / np.abs(ak_norm)\n"
     ]
    }
   ],
   "source": [
    "results_fail_rel, results_correct_rel, results_fail, results_correct = assert_dict_equal(fx_grad, pt_grad_to_fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16f0517d-5bff-4825-8a30-5fa86723f314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking gradients match--------------------------\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'embed_tokens', 'embedding') has PT grad norm 39.86210250854492 and flax grad norm 39.78125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 5.28529691696167 and flax grad norm 5.2578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 83.90470123291016 and flax grad norm 83.4375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 6.587379455566406 and flax grad norm 6.59765625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 7.243805885314941 and flax grad norm 7.20703125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 58.8936653137207 and flax grad norm 58.59375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn_layer_norm', 'bias') has PT grad norm 16.801586151123047 and flax grad norm 16.703125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn_layer_norm', 'scale') has PT grad norm 12.863924980163574 and flax grad norm 12.7890625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'fc1', 'bias') has PT grad norm 5.173980236053467 and flax grad norm 5.140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'fc1', 'kernel') has PT grad norm 552.0279541015625 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'fc2', 'bias') has PT grad norm 15.453487396240234 and flax grad norm 15.3828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'fc2', 'kernel') has PT grad norm 93.37858581542969 and flax grad norm 92.5.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'final_layer_norm', 'bias') has PT grad norm 127.92769622802734 and flax grad norm 127.4375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'final_layer_norm', 'scale') has PT grad norm 144.46214294433594 and flax grad norm 144.875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'out_proj', 'bias') has PT grad norm 43.27117156982422 and flax grad norm 43.0625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel') has PT grad norm 53.61735534667969 and flax grad norm 53.34375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'v_proj', 'bias') has PT grad norm 48.11916732788086 and flax grad norm 47.90625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel') has PT grad norm 119.48453521728516 and flax grad norm 119.0625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn_layer_norm', 'bias') has PT grad norm 5.2825727462768555 and flax grad norm 5.2578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn_layer_norm', 'scale') has PT grad norm 4.301183700561523 and flax grad norm 4.25.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 16.2856502532959 and flax grad norm 16.21875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 279.05181884765625 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 12.052743911743164 and flax grad norm 12.03125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 20.851003646850586 and flax grad norm 20.765625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 162.196044921875 and flax grad norm 161.375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn_layer_norm', 'bias') has PT grad norm 45.44382858276367 and flax grad norm 45.21875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn_layer_norm', 'scale') has PT grad norm 38.52711868286133 and flax grad norm 38.34375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'fc1', 'bias') has PT grad norm 17.155752182006836 and flax grad norm 17.09375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'fc1', 'kernel') has PT grad norm 305.48760986328125 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'fc2', 'bias') has PT grad norm 17.310277938842773 and flax grad norm 17.265625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'fc2', 'kernel') has PT grad norm 254.1536865234375 and flax grad norm 252.375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'final_layer_norm', 'bias') has PT grad norm 7.030727863311768 and flax grad norm 7.01953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'final_layer_norm', 'scale') has PT grad norm 8.44415283203125 and flax grad norm 8.40625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'out_proj', 'bias') has PT grad norm 97.77052307128906 and flax grad norm 97.375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel') has PT grad norm 169.980224609375 and flax grad norm 169.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'v_proj', 'bias') has PT grad norm 99.77791595458984 and flax grad norm 99.25.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel') has PT grad norm 175.14663696289062 and flax grad norm 174.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn_layer_norm', 'bias') has PT grad norm 16.597745895385742 and flax grad norm 16.515625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn_layer_norm', 'scale') has PT grad norm 20.549999237060547 and flax grad norm 20.484375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'fc1', 'kernel') has PT grad norm 3.3913259506225586 and flax grad norm 3.326171875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'fc2', 'kernel') has PT grad norm 1.2667726278305054 and flax grad norm 1.251953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'final_layer_norm', 'bias') has PT grad norm 2.5988903045654297 and flax grad norm 2.5703125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'final_layer_norm', 'scale') has PT grad norm 0.7117212414741516 and flax grad norm 0.6953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 3.3945915699005127 and flax grad norm 3.359375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 3.0296528339385986 and flax grad norm 2.998046875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'fc1', 'kernel') has PT grad norm 10.76535415649414 and flax grad norm 10.6328125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'fc2', 'kernel') has PT grad norm 8.510902404785156 and flax grad norm 8.4296875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'final_layer_norm', 'bias') has PT grad norm 16.430465698242188 and flax grad norm 16.359375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'final_layer_norm', 'scale') has PT grad norm 1.4433948993682861 and flax grad norm 1.42578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'out_proj', 'bias') has PT grad norm 1.487431287765503 and flax grad norm 1.47265625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'v_proj', 'bias') has PT grad norm 1.333180546760559 and flax grad norm 1.3203125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel') has PT grad norm 4.9185943603515625 and flax grad norm 4.8671875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 2.2475123405456543 and flax grad norm 2.236328125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 34.45231628417969 and flax grad norm 34.25.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 3.8820455074310303 and flax grad norm 3.86328125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 32.31367111206055 and flax grad norm 32.15625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn_layer_norm', 'bias') has PT grad norm 5.389934062957764 and flax grad norm 5.36328125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn_layer_norm', 'scale') has PT grad norm 4.05630350112915 and flax grad norm 4.0390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'fc1', 'bias') has PT grad norm 2.389892578125 and flax grad norm 2.376953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'fc1', 'kernel') has PT grad norm 285.70281982421875 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'fc2', 'bias') has PT grad norm 4.519000053405762 and flax grad norm 4.5.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'fc2', 'kernel') has PT grad norm 51.36979293823242 and flax grad norm 51.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'final_layer_norm', 'bias') has PT grad norm 65.48123168945312 and flax grad norm 65.125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'final_layer_norm', 'scale') has PT grad norm 30.635169982910156 and flax grad norm 30.609375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'out_proj', 'bias') has PT grad norm 11.804428100585938 and flax grad norm 11.75.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel') has PT grad norm 6.012938022613525 and flax grad norm 5.9609375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'v_proj', 'bias') has PT grad norm 10.5188570022583 and flax grad norm 10.4765625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel') has PT grad norm 49.815711975097656 and flax grad norm 49.375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn_layer_norm', 'bias') has PT grad norm 2.2475852966308594 and flax grad norm 2.236328125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn_layer_norm', 'scale') has PT grad norm 0.7706422209739685 and flax grad norm 0.75732421875.\n",
      "❌ Layer ('encoder', 'encoder', 'layer_norm', 'bias') has PT grad norm 41.80746078491211 and flax grad norm 41.63560104370117.\n",
      "❌ Layer ('encoder', 'encoder', 'layer_norm', 'scale') has PT grad norm 35.0162353515625 and flax grad norm 34.80531311035156.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'k_proj', 'kernel') has PT grad norm 0.28037506341934204 and flax grad norm 0.23043109476566315.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'out_proj', 'bias') has PT grad norm 0.21775557100772858 and flax grad norm 0.1887841820716858.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'out_proj', 'kernel') has PT grad norm 1.5720638036727905 and flax grad norm 1.33225679397583.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'q_proj', 'bias') has PT grad norm 0.07816984504461288 and flax grad norm 0.0643257275223732.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'q_proj', 'kernel') has PT grad norm 0.24319270253181458 and flax grad norm 0.19909101724624634.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'v_proj', 'bias') has PT grad norm 0.8443093299865723 and flax grad norm 0.72789067029953.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'v_proj', 'kernel') has PT grad norm 1.2489112615585327 and flax grad norm 1.059487223625183.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.6354469656944275 and flax grad norm 0.5429699420928955.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 2.809272527694702 and flax grad norm 2.425384044647217.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.1641029566526413 and flax grad norm 0.14287205040454865.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 2.179293632507324 and flax grad norm 1.8549848794937134.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'final_layer_norm', 'bias') has PT grad norm 3.3679847717285156 and flax grad norm 2.9132819175720215.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'final_layer_norm', 'scale') has PT grad norm 4.072488307952881 and flax grad norm 3.450411796569824.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'layer_norm', 'bias') has PT grad norm 2.1898903846740723 and flax grad norm 1.8968398571014404.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'layer_norm', 'scale') has PT grad norm 2.551924228668213 and flax grad norm 2.1689453125.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'k_proj', 'kernel') has PT grad norm 0.4768775701522827 and flax grad norm 0.4004431962966919.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'out_proj', 'bias') has PT grad norm 0.17502975463867188 and flax grad norm 0.14971910417079926.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'out_proj', 'kernel') has PT grad norm 1.688887357711792 and flax grad norm 1.4100209474563599.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'q_proj', 'bias') has PT grad norm 0.08073592185974121 and flax grad norm 0.06899707019329071.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4269952178001404 and flax grad norm 0.35969215631484985.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'v_proj', 'bias') has PT grad norm 0.6302688717842102 and flax grad norm 0.5322055816650391.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'v_proj', 'kernel') has PT grad norm 1.7588008642196655 and flax grad norm 1.4788819551467896.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3634817600250244 and flax grad norm 0.3069544732570648.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.8312103748321533 and flax grad norm 1.5551892518997192.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.16859814524650574 and flax grad norm 0.14614613354206085.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.5318032503128052 and flax grad norm 1.2774837017059326.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'final_layer_norm', 'bias') has PT grad norm 1.8284684419631958 and flax grad norm 1.5804070234298706.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'final_layer_norm', 'scale') has PT grad norm 1.8860584497451782 and flax grad norm 1.5444484949111938.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'layer_norm', 'bias') has PT grad norm 2.255664825439453 and flax grad norm 1.839105248451233.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'layer_norm', 'scale') has PT grad norm 2.07712459564209 and flax grad norm 1.7764579057693481.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'k_proj', 'kernel') has PT grad norm 0.5083320140838623 and flax grad norm 0.4085859954357147.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'out_proj', 'bias') has PT grad norm 0.15212149918079376 and flax grad norm 0.12835854291915894.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'out_proj', 'kernel') has PT grad norm 1.3483784198760986 and flax grad norm 1.1139748096466064.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'q_proj', 'bias') has PT grad norm 0.10683486610651016 and flax grad norm 0.08699087798595428.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5139941573143005 and flax grad norm 0.41244879364967346.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'v_proj', 'bias') has PT grad norm 0.5675718188285828 and flax grad norm 0.47268369793891907.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'v_proj', 'kernel') has PT grad norm 1.2417538166046143 and flax grad norm 1.0225051641464233.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 0.6127423048019409 and flax grad norm 0.5090510249137878.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.15080440044403076 and flax grad norm 0.1272840052843094.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 0.6551542282104492 and flax grad norm 0.533413827419281.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'final_layer_norm', 'bias') has PT grad norm 0.11640147864818573 and flax grad norm 0.09384142607450485.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'final_layer_norm', 'scale') has PT grad norm 0.15305566787719727 and flax grad norm 0.12493584305047989.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'layer_norm', 'bias') has PT grad norm 2.707404613494873 and flax grad norm 2.2474253177642822.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'layer_norm', 'scale') has PT grad norm 2.6437695026397705 and flax grad norm 2.160187244415283.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'k_proj', 'kernel') has PT grad norm 0.48123008012771606 and flax grad norm 0.39347949624061584.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'out_proj', 'bias') has PT grad norm 0.15392206609249115 and flax grad norm 0.12927395105361938.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'out_proj', 'kernel') has PT grad norm 1.4576045274734497 and flax grad norm 1.2024532556533813.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'q_proj', 'bias') has PT grad norm 0.08779365569353104 and flax grad norm 0.07099972665309906.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4874456822872162 and flax grad norm 0.39279794692993164.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'v_proj', 'bias') has PT grad norm 0.5804410576820374 and flax grad norm 0.4814050793647766.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'v_proj', 'kernel') has PT grad norm 1.3450673818588257 and flax grad norm 1.0995910167694092.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 0.6541049480438232 and flax grad norm 0.5295652151107788.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.14955636858940125 and flax grad norm 0.1262766718864441.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 0.6312607526779175 and flax grad norm 0.5172069668769836.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'final_layer_norm', 'bias') has PT grad norm 0.1324380487203598 and flax grad norm 0.10319042205810547.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'final_layer_norm', 'scale') has PT grad norm 0.2228212207555771 and flax grad norm 0.18071196973323822.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'layer_norm', 'bias') has PT grad norm 2.924738883972168 and flax grad norm 2.419320583343506.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'layer_norm', 'scale') has PT grad norm 2.1120128631591797 and flax grad norm 1.691174864768982.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'k_proj', 'kernel') has PT grad norm 0.557593047618866 and flax grad norm 0.4594143033027649.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'out_proj', 'bias') has PT grad norm 0.14002037048339844 and flax grad norm 0.11840979009866714.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'out_proj', 'kernel') has PT grad norm 1.6002331972122192 and flax grad norm 1.3205522298812866.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'q_proj', 'bias') has PT grad norm 0.14304043352603912 and flax grad norm 0.1226092129945755.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5401696562767029 and flax grad norm 0.444502055644989.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'v_proj', 'bias') has PT grad norm 0.5773617029190063 and flax grad norm 0.4831111431121826.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'v_proj', 'kernel') has PT grad norm 1.5228155851364136 and flax grad norm 1.2537437677383423.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.14216212928295135 and flax grad norm 0.11990304291248322.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.3638768196105957 and flax grad norm 1.1306654214859009.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13446617126464844 and flax grad norm 0.113617442548275.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.4486067295074463 and flax grad norm 1.1995127201080322.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'final_layer_norm', 'bias') has PT grad norm 0.5859650373458862 and flax grad norm 0.4924713671207428.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'final_layer_norm', 'scale') has PT grad norm 0.6069837808609009 and flax grad norm 0.5111620426177979.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'layer_norm', 'bias') has PT grad norm 2.820687770843506 and flax grad norm 2.3652708530426025.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'layer_norm', 'scale') has PT grad norm 1.4831488132476807 and flax grad norm 1.318660855293274.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'k_proj', 'kernel') has PT grad norm 0.5742043852806091 and flax grad norm 0.4651747941970825.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'out_proj', 'bias') has PT grad norm 0.1328696608543396 and flax grad norm 0.11201778799295425.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'out_proj', 'kernel') has PT grad norm 2.052797555923462 and flax grad norm 1.6649430990219116.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'q_proj', 'bias') has PT grad norm 0.1383107304573059 and flax grad norm 0.1122388020157814.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5238744020462036 and flax grad norm 0.4256390631198883.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'v_proj', 'bias') has PT grad norm 0.5416989922523499 and flax grad norm 0.4519098997116089.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'v_proj', 'kernel') has PT grad norm 1.5308847427368164 and flax grad norm 1.2519383430480957.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.1259712427854538 and flax grad norm 0.10400155931711197.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.4478530883789062 and flax grad norm 1.1909410953521729.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13247522711753845 and flax grad norm 0.11168470233678818.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.7494932413101196 and flax grad norm 1.4436219930648804.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'final_layer_norm', 'bias') has PT grad norm 0.5598880052566528 and flax grad norm 0.45601779222488403.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'final_layer_norm', 'scale') has PT grad norm 1.0347768068313599 and flax grad norm 0.8765773773193359.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'layer_norm', 'bias') has PT grad norm 2.715980052947998 and flax grad norm 2.275428295135498.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'layer_norm', 'scale') has PT grad norm 1.9814205169677734 and flax grad norm 1.647208571434021.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'k_proj', 'kernel') has PT grad norm 0.4469851851463318 and flax grad norm 0.36729735136032104.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'out_proj', 'bias') has PT grad norm 0.13142453134059906 and flax grad norm 0.11055954545736313.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'out_proj', 'kernel') has PT grad norm 1.9143421649932861 and flax grad norm 1.5775372982025146.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'q_proj', 'bias') has PT grad norm 0.10769005864858627 and flax grad norm 0.08807209879159927.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'q_proj', 'kernel') has PT grad norm 0.46542465686798096 and flax grad norm 0.37908872961997986.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'v_proj', 'bias') has PT grad norm 0.4869745075702667 and flax grad norm 0.40675675868988037.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'v_proj', 'kernel') has PT grad norm 1.469372034072876 and flax grad norm 1.2017894983291626.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.1434319168329239 and flax grad norm 0.11712374538183212.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.4998465776443481 and flax grad norm 1.2292531728744507.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13336989283561707 and flax grad norm 0.11251944303512573.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.4687641859054565 and flax grad norm 1.2205621004104614.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'final_layer_norm', 'bias') has PT grad norm 0.5628701448440552 and flax grad norm 0.4618908762931824.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'final_layer_norm', 'scale') has PT grad norm 0.7930547595024109 and flax grad norm 0.6670147776603699.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'layer_norm', 'bias') has PT grad norm 2.381314516067505 and flax grad norm 1.9863977432250977.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'layer_norm', 'scale') has PT grad norm 2.0936431884765625 and flax grad norm 1.8034288883209229.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'k_proj', 'kernel') has PT grad norm 0.4350140690803528 and flax grad norm 0.35675913095474243.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'out_proj', 'bias') has PT grad norm 0.13229776918888092 and flax grad norm 0.11156023293733597.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'out_proj', 'kernel') has PT grad norm 2.2099153995513916 and flax grad norm 1.8171441555023193.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'q_proj', 'bias') has PT grad norm 0.15357331931591034 and flax grad norm 0.12450321018695831.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5409072041511536 and flax grad norm 0.43836838006973267.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'v_proj', 'bias') has PT grad norm 0.49617692828178406 and flax grad norm 0.4134994447231293.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'v_proj', 'kernel') has PT grad norm 1.6282873153686523 and flax grad norm 1.3312454223632812.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.1935179978609085 and flax grad norm 0.1578790694475174.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.7321749925613403 and flax grad norm 1.4140604734420776.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13403764367103577 and flax grad norm 0.1131025105714798.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 3.414853096008301 and flax grad norm 2.755664587020874.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'final_layer_norm', 'bias') has PT grad norm 0.8818920850753784 and flax grad norm 0.7221885919570923.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'final_layer_norm', 'scale') has PT grad norm 3.4858107566833496 and flax grad norm 2.865558624267578.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'layer_norm', 'bias') has PT grad norm 2.4670019149780273 and flax grad norm 2.0517497062683105.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'layer_norm', 'scale') has PT grad norm 1.6560161113739014 and flax grad norm 1.3090153932571411.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'k_proj', 'kernel') has PT grad norm 0.6748400926589966 and flax grad norm 0.5392290353775024.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'out_proj', 'bias') has PT grad norm 0.1350592076778412 and flax grad norm 0.1136399433016777.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'out_proj', 'kernel') has PT grad norm 2.0356130599975586 and flax grad norm 1.6796478033065796.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'q_proj', 'bias') has PT grad norm 0.15481184422969818 and flax grad norm 0.12415021657943726.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'q_proj', 'kernel') has PT grad norm 0.7325260043144226 and flax grad norm 0.5841783285140991.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'v_proj', 'bias') has PT grad norm 0.4935604929924011 and flax grad norm 0.4122646450996399.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'v_proj', 'kernel') has PT grad norm 1.6108660697937012 and flax grad norm 1.3239330053329468.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.19127795100212097 and flax grad norm 0.1564301699399948.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.8110522031784058 and flax grad norm 1.479600191116333.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13880515098571777 and flax grad norm 0.11707475036382675.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.9165130853652954 and flax grad norm 1.583343744277954.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'final_layer_norm', 'bias') has PT grad norm 0.9494715929031372 and flax grad norm 0.7837269902229309.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'final_layer_norm', 'scale') has PT grad norm 2.7706851959228516 and flax grad norm 2.182366132736206.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'layer_norm', 'bias') has PT grad norm 2.9434971809387207 and flax grad norm 2.451265811920166.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'layer_norm', 'scale') has PT grad norm 2.126131296157837 and flax grad norm 1.8108830451965332.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'k_proj', 'kernel') has PT grad norm 0.6838885545730591 and flax grad norm 0.5566033720970154.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'out_proj', 'bias') has PT grad norm 0.14493951201438904 and flax grad norm 0.12204547971487045.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'out_proj', 'kernel') has PT grad norm 2.5677101612091064 and flax grad norm 2.1262154579162598.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'q_proj', 'bias') has PT grad norm 0.1750863939523697 and flax grad norm 0.14122818410396576.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'q_proj', 'kernel') has PT grad norm 0.7184332609176636 and flax grad norm 0.580525815486908.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'v_proj', 'bias') has PT grad norm 0.5682583451271057 and flax grad norm 0.47484153509140015.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'v_proj', 'kernel') has PT grad norm 1.827661395072937 and flax grad norm 1.51165771484375.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.42279747128486633 and flax grad norm 0.34819650650024414.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 3.652240037918091 and flax grad norm 2.997344732284546.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.1532972902059555 and flax grad norm 0.130055233836174.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 3.515766143798828 and flax grad norm 2.9617629051208496.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'final_layer_norm', 'bias') has PT grad norm 1.8942078351974487 and flax grad norm 1.5514527559280396.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'final_layer_norm', 'scale') has PT grad norm 7.0014190673828125 and flax grad norm 5.534465312957764.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'layer_norm', 'bias') has PT grad norm 3.3812739849090576 and flax grad norm 2.823530435562134.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'layer_norm', 'scale') has PT grad norm 1.632197618484497 and flax grad norm 1.3253931999206543.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'k_proj', 'kernel') has PT grad norm 0.8465508222579956 and flax grad norm 0.7023743987083435.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'out_proj', 'bias') has PT grad norm 0.1576486974954605 and flax grad norm 0.13404370844364166.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'out_proj', 'kernel') has PT grad norm 2.6179518699645996 and flax grad norm 2.196810483932495.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'q_proj', 'bias') has PT grad norm 0.19153258204460144 and flax grad norm 0.15700407326221466.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'q_proj', 'kernel') has PT grad norm 0.9289499521255493 and flax grad norm 0.7529858350753784.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'v_proj', 'bias') has PT grad norm 0.6878226399421692 and flax grad norm 0.5863615274429321.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'v_proj', 'kernel') has PT grad norm 2.2017531394958496 and flax grad norm 1.8559203147888184.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3930789530277252 and flax grad norm 0.34359827637672424.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 4.36956787109375 and flax grad norm 3.8329172134399414.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.1749834418296814 and flax grad norm 0.15017865598201752.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 3.50451922416687 and flax grad norm 2.948387622833252.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'final_layer_norm', 'bias') has PT grad norm 1.6479648351669312 and flax grad norm 1.4270379543304443.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'final_layer_norm', 'scale') has PT grad norm 8.064411163330078 and flax grad norm 6.882790565490723.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'layer_norm', 'bias') has PT grad norm 3.7293262481689453 and flax grad norm 3.1710524559020996.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'layer_norm', 'scale') has PT grad norm 3.0567433834075928 and flax grad norm 2.614061117172241.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'k_proj', 'kernel') has PT grad norm 1.1159549951553345 and flax grad norm 0.9184103012084961.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'out_proj', 'bias') has PT grad norm 0.1734507828950882 and flax grad norm 0.149231418967247.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'out_proj', 'kernel') has PT grad norm 3.5496184825897217 and flax grad norm 2.996391534805298.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'q_proj', 'bias') has PT grad norm 0.2928359806537628 and flax grad norm 0.23927631974220276.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'q_proj', 'kernel') has PT grad norm 1.4664556980133057 and flax grad norm 1.1979496479034424.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'v_proj', 'bias') has PT grad norm 0.6948534250259399 and flax grad norm 0.5920690298080444.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'v_proj', 'kernel') has PT grad norm 2.407099723815918 and flax grad norm 2.0250449180603027.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3426833748817444 and flax grad norm 0.2849460542201996.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 5.176263809204102 and flax grad norm 4.342859745025635.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.15377110242843628 and flax grad norm 0.1299199014902115.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 4.212915897369385 and flax grad norm 3.5380475521087646.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'final_layer_norm', 'bias') has PT grad norm 2.0790393352508545 and flax grad norm 1.780023455619812.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'final_layer_norm', 'scale') has PT grad norm 1.615254282951355 and flax grad norm 1.3672000169754028.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'layer_norm', 'bias') has PT grad norm 4.005133628845215 and flax grad norm 3.3703696727752686.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'layer_norm', 'scale') has PT grad norm 3.461199998855591 and flax grad norm 2.8454177379608154.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'k_proj', 'kernel') has PT grad norm 0.5701814293861389 and flax grad norm 0.47327929735183716.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'out_proj', 'bias') has PT grad norm 0.16848407685756683 and flax grad norm 0.14606811106204987.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'out_proj', 'kernel') has PT grad norm 1.458738923072815 and flax grad norm 1.2041767835617065.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'q_proj', 'bias') has PT grad norm 0.12251641601324081 and flax grad norm 0.10178196430206299.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5855506062507629 and flax grad norm 0.4898696541786194.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'v_proj', 'bias') has PT grad norm 0.6343364119529724 and flax grad norm 0.542090892791748.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'v_proj', 'kernel') has PT grad norm 1.6253340244293213 and flax grad norm 1.3723618984222412.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3385002017021179 and flax grad norm 0.2821136713027954.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.6966241598129272 and flax grad norm 1.4235783815383911.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.15465417504310608 and flax grad norm 0.13468612730503082.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.3911552429199219 and flax grad norm 1.138956904411316.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'final_layer_norm', 'bias') has PT grad norm 2.0150527954101562 and flax grad norm 1.6236039400100708.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'final_layer_norm', 'scale') has PT grad norm 2.837282419204712 and flax grad norm 2.4950859546661377.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'layer_norm', 'bias') has PT grad norm 2.4047434329986572 and flax grad norm 2.033325672149658.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'layer_norm', 'scale') has PT grad norm 1.931193232536316 and flax grad norm 1.6288844347000122.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'k_proj', 'kernel') has PT grad norm 1.4932148456573486 and flax grad norm 1.2391217947006226.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'out_proj', 'bias') has PT grad norm 0.15450838208198547 and flax grad norm 0.1307433694601059.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'out_proj', 'kernel') has PT grad norm 4.302241802215576 and flax grad norm 3.5949900150299072.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'q_proj', 'bias') has PT grad norm 0.35267373919487 and flax grad norm 0.2899874746799469.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'q_proj', 'kernel') has PT grad norm 2.124950885772705 and flax grad norm 1.7456589937210083.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'v_proj', 'bias') has PT grad norm 0.6246792674064636 and flax grad norm 0.5248806476593018.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'v_proj', 'kernel') has PT grad norm 2.8608758449554443 and flax grad norm 2.3797028064727783.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.5724965929985046 and flax grad norm 0.47050777077674866.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 9.919981956481934 and flax grad norm 8.047381401062012.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.10857659578323364 and flax grad norm 0.0896400511264801.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 4.873873233795166 and flax grad norm 4.028935432434082.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'final_layer_norm', 'bias') has PT grad norm 3.85205078125 and flax grad norm 3.218515157699585.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'final_layer_norm', 'scale') has PT grad norm 7.600725173950195 and flax grad norm 6.113032817840576.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'layer_norm', 'bias') has PT grad norm 3.6638898849487305 and flax grad norm 3.0701546669006348.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'layer_norm', 'scale') has PT grad norm 3.643948793411255 and flax grad norm 2.871899366378784.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'k_proj', 'kernel') has PT grad norm 2.840862512588501 and flax grad norm 2.2892277240753174.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'out_proj', 'bias') has PT grad norm 0.08615677803754807 and flax grad norm 0.07123836874961853.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'out_proj', 'kernel') has PT grad norm 3.1630890369415283 and flax grad norm 2.5854949951171875.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'q_proj', 'bias') has PT grad norm 1.048926830291748 and flax grad norm 0.8321863412857056.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'q_proj', 'kernel') has PT grad norm 7.590689659118652 and flax grad norm 5.997287273406982.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'v_proj', 'bias') has PT grad norm 0.4387647807598114 and flax grad norm 0.36297324299812317.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'v_proj', 'kernel') has PT grad norm 2.233337163925171 and flax grad norm 1.840446949005127.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.8139208555221558 and flax grad norm 1.7515112161636353.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 4.741293907165527 and flax grad norm 4.730079174041748.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'final_layer_norm', 'bias') has PT grad norm 0.7543826699256897 and flax grad norm 0.6341257691383362.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'final_layer_norm', 'scale') has PT grad norm 1.5453143119812012 and flax grad norm 1.3704603910446167.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'layer_norm', 'bias') has PT grad norm 5.081017017364502 and flax grad norm 4.1198296546936035.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'layer_norm', 'scale') has PT grad norm 7.9863457679748535 and flax grad norm 6.85925817489624.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'k_proj', 'kernel') has PT grad norm 1.0193841457366943 and flax grad norm 0.9903597831726074.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'out_proj', 'kernel') has PT grad norm 1.6452890634536743 and flax grad norm 1.6249698400497437.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'q_proj', 'kernel') has PT grad norm 1.4778367280960083 and flax grad norm 1.4236904382705688.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 2.6882011890411377 and flax grad norm 2.6714913845062256.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 8.244036674499512 and flax grad norm 8.186735153198242.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'layer_norm', 'scale') has PT grad norm 0.9385895729064941 and flax grad norm 0.9276376366615295.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'k_proj', 'kernel') has PT grad norm 0.663261353969574 and flax grad norm 0.5334460139274597.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'out_proj', 'bias') has PT grad norm 0.14755094051361084 and flax grad norm 0.1279148906469345.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'out_proj', 'kernel') has PT grad norm 1.586648941040039 and flax grad norm 1.3011891841888428.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'q_proj', 'bias') has PT grad norm 0.1115623265504837 and flax grad norm 0.0898231640458107.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'q_proj', 'kernel') has PT grad norm 0.6169036626815796 and flax grad norm 0.49911993741989136.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'v_proj', 'bias') has PT grad norm 0.5506433844566345 and flax grad norm 0.4641400873661041.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'v_proj', 'kernel') has PT grad norm 1.6424963474273682 and flax grad norm 1.3804289102554321.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.2956838011741638 and flax grad norm 0.2535749673843384.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.5661870241165161 and flax grad norm 1.3055212497711182.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13578824698925018 and flax grad norm 0.12107577919960022.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.328986644744873 and flax grad norm 1.0906822681427002.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'final_layer_norm', 'bias') has PT grad norm 1.6631048917770386 and flax grad norm 1.4365384578704834.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'final_layer_norm', 'scale') has PT grad norm 2.3605599403381348 and flax grad norm 1.9916512966156006.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'layer_norm', 'bias') has PT grad norm 2.31231951713562 and flax grad norm 1.879764199256897.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'layer_norm', 'scale') has PT grad norm 1.6664106845855713 and flax grad norm 1.4309577941894531.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'k_proj', 'kernel') has PT grad norm 0.6977355480194092 and flax grad norm 0.5262407064437866.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'out_proj', 'bias') has PT grad norm 0.14140819013118744 and flax grad norm 0.12497515976428986.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'out_proj', 'kernel') has PT grad norm 1.6186606884002686 and flax grad norm 1.3348255157470703.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'q_proj', 'bias') has PT grad norm 0.1379985511302948 and flax grad norm 0.10545331984758377.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'q_proj', 'kernel') has PT grad norm 0.6723607778549194 and flax grad norm 0.5030402541160583.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'v_proj', 'bias') has PT grad norm 0.539177656173706 and flax grad norm 0.4853034019470215.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'v_proj', 'kernel') has PT grad norm 1.688796043395996 and flax grad norm 1.4400748014450073.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.2725672125816345 and flax grad norm 0.23674872517585754.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.3744730949401855 and flax grad norm 1.1442010402679443.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13616423308849335 and flax grad norm 0.12060566991567612.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.3792834281921387 and flax grad norm 1.1353659629821777.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'final_layer_norm', 'bias') has PT grad norm 1.4603629112243652 and flax grad norm 1.353958249092102.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'final_layer_norm', 'scale') has PT grad norm 1.7097939252853394 and flax grad norm 1.4790565967559814.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'layer_norm', 'bias') has PT grad norm 2.2434799671173096 and flax grad norm 2.0353164672851562.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'layer_norm', 'scale') has PT grad norm 1.9907805919647217 and flax grad norm 1.460874319076538.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'k_proj', 'kernel') has PT grad norm 0.5049619078636169 and flax grad norm 0.47171860933303833.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'out_proj', 'bias') has PT grad norm 0.13857980072498322 and flax grad norm 0.12265857309103012.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'out_proj', 'kernel') has PT grad norm 1.4937728643417358 and flax grad norm 1.2307308912277222.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4980994164943695 and flax grad norm 0.45607683062553406.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'v_proj', 'bias') has PT grad norm 0.570553183555603 and flax grad norm 0.5282402634620667.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'v_proj', 'kernel') has PT grad norm 1.435909628868103 and flax grad norm 1.2354021072387695.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3129764199256897 and flax grad norm 0.26470768451690674.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.5485461950302124 and flax grad norm 1.2776310443878174.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13712096214294434 and flax grad norm 0.11800936609506607.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 2.1302201747894287 and flax grad norm 1.7363253831863403.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'final_layer_norm', 'bias') has PT grad norm 1.6506969928741455 and flax grad norm 1.5052634477615356.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'final_layer_norm', 'scale') has PT grad norm 1.5160757303237915 and flax grad norm 1.3353363275527954.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'layer_norm', 'bias') has PT grad norm 2.3128128051757812 and flax grad norm 2.294436454772949.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'layer_norm', 'scale') has PT grad norm 1.7501083612442017 and flax grad norm 1.4154596328735352.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'k_proj', 'kernel') has PT grad norm 0.8255481719970703 and flax grad norm 0.7696787714958191.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'out_proj', 'bias') has PT grad norm 0.14385086297988892 and flax grad norm 0.12325192987918854.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'out_proj', 'kernel') has PT grad norm 2.26468825340271 and flax grad norm 1.8220525979995728.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'q_proj', 'bias') has PT grad norm 0.12971392273902893 and flax grad norm 0.11721494048833847.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'q_proj', 'kernel') has PT grad norm 0.6909827589988708 and flax grad norm 0.5956977009773254.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'v_proj', 'bias') has PT grad norm 0.5136300325393677 and flax grad norm 0.44002124667167664.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'v_proj', 'kernel') has PT grad norm 1.3328678607940674 and flax grad norm 1.1089813709259033.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.2719024121761322 and flax grad norm 0.22421987354755402.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.5506606101989746 and flax grad norm 1.256251335144043.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.14421305060386658 and flax grad norm 0.12309099733829498.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 13.33632755279541 and flax grad norm 10.606297492980957.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'final_layer_norm', 'bias') has PT grad norm 1.3961037397384644 and flax grad norm 1.158693790435791.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'final_layer_norm', 'scale') has PT grad norm 1.5829676389694214 and flax grad norm 1.2753269672393799.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'layer_norm', 'bias') has PT grad norm 2.1317148208618164 and flax grad norm 1.833290696144104.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'layer_norm', 'scale') has PT grad norm 1.6133346557617188 and flax grad norm 1.43948495388031.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'k_proj', 'kernel') has PT grad norm 0.3956725597381592 and flax grad norm 0.3324633538722992.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'out_proj', 'bias') has PT grad norm 0.14151199162006378 and flax grad norm 0.12126930058002472.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'out_proj', 'kernel') has PT grad norm 1.2502819299697876 and flax grad norm 1.0330287218093872.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'q_proj', 'bias') has PT grad norm 0.11033734679222107 and flax grad norm 0.09343618154525757.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4156510531902313 and flax grad norm 0.3469286262989044.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'v_proj', 'bias') has PT grad norm 0.5149933099746704 and flax grad norm 0.43779632449150085.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'v_proj', 'kernel') has PT grad norm 1.1261574029922485 and flax grad norm 0.9367255568504333.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.2724614143371582 and flax grad norm 0.2271692454814911.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.2208112478256226 and flax grad norm 1.0076512098312378.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.14333057403564453 and flax grad norm 0.12302936613559723.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.3926641941070557 and flax grad norm 1.140440583229065.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'final_layer_norm', 'bias') has PT grad norm 1.5276572704315186 and flax grad norm 1.2784188985824585.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'final_layer_norm', 'scale') has PT grad norm 2.329549789428711 and flax grad norm 1.8466432094573975.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'layer_norm', 'bias') has PT grad norm 2.019887685775757 and flax grad norm 1.7256814241409302.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'layer_norm', 'scale') has PT grad norm 1.8386820554733276 and flax grad norm 1.6620997190475464.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'k_proj', 'kernel') has PT grad norm 0.3077864348888397 and flax grad norm 0.252309113740921.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'out_proj', 'bias') has PT grad norm 0.1579386591911316 and flax grad norm 0.13428713381290436.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'out_proj', 'kernel') has PT grad norm 1.191630482673645 and flax grad norm 0.9859640598297119.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'q_proj', 'bias') has PT grad norm 0.10788789391517639 and flax grad norm 0.0905650332570076.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'q_proj', 'kernel') has PT grad norm 0.34649205207824707 and flax grad norm 0.2840796709060669.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'v_proj', 'bias') has PT grad norm 0.5713379383087158 and flax grad norm 0.4863416254520416.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'v_proj', 'kernel') has PT grad norm 1.139765977859497 and flax grad norm 0.9417921900749207.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 0.8313102722167969 and flax grad norm 0.7146576046943665.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.1538868248462677 and flax grad norm 0.13061857223510742.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 0.6679737567901611 and flax grad norm 0.5670700669288635.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'final_layer_norm', 'bias') has PT grad norm 0.29243603348731995 and flax grad norm 0.26346129179000854.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'final_layer_norm', 'scale') has PT grad norm 0.2991783916950226 and flax grad norm 0.25617069005966187.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'layer_norm', 'bias') has PT grad norm 2.539250135421753 and flax grad norm 2.158928394317627.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'layer_norm', 'scale') has PT grad norm 1.2898962497711182 and flax grad norm 1.0684118270874023.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'k_proj', 'kernel') has PT grad norm 0.3787970542907715 and flax grad norm 0.33227795362472534.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'out_proj', 'bias') has PT grad norm 0.1567663550376892 and flax grad norm 0.1327163577079773.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'out_proj', 'kernel') has PT grad norm 1.3819712400436401 and flax grad norm 1.1543834209442139.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4538426697254181 and flax grad norm 0.3876984119415283.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'v_proj', 'bias') has PT grad norm 0.5985393524169922 and flax grad norm 0.5076965093612671.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'v_proj', 'kernel') has PT grad norm 1.1285979747772217 and flax grad norm 0.9588875770568848.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 0.6804075837135315 and flax grad norm 0.5644658207893372.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.15338021516799927 and flax grad norm 0.1296704113483429.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 0.6627682447433472 and flax grad norm 0.5453035831451416.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'final_layer_norm', 'bias') has PT grad norm 0.13218151032924652 and flax grad norm 0.10950564593076706.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'final_layer_norm', 'scale') has PT grad norm 0.13265924155712128 and flax grad norm 0.10935545712709427.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'layer_norm', 'bias') has PT grad norm 2.7990152835845947 and flax grad norm 2.4249210357666016.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'layer_norm', 'scale') has PT grad norm 1.5304886102676392 and flax grad norm 1.2073512077331543.\n",
      "❌ Layer ('encoder', 'encoder', 'pos_conv_embed', 'conv', 'bias') has PT grad norm 0.1709800362586975 and flax grad norm 0.15208765864372253.\n",
      "❌ Layer ('encoder', 'encoder', 'pos_conv_embed', 'conv', 'weight_g') has PT grad norm 0.8583779335021973 and flax grad norm 0.8099506497383118.\n",
      "❌ Layer ('encoder', 'encoder', 'pos_conv_embed', 'conv', 'weight_v') has PT grad norm 3.212503433227539 and flax grad norm 2.898043394088745.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'conv', 'bias') has PT grad norm 53.999027252197266 and flax grad norm 46.69960021972656.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'conv', 'kernel') has PT grad norm 16.797372817993164 and flax grad norm 14.338088035583496.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'layer_norm', 'bias') has PT grad norm 1.9641953706741333 and flax grad norm 1.8528305292129517.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'layer_norm', 'scale') has PT grad norm 3.293529510498047 and flax grad norm 3.125916004180908.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '1', 'conv', 'bias') has PT grad norm 1.3159266710281372 and flax grad norm 1.208594560623169.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '1', 'conv', 'kernel') has PT grad norm 13.078824996948242 and flax grad norm 12.574493408203125.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '1', 'layer_norm', 'bias') has PT grad norm 1.5428920984268188 and flax grad norm 1.5147638320922852.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '1', 'layer_norm', 'scale') has PT grad norm 1.6445261240005493 and flax grad norm 1.6656911373138428.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '2', 'conv', 'bias') has PT grad norm 0.470126211643219 and flax grad norm 0.4294426143169403.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '2', 'conv', 'kernel') has PT grad norm 4.200284004211426 and flax grad norm 3.9909651279449463.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '2', 'layer_norm', 'bias') has PT grad norm 1.3073735237121582 and flax grad norm 1.255516767501831.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '3', 'conv', 'bias') has PT grad norm 0.38212770223617554 and flax grad norm 0.3362901508808136.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '3', 'conv', 'kernel') has PT grad norm 4.338206768035889 and flax grad norm 3.9861011505126953.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '3', 'layer_norm', 'bias') has PT grad norm 1.6185942888259888 and flax grad norm 1.4730123281478882.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '3', 'layer_norm', 'scale') has PT grad norm 1.8413660526275635 and flax grad norm 1.7024482488632202.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '4', 'conv', 'bias') has PT grad norm 0.4469415545463562 and flax grad norm 0.40836623311042786.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '4', 'conv', 'kernel') has PT grad norm 4.82208776473999 and flax grad norm 4.40128755569458.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '4', 'layer_norm', 'bias') has PT grad norm 1.9140969514846802 and flax grad norm 1.7336702346801758.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '4', 'layer_norm', 'scale') has PT grad norm 2.5290510654449463 and flax grad norm 2.3097593784332275.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '5', 'conv', 'bias') has PT grad norm 0.4880770444869995 and flax grad norm 0.45022156834602356.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '5', 'conv', 'kernel') has PT grad norm 3.9747164249420166 and flax grad norm 3.6576595306396484.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '5', 'layer_norm', 'bias') has PT grad norm 2.1669669151306152 and flax grad norm 1.9492902755737305.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '5', 'layer_norm', 'scale') has PT grad norm 3.4052789211273193 and flax grad norm 3.115356206893921.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '6', 'conv', 'bias') has PT grad norm 0.6659819483757019 and flax grad norm 0.6206995844841003.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '6', 'conv', 'kernel') has PT grad norm 4.915854454040527 and flax grad norm 4.5639190673828125.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '6', 'layer_norm', 'bias') has PT grad norm 4.91267728805542 and flax grad norm 4.279147624969482.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '6', 'layer_norm', 'scale') has PT grad norm 9.107418060302734 and flax grad norm 8.104247093200684.\n",
      "❌ Layer ('encoder', 'feature_projection', 'layer_norm', 'bias') has PT grad norm 0.746501088142395 and flax grad norm 0.6391669511795044.\n",
      "❌ Layer ('encoder', 'feature_projection', 'layer_norm', 'scale') has PT grad norm 1.1390292644500732 and flax grad norm 1.0014723539352417.\n",
      "❌ Layer ('encoder', 'feature_projection', 'projection', 'bias') has PT grad norm 0.22121499478816986 and flax grad norm 0.1966480016708374.\n",
      "❌ Layer ('encoder', 'feature_projection', 'projection', 'kernel') has PT grad norm 7.47062349319458 and flax grad norm 6.552497863769531.\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking gradients match--------------------------\")\n",
    "if len(results_fail) == 0:\n",
    "    print(\"✅ All grads pass\")\n",
    "else:\n",
    "    print(\"\\n\".join(results_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ab13c17-7151-4f10-a9fa-0296914678e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Checking rel gradients match--------------------------\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'embed_positions', 'embedding') has PT grad norm 0.0009617977775633335 and flax grad norm 0.001007080078125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layernorm_embedding', 'scale') has PT grad norm 0.0005227156216278672 and flax grad norm 0.0005459785461425781.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 3.740085324777098e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 5.6070362916216254e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0001576704962644726 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 0.0031112022697925568 and flax grad norm 0.0032482147216796875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 2.3031611817714293e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 3.5919500078307465e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.000444757315563038 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 0.0037318530958145857 and flax grad norm 0.0038909912109375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.00048297044122591615 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.0004706356849055737 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'fc1', 'bias') has PT grad norm 0.0002640218590386212 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'fc1', 'kernel') has PT grad norm 0.003914659842848778 and flax grad norm 0.004062652587890625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'fc2', 'bias') has PT grad norm 0.0003810059861280024 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'fc2', 'kernel') has PT grad norm 0.0030228225514292717 and flax grad norm 0.00313568115234375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'final_layer_norm', 'bias') has PT grad norm 0.0011233228724449873 and flax grad norm 0.001171112060546875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'final_layer_norm', 'scale') has PT grad norm 0.0009734891355037689 and flax grad norm 0.001007080078125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias') has PT grad norm 2.8044976063679172e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel') has PT grad norm 0.0009688898571766913 and flax grad norm 0.0010356903076171875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.00028946585371159017 and flax grad norm 0.0003452301025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel') has PT grad norm 0.0015175605658441782 and flax grad norm 0.0015821456909179688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias') has PT grad norm 0.00010499362542759627 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel') has PT grad norm 0.0008934183861128986 and flax grad norm 0.0009455680847167969.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.0004153798508923501 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel') has PT grad norm 0.0018770891474559903 and flax grad norm 0.001983642578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'bias') has PT grad norm 0.00015797442756593227 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '0', 'self_attn_layer_norm', 'scale') has PT grad norm 0.00017602449224796146 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 3.185018221385505e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 0.00014299941540230066 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0002705826482269913 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 0.004823728464543819 and flax grad norm 0.005008697509765625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 4.7828330025367904e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 0.00010528130951570347 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.0006108622183091938 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 0.005095413886010647 and flax grad norm 0.005290985107421875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.0008279162575490773 and flax grad norm 0.0008459091186523438.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.0008520675473846495 and flax grad norm 0.0008802413940429688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'fc1', 'bias') has PT grad norm 0.0005070572951808572 and flax grad norm 0.0005459785461425781.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'fc1', 'kernel') has PT grad norm 0.00772611889988184 and flax grad norm 0.0080108642578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'fc2', 'bias') has PT grad norm 0.0007143656839616597 and flax grad norm 0.000732421875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'fc2', 'kernel') has PT grad norm 0.007202502805739641 and flax grad norm 0.007457733154296875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'final_layer_norm', 'bias') has PT grad norm 0.0022307331673800945 and flax grad norm 0.002315521240234375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'final_layer_norm', 'scale') has PT grad norm 0.001785012544132769 and flax grad norm 0.0018434524536132812.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias') has PT grad norm 6.921692385919442e-12 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel') has PT grad norm 0.0006103404448367655 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.0006000977591611445 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel') has PT grad norm 0.0013093355810269713 and flax grad norm 0.0013589859008789062.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias') has PT grad norm 6.314019265118986e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel') has PT grad norm 0.0006786587182432413 and flax grad norm 0.000732421875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.0008599572465755045 and flax grad norm 0.0008802413940429688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel') has PT grad norm 0.0017830035649240017 and flax grad norm 0.0018596649169921875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'bias') has PT grad norm 0.0002709675463847816 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '1', 'self_attn_layer_norm', 'scale') has PT grad norm 0.00039250036934390664 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 1.6359459209525085e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'fc1', 'kernel') has PT grad norm 552.0279541015625 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.0165590680344394e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel') has PT grad norm 2.52727858196522e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'q_proj', 'bias') has PT grad norm 1.5384832963150075e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel') has PT grad norm 3.82086788874858e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '10', 'self_attn_layer_norm', 'scale') has PT grad norm 4.301183700561523 and flax grad norm 4.25.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 2.5305797635155614e-07 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 279.05181884765625 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'fc1', 'kernel') has PT grad norm 305.48760986328125 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'k_proj', 'bias') has PT grad norm 2.0574725922983816e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel') has PT grad norm 3.589737218590017e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'q_proj', 'bias') has PT grad norm 5.373528466634525e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel') has PT grad norm 9.379733256764666e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 4.506685302718694e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 0.0003341579285915941 and flax grad norm 0.0003452301025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0006355933146551251 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 0.01140375342220068 and flax grad norm 0.01183319091796875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 9.996992048399989e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 0.00019818365399260074 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.001276095979847014 and flax grad norm 0.0013151168823242188.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 0.010662899352610111 and flax grad norm 0.01107025146484375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.0016549398424103856 and flax grad norm 0.0017261505126953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.0018105546478182077 and flax grad norm 0.0018749237060546875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'fc1', 'bias') has PT grad norm 0.0008665498462505639 and flax grad norm 0.0009136199951171875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'fc1', 'kernel') has PT grad norm 0.013435290195047855 and flax grad norm 0.01392364501953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'fc2', 'bias') has PT grad norm 0.0006541278562508523 and flax grad norm 0.000690460205078125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'fc2', 'kernel') has PT grad norm 0.007116218563169241 and flax grad norm 0.007366180419921875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'final_layer_norm', 'bias') has PT grad norm 0.001725233276374638 and flax grad norm 0.00177764892578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'final_layer_norm', 'scale') has PT grad norm 0.0017650492954999208 and flax grad norm 0.001827239990234375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.6330914051621015e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel') has PT grad norm 0.0007736175321042538 and flax grad norm 0.0008096694946289062.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.0010253244545310736 and flax grad norm 0.001064300537109375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel') has PT grad norm 0.0035945747513324022 and flax grad norm 0.003742218017578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'q_proj', 'bias') has PT grad norm 5.106745447847061e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel') has PT grad norm 0.0006065217894501984 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.0014524805592373013 and flax grad norm 0.0015048980712890625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel') has PT grad norm 0.00649979617446661 and flax grad norm 0.006755828857421875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn_layer_norm', 'bias') has PT grad norm 0.0006358700920827687 and flax grad norm 0.0006461143493652344.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '2', 'self_attn_layer_norm', 'scale') has PT grad norm 0.00122013408690691 and flax grad norm 0.0012683868408203125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 1.504891697368116e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 5.252928895060904e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0003855098912026733 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 0.0069582294672727585 and flax grad norm 0.00717926025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 1.4055840438231826e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 3.4981272619916126e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.0005586237530224025 and flax grad norm 0.0005979537963867188.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 0.004697555676102638 and flax grad norm 0.004852294921875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.0009364872239530087 and flax grad norm 0.0009765625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.0009157647145912051 and flax grad norm 0.0009455680847167969.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'fc1', 'bias') has PT grad norm 0.00017045566346496344 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'fc1', 'kernel') has PT grad norm 0.002625582506880164 and flax grad norm 0.00266265869140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'fc2', 'bias') has PT grad norm 0.00014011967869009823 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.1046415518412012e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel') has PT grad norm 0.0008886693394742906 and flax grad norm 0.0009136199951171875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel') has PT grad norm 0.004965487401932478 and flax grad norm 0.00514984130859375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'q_proj', 'bias') has PT grad norm 7.391737017314881e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel') has PT grad norm 0.0008550950442440808 and flax grad norm 0.0008802413940429688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.0010288483463227749 and flax grad norm 0.001064300537109375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel') has PT grad norm 0.006293753627687693 and flax grad norm 0.006542205810546875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn_layer_norm', 'bias') has PT grad norm 0.00038519641384482384 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '3', 'self_attn_layer_norm', 'scale') has PT grad norm 0.0004377489385660738 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 4.6388944768827045e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 9.567788220010698e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.00015257026825565845 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 2.7510216114023933e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 6.205538375070319e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.0003639751812443137 and flax grad norm 0.0003452301025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.00041411464917473495 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.0004386305226944387 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'fc1', 'bias') has PT grad norm 0.0002191578969359398 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'fc2', 'bias') has PT grad norm 0.00010923683294095099 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'fc2', 'kernel') has PT grad norm 0.007763429544866085 and flax grad norm 0.007678985595703125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'k_proj', 'bias') has PT grad norm 4.557266564987617e-13 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel') has PT grad norm 8.098435500869527e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.0015993136912584305 and flax grad norm 0.0015821456909179688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'q_proj', 'bias') has PT grad norm 2.413846459603519e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel') has PT grad norm 5.345232693798607e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn_layer_norm', 'bias') has PT grad norm 0.00015249331772793084 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '4', 'self_attn_layer_norm', 'scale') has PT grad norm 0.00016941216017585248 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 6.942013630606425e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.0004428474640008062 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 0.006923240143805742 and flax grad norm 0.0068359375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 5.706692536477931e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.0010559158399701118 and flax grad norm 0.0010356903076171875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.0011411583982408047 and flax grad norm 0.0011186599731445312.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.0011166956974193454 and flax grad norm 0.0010919570922851562.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'fc1', 'kernel') has PT grad norm 0.012585051357746124 and flax grad norm 0.01239013671875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'fc2', 'bias') has PT grad norm 0.0002488051832187921 and flax grad norm 0.000244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'fc2', 'kernel') has PT grad norm 0.04023198038339615 and flax grad norm 0.03961181640625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'final_layer_norm', 'bias') has PT grad norm 0.05561332404613495 and flax grad norm 0.054962158203125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.4497721599615598e-12 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel') has PT grad norm 4.0030977288552094e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.005375990178436041 and flax grad norm 0.005313873291015625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'q_proj', 'bias') has PT grad norm 1.3804740319756093e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel') has PT grad norm 2.2529673060489586e-06 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.007710062898695469 and flax grad norm 0.007633209228515625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel') has PT grad norm 0.008569987490773201 and flax grad norm 0.00848388671875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn_layer_norm', 'bias') has PT grad norm 0.00047144826385192573 and flax grad norm 0.00048828125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '5', 'self_attn_layer_norm', 'scale') has PT grad norm 0.0004624506982509047 and flax grad norm 0.0004229545593261719.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 1.698367002589407e-10 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'k_proj', 'kernel') has PT grad norm 0.0015654521994292736 and flax grad norm 0.0015439987182617188.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.001831153640523553 and flax grad norm 0.0018110275268554688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 0.03092658333480358 and flax grad norm 0.030517578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 3.2589152397122234e-05 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'q_proj', 'kernel') has PT grad norm 0.0015756402863189578 and flax grad norm 0.0015439987182617188.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.0037956535816192627 and flax grad norm 0.003749847412109375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 0.031561367213726044 and flax grad norm 0.031158447265625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.009014640003442764 and flax grad norm 0.008880615234375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.007030484266579151 and flax grad norm 0.00691986083984375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'fc1', 'bias') has PT grad norm 0.005374342203140259 and flax grad norm 0.005298614501953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'fc1', 'kernel') has PT grad norm 0.3785642385482788 and flax grad norm 0.37255859375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'fc2', 'bias') has PT grad norm 0.0029524515848606825 and flax grad norm 0.0029201507568359375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'fc2', 'kernel') has PT grad norm 0.32468071579933167 and flax grad norm 0.32080078125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'final_layer_norm', 'bias') has PT grad norm 0.287396639585495 and flax grad norm 0.2841796875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.3996489514264687e-12 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel') has PT grad norm 2.4567978584855155e-07 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.027636829763650894 and flax grad norm 0.02728271484375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel') has PT grad norm 0.011242064647376537 and flax grad norm 0.01108551025390625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'q_proj', 'bias') has PT grad norm 6.858999057612891e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel') has PT grad norm 8.986243216213552e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.032769203186035156 and flax grad norm 0.03240966796875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel') has PT grad norm 0.042277418076992035 and flax grad norm 0.04168701171875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn_layer_norm', 'bias') has PT grad norm 0.0018351204926148057 and flax grad norm 0.0018110275268554688.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '6', 'self_attn_layer_norm', 'scale') has PT grad norm 0.0018175988225266337 and flax grad norm 0.0017938613891601562.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 2.742335469818613e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.012445393949747086 and flax grad norm 0.01229095458984375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 0.1681554615497589 and flax grad norm 0.1658935546875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn', 'q_proj', 'bias') has PT grad norm 0.0005188906216062605 and flax grad norm 0.0005459785461425781.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn', 'v_proj', 'bias') has PT grad norm 0.023994572460651398 and flax grad norm 0.0236968994140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 0.1716882735490799 and flax grad norm 0.16943359375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn_layer_norm', 'bias') has PT grad norm 0.05020512640476227 and flax grad norm 0.049530029296875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'encoder_attn_layer_norm', 'scale') has PT grad norm 0.02769804373383522 and flax grad norm 0.02728271484375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'fc1', 'bias') has PT grad norm 0.030014945194125175 and flax grad norm 0.0294647216796875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'fc1', 'kernel') has PT grad norm 3.3913259506225586 and flax grad norm 3.326171875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'fc2', 'bias') has PT grad norm 0.04252645745873451 and flax grad norm 0.042083740234375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'fc2', 'kernel') has PT grad norm 1.2667726278305054 and flax grad norm 1.251953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'final_layer_norm', 'bias') has PT grad norm 2.5988903045654297 and flax grad norm 2.5703125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'final_layer_norm', 'scale') has PT grad norm 0.7117212414741516 and flax grad norm 0.6953125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'k_proj', 'bias') has PT grad norm 5.571154648720267e-12 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel') has PT grad norm 2.5207384624081897e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'out_proj', 'bias') has PT grad norm 0.1723809391260147 and flax grad norm 0.17041015625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel') has PT grad norm 0.07255661487579346 and flax grad norm 0.0716552734375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'q_proj', 'bias') has PT grad norm 7.539117774513215e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel') has PT grad norm 1.249353775989448e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'v_proj', 'bias') has PT grad norm 0.1603565514087677 and flax grad norm 0.1585693359375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel') has PT grad norm 0.26556843519210815 and flax grad norm 0.261962890625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '7', 'self_attn_layer_norm', 'bias') has PT grad norm 0.012499992735683918 and flax grad norm 0.01235198974609375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 5.697579297248012e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'encoder_attn', 'out_proj', 'bias') has PT grad norm 0.2157812863588333 and flax grad norm 0.213623046875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'encoder_attn', 'out_proj', 'kernel') has PT grad norm 3.3945915699005127 and flax grad norm 3.359375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'encoder_attn', 'v_proj', 'kernel') has PT grad norm 3.0296528339385986 and flax grad norm 2.998046875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'fc1', 'bias') has PT grad norm 0.16402360796928406 and flax grad norm 0.162109375.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'fc1', 'kernel') has PT grad norm 10.76535415649414 and flax grad norm 10.6328125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'final_layer_norm', 'scale') has PT grad norm 1.4433948993682861 and flax grad norm 1.42578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'k_proj', 'bias') has PT grad norm 7.060892842636335e-11 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel') has PT grad norm 4.777335416150663e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'out_proj', 'bias') has PT grad norm 1.487431287765503 and flax grad norm 1.47265625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel') has PT grad norm 0.5076783299446106 and flax grad norm 0.50244140625.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'q_proj', 'bias') has PT grad norm 7.13960390807955e-10 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel') has PT grad norm 2.634178652982655e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel') has PT grad norm 4.9185943603515625 and flax grad norm 4.8671875.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '8', 'self_attn_layer_norm', 'scale') has PT grad norm 0.14916275441646576 and flax grad norm 0.1436767578125.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'encoder_attn', 'k_proj', 'bias') has PT grad norm 8.63976197251759e-08 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'fc1', 'kernel') has PT grad norm 285.70281982421875 and flax grad norm inf.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'k_proj', 'bias') has PT grad norm 1.825268713950834e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel') has PT grad norm 8.711315224729788e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'q_proj', 'bias') has PT grad norm 1.669988258790056e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel') has PT grad norm 7.908984578364198e-09 and flax grad norm 0.0.\n",
      "❌ Layer ('decoder', 'model', 'decoder', 'layers', '9', 'self_attn_layer_norm', 'scale') has PT grad norm 0.7706422209739685 and flax grad norm 0.75732421875.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'k_proj', 'bias') has PT grad norm 4.651916896136754e-08 and flax grad norm 0.0001400008623022586.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'k_proj', 'kernel') has PT grad norm 0.28037506341934204 and flax grad norm 0.23043109476566315.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'out_proj', 'bias') has PT grad norm 0.21775557100772858 and flax grad norm 0.1887841820716858.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'out_proj', 'kernel') has PT grad norm 1.5720638036727905 and flax grad norm 1.33225679397583.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'q_proj', 'bias') has PT grad norm 0.07816984504461288 and flax grad norm 0.0643257275223732.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'q_proj', 'kernel') has PT grad norm 0.24319270253181458 and flax grad norm 0.19909101724624634.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'v_proj', 'bias') has PT grad norm 0.8443093299865723 and flax grad norm 0.72789067029953.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'attention', 'v_proj', 'kernel') has PT grad norm 1.2489112615585327 and flax grad norm 1.059487223625183.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.6354469656944275 and flax grad norm 0.5429699420928955.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 2.809272527694702 and flax grad norm 2.425384044647217.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.1641029566526413 and flax grad norm 0.14287205040454865.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 2.179293632507324 and flax grad norm 1.8549848794937134.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'final_layer_norm', 'bias') has PT grad norm 3.3679847717285156 and flax grad norm 2.9132819175720215.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'final_layer_norm', 'scale') has PT grad norm 4.072488307952881 and flax grad norm 3.450411796569824.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'layer_norm', 'bias') has PT grad norm 2.1898903846740723 and flax grad norm 1.8968398571014404.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '0', 'layer_norm', 'scale') has PT grad norm 2.551924228668213 and flax grad norm 2.1689453125.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'k_proj', 'bias') has PT grad norm 5.876750464040015e-08 and flax grad norm 0.00012676748156081885.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'k_proj', 'kernel') has PT grad norm 0.4768775701522827 and flax grad norm 0.4004431962966919.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'out_proj', 'bias') has PT grad norm 0.17502975463867188 and flax grad norm 0.14971910417079926.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'out_proj', 'kernel') has PT grad norm 1.688887357711792 and flax grad norm 1.4100209474563599.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'q_proj', 'bias') has PT grad norm 0.08073592185974121 and flax grad norm 0.06899707019329071.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4269952178001404 and flax grad norm 0.35969215631484985.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'v_proj', 'bias') has PT grad norm 0.6302688717842102 and flax grad norm 0.5322055816650391.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'attention', 'v_proj', 'kernel') has PT grad norm 1.7588008642196655 and flax grad norm 1.4788819551467896.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3634817600250244 and flax grad norm 0.3069544732570648.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.8312103748321533 and flax grad norm 1.5551892518997192.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.16859814524650574 and flax grad norm 0.14614613354206085.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.5318032503128052 and flax grad norm 1.2774837017059326.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'final_layer_norm', 'bias') has PT grad norm 1.8284684419631958 and flax grad norm 1.5804070234298706.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'final_layer_norm', 'scale') has PT grad norm 1.8860584497451782 and flax grad norm 1.5444484949111938.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'layer_norm', 'bias') has PT grad norm 2.255664825439453 and flax grad norm 1.839105248451233.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '1', 'layer_norm', 'scale') has PT grad norm 2.07712459564209 and flax grad norm 1.7764579057693481.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'k_proj', 'bias') has PT grad norm 2.1705673702854256e-08 and flax grad norm 6.356251105898991e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'k_proj', 'kernel') has PT grad norm 0.5083320140838623 and flax grad norm 0.4085859954357147.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'out_proj', 'bias') has PT grad norm 0.15212149918079376 and flax grad norm 0.12835854291915894.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'out_proj', 'kernel') has PT grad norm 1.3483784198760986 and flax grad norm 1.1139748096466064.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'q_proj', 'bias') has PT grad norm 0.10683486610651016 and flax grad norm 0.08699087798595428.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5139941573143005 and flax grad norm 0.41244879364967346.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'v_proj', 'bias') has PT grad norm 0.5675718188285828 and flax grad norm 0.47268369793891907.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'attention', 'v_proj', 'kernel') has PT grad norm 1.2417538166046143 and flax grad norm 1.0225051641464233.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.022834643721580505 and flax grad norm 0.018518751487135887.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 0.6127423048019409 and flax grad norm 0.5090510249137878.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.15080440044403076 and flax grad norm 0.1272840052843094.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 0.6551542282104492 and flax grad norm 0.533413827419281.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'final_layer_norm', 'bias') has PT grad norm 0.11640147864818573 and flax grad norm 0.09384142607450485.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'final_layer_norm', 'scale') has PT grad norm 0.15305566787719727 and flax grad norm 0.12493584305047989.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'layer_norm', 'bias') has PT grad norm 2.707404613494873 and flax grad norm 2.2474253177642822.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '10', 'layer_norm', 'scale') has PT grad norm 2.6437695026397705 and flax grad norm 2.160187244415283.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'k_proj', 'bias') has PT grad norm 2.0168124947872457e-08 and flax grad norm 6.88992950017564e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'k_proj', 'kernel') has PT grad norm 0.48123008012771606 and flax grad norm 0.39347949624061584.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'out_proj', 'bias') has PT grad norm 0.15392206609249115 and flax grad norm 0.12927395105361938.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'out_proj', 'kernel') has PT grad norm 1.4576045274734497 and flax grad norm 1.2024532556533813.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'q_proj', 'bias') has PT grad norm 0.08779365569353104 and flax grad norm 0.07099972665309906.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4874456822872162 and flax grad norm 0.39279794692993164.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'v_proj', 'bias') has PT grad norm 0.5804410576820374 and flax grad norm 0.4814050793647766.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'attention', 'v_proj', 'kernel') has PT grad norm 1.3450673818588257 and flax grad norm 1.0995910167694092.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.02143106982111931 and flax grad norm 0.017078129574656487.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 0.6541049480438232 and flax grad norm 0.5295652151107788.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.14955636858940125 and flax grad norm 0.1262766718864441.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 0.6312607526779175 and flax grad norm 0.5172069668769836.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'final_layer_norm', 'bias') has PT grad norm 0.1324380487203598 and flax grad norm 0.10319042205810547.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'final_layer_norm', 'scale') has PT grad norm 0.2228212207555771 and flax grad norm 0.18071196973323822.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'layer_norm', 'bias') has PT grad norm 2.924738883972168 and flax grad norm 2.419320583343506.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '11', 'layer_norm', 'scale') has PT grad norm 2.1120128631591797 and flax grad norm 1.691174864768982.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'k_proj', 'bias') has PT grad norm 2.427025513895842e-08 and flax grad norm 6.341699190670624e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'k_proj', 'kernel') has PT grad norm 0.557593047618866 and flax grad norm 0.4594143033027649.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'out_proj', 'bias') has PT grad norm 0.14002037048339844 and flax grad norm 0.11840979009866714.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'out_proj', 'kernel') has PT grad norm 1.6002331972122192 and flax grad norm 1.3205522298812866.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'q_proj', 'bias') has PT grad norm 0.14304043352603912 and flax grad norm 0.1226092129945755.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5401696562767029 and flax grad norm 0.444502055644989.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'v_proj', 'bias') has PT grad norm 0.5773617029190063 and flax grad norm 0.4831111431121826.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'attention', 'v_proj', 'kernel') has PT grad norm 1.5228155851364136 and flax grad norm 1.2537437677383423.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.14216212928295135 and flax grad norm 0.11990304291248322.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.3638768196105957 and flax grad norm 1.1306654214859009.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13446617126464844 and flax grad norm 0.113617442548275.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.4486067295074463 and flax grad norm 1.1995127201080322.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'final_layer_norm', 'bias') has PT grad norm 0.5859650373458862 and flax grad norm 0.4924713671207428.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'final_layer_norm', 'scale') has PT grad norm 0.6069837808609009 and flax grad norm 0.5111620426177979.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'layer_norm', 'bias') has PT grad norm 2.820687770843506 and flax grad norm 2.3652708530426025.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '12', 'layer_norm', 'scale') has PT grad norm 1.4831488132476807 and flax grad norm 1.318660855293274.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'k_proj', 'bias') has PT grad norm 2.230924955881619e-08 and flax grad norm 9.655563917476684e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'k_proj', 'kernel') has PT grad norm 0.5742043852806091 and flax grad norm 0.4651747941970825.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'out_proj', 'bias') has PT grad norm 0.1328696608543396 and flax grad norm 0.11201778799295425.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'out_proj', 'kernel') has PT grad norm 2.052797555923462 and flax grad norm 1.6649430990219116.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'q_proj', 'bias') has PT grad norm 0.1383107304573059 and flax grad norm 0.1122388020157814.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5238744020462036 and flax grad norm 0.4256390631198883.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'v_proj', 'bias') has PT grad norm 0.5416989922523499 and flax grad norm 0.4519098997116089.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'attention', 'v_proj', 'kernel') has PT grad norm 1.5308847427368164 and flax grad norm 1.2519383430480957.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.1259712427854538 and flax grad norm 0.10400155931711197.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.4478530883789062 and flax grad norm 1.1909410953521729.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13247522711753845 and flax grad norm 0.11168470233678818.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.7494932413101196 and flax grad norm 1.4436219930648804.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'final_layer_norm', 'bias') has PT grad norm 0.5598880052566528 and flax grad norm 0.45601779222488403.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'final_layer_norm', 'scale') has PT grad norm 1.0347768068313599 and flax grad norm 0.8765773773193359.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'layer_norm', 'bias') has PT grad norm 2.715980052947998 and flax grad norm 2.275428295135498.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '13', 'layer_norm', 'scale') has PT grad norm 1.9814205169677734 and flax grad norm 1.647208571434021.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'k_proj', 'bias') has PT grad norm 2.1171688402432665e-08 and flax grad norm 4.6337128878803924e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'k_proj', 'kernel') has PT grad norm 0.4469851851463318 and flax grad norm 0.36729735136032104.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'out_proj', 'bias') has PT grad norm 0.13142453134059906 and flax grad norm 0.11055954545736313.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'out_proj', 'kernel') has PT grad norm 1.9143421649932861 and flax grad norm 1.5775372982025146.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'q_proj', 'bias') has PT grad norm 0.10769005864858627 and flax grad norm 0.08807209879159927.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'q_proj', 'kernel') has PT grad norm 0.46542465686798096 and flax grad norm 0.37908872961997986.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'v_proj', 'bias') has PT grad norm 0.4869745075702667 and flax grad norm 0.40675675868988037.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'attention', 'v_proj', 'kernel') has PT grad norm 1.469372034072876 and flax grad norm 1.2017894983291626.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.1434319168329239 and flax grad norm 0.11712374538183212.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.4998465776443481 and flax grad norm 1.2292531728744507.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13336989283561707 and flax grad norm 0.11251944303512573.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.4687641859054565 and flax grad norm 1.2205621004104614.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'final_layer_norm', 'bias') has PT grad norm 0.5628701448440552 and flax grad norm 0.4618908762931824.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'final_layer_norm', 'scale') has PT grad norm 0.7930547595024109 and flax grad norm 0.6670147776603699.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'layer_norm', 'bias') has PT grad norm 2.381314516067505 and flax grad norm 1.9863977432250977.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '14', 'layer_norm', 'scale') has PT grad norm 2.0936431884765625 and flax grad norm 1.8034288883209229.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'k_proj', 'bias') has PT grad norm 1.8209625807230623e-08 and flax grad norm 3.875493348459713e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'k_proj', 'kernel') has PT grad norm 0.4350140690803528 and flax grad norm 0.35675913095474243.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'out_proj', 'bias') has PT grad norm 0.13229776918888092 and flax grad norm 0.11156023293733597.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'out_proj', 'kernel') has PT grad norm 2.2099153995513916 and flax grad norm 1.8171441555023193.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'q_proj', 'bias') has PT grad norm 0.15357331931591034 and flax grad norm 0.12450321018695831.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5409072041511536 and flax grad norm 0.43836838006973267.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'v_proj', 'bias') has PT grad norm 0.49617692828178406 and flax grad norm 0.4134994447231293.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'attention', 'v_proj', 'kernel') has PT grad norm 1.6282873153686523 and flax grad norm 1.3312454223632812.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.1935179978609085 and flax grad norm 0.1578790694475174.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.7321749925613403 and flax grad norm 1.4140604734420776.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13403764367103577 and flax grad norm 0.1131025105714798.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 3.414853096008301 and flax grad norm 2.755664587020874.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'final_layer_norm', 'bias') has PT grad norm 0.8818920850753784 and flax grad norm 0.7221885919570923.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'final_layer_norm', 'scale') has PT grad norm 3.4858107566833496 and flax grad norm 2.865558624267578.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'layer_norm', 'bias') has PT grad norm 2.4670019149780273 and flax grad norm 2.0517497062683105.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '15', 'layer_norm', 'scale') has PT grad norm 1.6560161113739014 and flax grad norm 1.3090153932571411.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'k_proj', 'bias') has PT grad norm 2.9881718432989146e-08 and flax grad norm 6.05465829721652e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'k_proj', 'kernel') has PT grad norm 0.6748400926589966 and flax grad norm 0.5392290353775024.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'out_proj', 'bias') has PT grad norm 0.1350592076778412 and flax grad norm 0.1136399433016777.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'out_proj', 'kernel') has PT grad norm 2.0356130599975586 and flax grad norm 1.6796478033065796.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'q_proj', 'bias') has PT grad norm 0.15481184422969818 and flax grad norm 0.12415021657943726.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'q_proj', 'kernel') has PT grad norm 0.7325260043144226 and flax grad norm 0.5841783285140991.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'v_proj', 'bias') has PT grad norm 0.4935604929924011 and flax grad norm 0.4122646450996399.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'attention', 'v_proj', 'kernel') has PT grad norm 1.6108660697937012 and flax grad norm 1.3239330053329468.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.19127795100212097 and flax grad norm 0.1564301699399948.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.8110522031784058 and flax grad norm 1.479600191116333.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13880515098571777 and flax grad norm 0.11707475036382675.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.9165130853652954 and flax grad norm 1.583343744277954.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'final_layer_norm', 'bias') has PT grad norm 0.9494715929031372 and flax grad norm 0.7837269902229309.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'final_layer_norm', 'scale') has PT grad norm 2.7706851959228516 and flax grad norm 2.182366132736206.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'layer_norm', 'bias') has PT grad norm 2.9434971809387207 and flax grad norm 2.451265811920166.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '16', 'layer_norm', 'scale') has PT grad norm 2.126131296157837 and flax grad norm 1.8108830451965332.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'k_proj', 'bias') has PT grad norm 2.35290809058597e-08 and flax grad norm 6.0486428992589936e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'k_proj', 'kernel') has PT grad norm 0.6838885545730591 and flax grad norm 0.5566033720970154.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'out_proj', 'bias') has PT grad norm 0.14493951201438904 and flax grad norm 0.12204547971487045.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'out_proj', 'kernel') has PT grad norm 2.5677101612091064 and flax grad norm 2.1262154579162598.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'q_proj', 'bias') has PT grad norm 0.1750863939523697 and flax grad norm 0.14122818410396576.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'q_proj', 'kernel') has PT grad norm 0.7184332609176636 and flax grad norm 0.580525815486908.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'v_proj', 'bias') has PT grad norm 0.5682583451271057 and flax grad norm 0.47484153509140015.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'attention', 'v_proj', 'kernel') has PT grad norm 1.827661395072937 and flax grad norm 1.51165771484375.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.42279747128486633 and flax grad norm 0.34819650650024414.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 3.652240037918091 and flax grad norm 2.997344732284546.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.1532972902059555 and flax grad norm 0.130055233836174.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 3.515766143798828 and flax grad norm 2.9617629051208496.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'final_layer_norm', 'bias') has PT grad norm 1.8942078351974487 and flax grad norm 1.5514527559280396.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'final_layer_norm', 'scale') has PT grad norm 7.0014190673828125 and flax grad norm 5.534465312957764.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'layer_norm', 'bias') has PT grad norm 3.3812739849090576 and flax grad norm 2.823530435562134.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '17', 'layer_norm', 'scale') has PT grad norm 1.632197618484497 and flax grad norm 1.3253931999206543.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'k_proj', 'bias') has PT grad norm 4.4978172297760466e-08 and flax grad norm 7.71211925894022e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'k_proj', 'kernel') has PT grad norm 0.8465508222579956 and flax grad norm 0.7023743987083435.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'out_proj', 'bias') has PT grad norm 0.1576486974954605 and flax grad norm 0.13404370844364166.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'out_proj', 'kernel') has PT grad norm 2.6179518699645996 and flax grad norm 2.196810483932495.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'q_proj', 'bias') has PT grad norm 0.19153258204460144 and flax grad norm 0.15700407326221466.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'q_proj', 'kernel') has PT grad norm 0.9289499521255493 and flax grad norm 0.7529858350753784.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'v_proj', 'bias') has PT grad norm 0.6878226399421692 and flax grad norm 0.5863615274429321.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'attention', 'v_proj', 'kernel') has PT grad norm 2.2017531394958496 and flax grad norm 1.8559203147888184.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3930789530277252 and flax grad norm 0.34359827637672424.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 4.36956787109375 and flax grad norm 3.8329172134399414.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.1749834418296814 and flax grad norm 0.15017865598201752.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 3.50451922416687 and flax grad norm 2.948387622833252.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'final_layer_norm', 'bias') has PT grad norm 1.6479648351669312 and flax grad norm 1.4270379543304443.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'final_layer_norm', 'scale') has PT grad norm 8.064411163330078 and flax grad norm 6.882790565490723.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'layer_norm', 'bias') has PT grad norm 3.7293262481689453 and flax grad norm 3.1710524559020996.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '18', 'layer_norm', 'scale') has PT grad norm 3.0567433834075928 and flax grad norm 2.614061117172241.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'k_proj', 'bias') has PT grad norm 3.8322170325955085e-08 and flax grad norm 0.00012944563059136271.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'k_proj', 'kernel') has PT grad norm 1.1159549951553345 and flax grad norm 0.9184103012084961.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'out_proj', 'bias') has PT grad norm 0.1734507828950882 and flax grad norm 0.149231418967247.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'out_proj', 'kernel') has PT grad norm 3.5496184825897217 and flax grad norm 2.996391534805298.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'q_proj', 'bias') has PT grad norm 0.2928359806537628 and flax grad norm 0.23927631974220276.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'q_proj', 'kernel') has PT grad norm 1.4664556980133057 and flax grad norm 1.1979496479034424.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'v_proj', 'bias') has PT grad norm 0.6948534250259399 and flax grad norm 0.5920690298080444.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'attention', 'v_proj', 'kernel') has PT grad norm 2.407099723815918 and flax grad norm 2.0250449180603027.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3426833748817444 and flax grad norm 0.2849460542201996.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 5.176263809204102 and flax grad norm 4.342859745025635.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.15377110242843628 and flax grad norm 0.1299199014902115.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 4.212915897369385 and flax grad norm 3.5380475521087646.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'final_layer_norm', 'bias') has PT grad norm 2.0790393352508545 and flax grad norm 1.780023455619812.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'final_layer_norm', 'scale') has PT grad norm 1.615254282951355 and flax grad norm 1.3672000169754028.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'layer_norm', 'bias') has PT grad norm 4.005133628845215 and flax grad norm 3.3703696727752686.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '19', 'layer_norm', 'scale') has PT grad norm 3.461199998855591 and flax grad norm 2.8454177379608154.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'k_proj', 'bias') has PT grad norm 4.5850764962551693e-08 and flax grad norm 0.00018737763457465917.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'k_proj', 'kernel') has PT grad norm 0.5701814293861389 and flax grad norm 0.47327929735183716.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'out_proj', 'bias') has PT grad norm 0.16848407685756683 and flax grad norm 0.14606811106204987.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'out_proj', 'kernel') has PT grad norm 1.458738923072815 and flax grad norm 1.2041767835617065.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'q_proj', 'bias') has PT grad norm 0.12251641601324081 and flax grad norm 0.10178196430206299.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'q_proj', 'kernel') has PT grad norm 0.5855506062507629 and flax grad norm 0.4898696541786194.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'v_proj', 'bias') has PT grad norm 0.6343364119529724 and flax grad norm 0.542090892791748.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'attention', 'v_proj', 'kernel') has PT grad norm 1.6253340244293213 and flax grad norm 1.3723618984222412.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3385002017021179 and flax grad norm 0.2821136713027954.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.6966241598129272 and flax grad norm 1.4235783815383911.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.15465417504310608 and flax grad norm 0.13468612730503082.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.3911552429199219 and flax grad norm 1.138956904411316.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'final_layer_norm', 'bias') has PT grad norm 2.0150527954101562 and flax grad norm 1.6236039400100708.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'final_layer_norm', 'scale') has PT grad norm 2.837282419204712 and flax grad norm 2.4950859546661377.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'layer_norm', 'bias') has PT grad norm 2.4047434329986572 and flax grad norm 2.033325672149658.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '2', 'layer_norm', 'scale') has PT grad norm 1.931193232536316 and flax grad norm 1.6288844347000122.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'k_proj', 'bias') has PT grad norm 4.7812289238891026e-08 and flax grad norm 8.380645886063576e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'k_proj', 'kernel') has PT grad norm 1.4932148456573486 and flax grad norm 1.2391217947006226.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'out_proj', 'bias') has PT grad norm 0.15450838208198547 and flax grad norm 0.1307433694601059.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'out_proj', 'kernel') has PT grad norm 4.302241802215576 and flax grad norm 3.5949900150299072.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'q_proj', 'bias') has PT grad norm 0.35267373919487 and flax grad norm 0.2899874746799469.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'q_proj', 'kernel') has PT grad norm 2.124950885772705 and flax grad norm 1.7456589937210083.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'v_proj', 'bias') has PT grad norm 0.6246792674064636 and flax grad norm 0.5248806476593018.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'attention', 'v_proj', 'kernel') has PT grad norm 2.8608758449554443 and flax grad norm 2.3797028064727783.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.5724965929985046 and flax grad norm 0.47050777077674866.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 9.919981956481934 and flax grad norm 8.047381401062012.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.10857659578323364 and flax grad norm 0.0896400511264801.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 4.873873233795166 and flax grad norm 4.028935432434082.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'final_layer_norm', 'bias') has PT grad norm 3.85205078125 and flax grad norm 3.218515157699585.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'final_layer_norm', 'scale') has PT grad norm 7.600725173950195 and flax grad norm 6.113032817840576.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'layer_norm', 'bias') has PT grad norm 3.6638898849487305 and flax grad norm 3.0701546669006348.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '20', 'layer_norm', 'scale') has PT grad norm 3.643948793411255 and flax grad norm 2.871899366378784.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'k_proj', 'bias') has PT grad norm 3.1326192129199626e-07 and flax grad norm 0.00047706899931654334.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'k_proj', 'kernel') has PT grad norm 2.840862512588501 and flax grad norm 2.2892277240753174.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'out_proj', 'bias') has PT grad norm 0.08615677803754807 and flax grad norm 0.07123836874961853.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'out_proj', 'kernel') has PT grad norm 3.1630890369415283 and flax grad norm 2.5854949951171875.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'q_proj', 'bias') has PT grad norm 1.048926830291748 and flax grad norm 0.8321863412857056.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'q_proj', 'kernel') has PT grad norm 7.590689659118652 and flax grad norm 5.997287273406982.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'v_proj', 'bias') has PT grad norm 0.4387647807598114 and flax grad norm 0.36297324299812317.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'attention', 'v_proj', 'kernel') has PT grad norm 2.233337163925171 and flax grad norm 1.840446949005127.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.05856064334511757 and flax grad norm 0.05549542233347893.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.8139208555221558 and flax grad norm 1.7515112161636353.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'final_layer_norm', 'bias') has PT grad norm 0.7543826699256897 and flax grad norm 0.6341257691383362.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'final_layer_norm', 'scale') has PT grad norm 1.5453143119812012 and flax grad norm 1.3704603910446167.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'layer_norm', 'bias') has PT grad norm 5.081017017364502 and flax grad norm 4.1198296546936035.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '21', 'layer_norm', 'scale') has PT grad norm 7.9863457679748535 and flax grad norm 6.85925817489624.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'k_proj', 'bias') has PT grad norm 4.904214634393611e-08 and flax grad norm 5.3974676120560616e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'k_proj', 'kernel') has PT grad norm 1.0193841457366943 and flax grad norm 0.9903597831726074.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'out_proj', 'kernel') has PT grad norm 1.6452890634536743 and flax grad norm 1.6249698400497437.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'q_proj', 'bias') has PT grad norm 0.20577362179756165 and flax grad norm 0.19894813001155853.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'attention', 'q_proj', 'kernel') has PT grad norm 1.4778367280960083 and flax grad norm 1.4236904382705688.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'final_layer_norm', 'scale') has PT grad norm 0.6793404221534729 and flax grad norm 0.6710605025291443.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '22', 'layer_norm', 'scale') has PT grad norm 0.9385895729064941 and flax grad norm 0.9276376366615295.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '23', 'attention', 'k_proj', 'bias') has PT grad norm 4.2194997718070226e-08 and flax grad norm 5.1647301006596535e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '23', 'final_layer_norm', 'scale') has PT grad norm 0.20580331981182098 and flax grad norm 0.2110152393579483.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'k_proj', 'bias') has PT grad norm 6.392731677351549e-08 and flax grad norm 0.00021488896163646132.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'k_proj', 'kernel') has PT grad norm 0.663261353969574 and flax grad norm 0.5334460139274597.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'out_proj', 'bias') has PT grad norm 0.14755094051361084 and flax grad norm 0.1279148906469345.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'out_proj', 'kernel') has PT grad norm 1.586648941040039 and flax grad norm 1.3011891841888428.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'q_proj', 'bias') has PT grad norm 0.1115623265504837 and flax grad norm 0.0898231640458107.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'q_proj', 'kernel') has PT grad norm 0.6169036626815796 and flax grad norm 0.49911993741989136.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'v_proj', 'bias') has PT grad norm 0.5506433844566345 and flax grad norm 0.4641400873661041.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'attention', 'v_proj', 'kernel') has PT grad norm 1.6424963474273682 and flax grad norm 1.3804289102554321.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.2956838011741638 and flax grad norm 0.2535749673843384.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.5661870241165161 and flax grad norm 1.3055212497711182.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13578824698925018 and flax grad norm 0.12107577919960022.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.328986644744873 and flax grad norm 1.0906822681427002.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'final_layer_norm', 'bias') has PT grad norm 1.6631048917770386 and flax grad norm 1.4365384578704834.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'final_layer_norm', 'scale') has PT grad norm 2.3605599403381348 and flax grad norm 1.9916512966156006.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'layer_norm', 'bias') has PT grad norm 2.31231951713562 and flax grad norm 1.879764199256897.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '3', 'layer_norm', 'scale') has PT grad norm 1.6664106845855713 and flax grad norm 1.4309577941894531.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'k_proj', 'bias') has PT grad norm 6.08673502711099e-08 and flax grad norm 0.00017631248920224607.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'k_proj', 'kernel') has PT grad norm 0.6977355480194092 and flax grad norm 0.5262407064437866.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'out_proj', 'bias') has PT grad norm 0.14140819013118744 and flax grad norm 0.12497515976428986.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'out_proj', 'kernel') has PT grad norm 1.6186606884002686 and flax grad norm 1.3348255157470703.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'q_proj', 'bias') has PT grad norm 0.1379985511302948 and flax grad norm 0.10545331984758377.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'q_proj', 'kernel') has PT grad norm 0.6723607778549194 and flax grad norm 0.5030402541160583.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'v_proj', 'bias') has PT grad norm 0.539177656173706 and flax grad norm 0.4853034019470215.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'attention', 'v_proj', 'kernel') has PT grad norm 1.688796043395996 and flax grad norm 1.4400748014450073.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.2725672125816345 and flax grad norm 0.23674872517585754.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.3744730949401855 and flax grad norm 1.1442010402679443.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13616423308849335 and flax grad norm 0.12060566991567612.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.3792834281921387 and flax grad norm 1.1353659629821777.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'final_layer_norm', 'bias') has PT grad norm 1.4603629112243652 and flax grad norm 1.353958249092102.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'final_layer_norm', 'scale') has PT grad norm 1.7097939252853394 and flax grad norm 1.4790565967559814.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'layer_norm', 'bias') has PT grad norm 2.2434799671173096 and flax grad norm 2.0353164672851562.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '4', 'layer_norm', 'scale') has PT grad norm 1.9907805919647217 and flax grad norm 1.460874319076538.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'k_proj', 'bias') has PT grad norm 4.105591600023217e-08 and flax grad norm 0.00011699338938342407.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'k_proj', 'kernel') has PT grad norm 0.5049619078636169 and flax grad norm 0.47171860933303833.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'out_proj', 'bias') has PT grad norm 0.13857980072498322 and flax grad norm 0.12265857309103012.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'out_proj', 'kernel') has PT grad norm 1.4937728643417358 and flax grad norm 1.2307308912277222.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'q_proj', 'bias') has PT grad norm 0.1178034096956253 and flax grad norm 0.11360441148281097.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4980994164943695 and flax grad norm 0.45607683062553406.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'v_proj', 'bias') has PT grad norm 0.570553183555603 and flax grad norm 0.5282402634620667.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'attention', 'v_proj', 'kernel') has PT grad norm 1.435909628868103 and flax grad norm 1.2354021072387695.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.3129764199256897 and flax grad norm 0.26470768451690674.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.5485461950302124 and flax grad norm 1.2776310443878174.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.13712096214294434 and flax grad norm 0.11800936609506607.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 2.1302201747894287 and flax grad norm 1.7363253831863403.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'final_layer_norm', 'bias') has PT grad norm 1.6506969928741455 and flax grad norm 1.5052634477615356.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'final_layer_norm', 'scale') has PT grad norm 1.5160757303237915 and flax grad norm 1.3353363275527954.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '5', 'layer_norm', 'scale') has PT grad norm 1.7501083612442017 and flax grad norm 1.4154596328735352.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'k_proj', 'bias') has PT grad norm 2.9147861368983286e-07 and flax grad norm 0.0002616594429127872.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'k_proj', 'kernel') has PT grad norm 0.8255481719970703 and flax grad norm 0.7696787714958191.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'out_proj', 'bias') has PT grad norm 0.14385086297988892 and flax grad norm 0.12325192987918854.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'out_proj', 'kernel') has PT grad norm 2.26468825340271 and flax grad norm 1.8220525979995728.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'q_proj', 'bias') has PT grad norm 0.12971392273902893 and flax grad norm 0.11721494048833847.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'q_proj', 'kernel') has PT grad norm 0.6909827589988708 and flax grad norm 0.5956977009773254.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'v_proj', 'bias') has PT grad norm 0.5136300325393677 and flax grad norm 0.44002124667167664.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'attention', 'v_proj', 'kernel') has PT grad norm 1.3328678607940674 and flax grad norm 1.1089813709259033.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.2719024121761322 and flax grad norm 0.22421987354755402.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.5506606101989746 and flax grad norm 1.256251335144043.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.14421305060386658 and flax grad norm 0.12309099733829498.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 13.33632755279541 and flax grad norm 10.606297492980957.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'final_layer_norm', 'bias') has PT grad norm 1.3961037397384644 and flax grad norm 1.158693790435791.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'final_layer_norm', 'scale') has PT grad norm 1.5829676389694214 and flax grad norm 1.2753269672393799.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'layer_norm', 'bias') has PT grad norm 2.1317148208618164 and flax grad norm 1.833290696144104.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '6', 'layer_norm', 'scale') has PT grad norm 1.6133346557617188 and flax grad norm 1.43948495388031.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'k_proj', 'bias') has PT grad norm 2.146412114711893e-08 and flax grad norm 7.63172865845263e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'k_proj', 'kernel') has PT grad norm 0.3956725597381592 and flax grad norm 0.3324633538722992.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'out_proj', 'bias') has PT grad norm 0.14151199162006378 and flax grad norm 0.12126930058002472.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'out_proj', 'kernel') has PT grad norm 1.2502819299697876 and flax grad norm 1.0330287218093872.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'q_proj', 'bias') has PT grad norm 0.11033734679222107 and flax grad norm 0.09343618154525757.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4156510531902313 and flax grad norm 0.3469286262989044.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'v_proj', 'bias') has PT grad norm 0.5149933099746704 and flax grad norm 0.43779632449150085.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'attention', 'v_proj', 'kernel') has PT grad norm 1.1261574029922485 and flax grad norm 0.9367255568504333.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.2724614143371582 and flax grad norm 0.2271692454814911.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 1.2208112478256226 and flax grad norm 1.0076512098312378.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.14333057403564453 and flax grad norm 0.12302936613559723.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 1.3926641941070557 and flax grad norm 1.140440583229065.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'final_layer_norm', 'bias') has PT grad norm 1.5276572704315186 and flax grad norm 1.2784188985824585.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'final_layer_norm', 'scale') has PT grad norm 2.329549789428711 and flax grad norm 1.8466432094573975.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'layer_norm', 'bias') has PT grad norm 2.019887685775757 and flax grad norm 1.7256814241409302.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '7', 'layer_norm', 'scale') has PT grad norm 1.8386820554733276 and flax grad norm 1.6620997190475464.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'k_proj', 'bias') has PT grad norm 1.9166110476476206e-08 and flax grad norm 4.634015931515023e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'k_proj', 'kernel') has PT grad norm 0.3077864348888397 and flax grad norm 0.252309113740921.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'out_proj', 'bias') has PT grad norm 0.1579386591911316 and flax grad norm 0.13428713381290436.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'out_proj', 'kernel') has PT grad norm 1.191630482673645 and flax grad norm 0.9859640598297119.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'q_proj', 'bias') has PT grad norm 0.10788789391517639 and flax grad norm 0.0905650332570076.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'q_proj', 'kernel') has PT grad norm 0.34649205207824707 and flax grad norm 0.2840796709060669.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'v_proj', 'bias') has PT grad norm 0.5713379383087158 and flax grad norm 0.4863416254520416.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'attention', 'v_proj', 'kernel') has PT grad norm 1.139765977859497 and flax grad norm 0.9417921900749207.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.062266308814287186 and flax grad norm 0.05519472435116768.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 0.8313102722167969 and flax grad norm 0.7146576046943665.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.1538868248462677 and flax grad norm 0.13061857223510742.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 0.6679737567901611 and flax grad norm 0.5670700669288635.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'final_layer_norm', 'bias') has PT grad norm 0.29243603348731995 and flax grad norm 0.26346129179000854.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'final_layer_norm', 'scale') has PT grad norm 0.2991783916950226 and flax grad norm 0.25617069005966187.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'layer_norm', 'bias') has PT grad norm 2.539250135421753 and flax grad norm 2.158928394317627.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '8', 'layer_norm', 'scale') has PT grad norm 1.2898962497711182 and flax grad norm 1.0684118270874023.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'k_proj', 'bias') has PT grad norm 2.0378276843757703e-08 and flax grad norm 3.415643732296303e-05.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'k_proj', 'kernel') has PT grad norm 0.3787970542907715 and flax grad norm 0.33227795362472534.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'out_proj', 'bias') has PT grad norm 0.1567663550376892 and flax grad norm 0.1327163577079773.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'out_proj', 'kernel') has PT grad norm 1.3819712400436401 and flax grad norm 1.1543834209442139.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'q_proj', 'bias') has PT grad norm 0.250324547290802 and flax grad norm 0.24077771604061127.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'q_proj', 'kernel') has PT grad norm 0.4538426697254181 and flax grad norm 0.3876984119415283.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'v_proj', 'bias') has PT grad norm 0.5985393524169922 and flax grad norm 0.5076965093612671.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'attention', 'v_proj', 'kernel') has PT grad norm 1.1285979747772217 and flax grad norm 0.9588875770568848.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'feed_forward', 'intermediate_dense', 'bias') has PT grad norm 0.031135477125644684 and flax grad norm 0.026384996250271797.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'feed_forward', 'intermediate_dense', 'kernel') has PT grad norm 0.6804075837135315 and flax grad norm 0.5644658207893372.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'feed_forward', 'output_dense', 'bias') has PT grad norm 0.15338021516799927 and flax grad norm 0.1296704113483429.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'feed_forward', 'output_dense', 'kernel') has PT grad norm 0.6627682447433472 and flax grad norm 0.5453035831451416.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'final_layer_norm', 'bias') has PT grad norm 0.13218151032924652 and flax grad norm 0.10950564593076706.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'final_layer_norm', 'scale') has PT grad norm 0.13265924155712128 and flax grad norm 0.10935545712709427.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'layer_norm', 'bias') has PT grad norm 2.7990152835845947 and flax grad norm 2.4249210357666016.\n",
      "❌ Layer ('encoder', 'encoder', 'layers', '9', 'layer_norm', 'scale') has PT grad norm 1.5304886102676392 and flax grad norm 1.2073512077331543.\n",
      "❌ Layer ('encoder', 'encoder', 'pos_conv_embed', 'conv', 'bias') has PT grad norm 0.1709800362586975 and flax grad norm 0.15208765864372253.\n",
      "❌ Layer ('encoder', 'encoder', 'pos_conv_embed', 'conv', 'weight_g') has PT grad norm 0.8583779335021973 and flax grad norm 0.8099506497383118.\n",
      "❌ Layer ('encoder', 'encoder', 'pos_conv_embed', 'conv', 'weight_v') has PT grad norm 3.212503433227539 and flax grad norm 2.898043394088745.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'conv', 'bias') has PT grad norm 53.999027252197266 and flax grad norm 46.69960021972656.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'conv', 'kernel') has PT grad norm 16.797372817993164 and flax grad norm 14.338088035583496.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'layer_norm', 'bias') has PT grad norm 1.9641953706741333 and flax grad norm 1.8528305292129517.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '0', 'layer_norm', 'scale') has PT grad norm 3.293529510498047 and flax grad norm 3.125916004180908.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '1', 'conv', 'bias') has PT grad norm 1.3159266710281372 and flax grad norm 1.208594560623169.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '1', 'conv', 'kernel') has PT grad norm 13.078824996948242 and flax grad norm 12.574493408203125.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '1', 'layer_norm', 'bias') has PT grad norm 1.5428920984268188 and flax grad norm 1.5147638320922852.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '1', 'layer_norm', 'scale') has PT grad norm 1.6445261240005493 and flax grad norm 1.6656911373138428.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '2', 'conv', 'bias') has PT grad norm 0.470126211643219 and flax grad norm 0.4294426143169403.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '2', 'conv', 'kernel') has PT grad norm 4.200284004211426 and flax grad norm 3.9909651279449463.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '2', 'layer_norm', 'bias') has PT grad norm 1.3073735237121582 and flax grad norm 1.255516767501831.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '3', 'conv', 'bias') has PT grad norm 0.38212770223617554 and flax grad norm 0.3362901508808136.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '3', 'conv', 'kernel') has PT grad norm 4.338206768035889 and flax grad norm 3.9861011505126953.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '3', 'layer_norm', 'bias') has PT grad norm 1.6185942888259888 and flax grad norm 1.4730123281478882.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '3', 'layer_norm', 'scale') has PT grad norm 1.8413660526275635 and flax grad norm 1.7024482488632202.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '4', 'conv', 'bias') has PT grad norm 0.4469415545463562 and flax grad norm 0.40836623311042786.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '4', 'conv', 'kernel') has PT grad norm 4.82208776473999 and flax grad norm 4.40128755569458.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '4', 'layer_norm', 'bias') has PT grad norm 1.9140969514846802 and flax grad norm 1.7336702346801758.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '4', 'layer_norm', 'scale') has PT grad norm 2.5290510654449463 and flax grad norm 2.3097593784332275.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '5', 'conv', 'bias') has PT grad norm 0.4880770444869995 and flax grad norm 0.45022156834602356.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '5', 'conv', 'kernel') has PT grad norm 3.9747164249420166 and flax grad norm 3.6576595306396484.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '5', 'layer_norm', 'bias') has PT grad norm 2.1669669151306152 and flax grad norm 1.9492902755737305.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '5', 'layer_norm', 'scale') has PT grad norm 3.4052789211273193 and flax grad norm 3.115356206893921.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '6', 'conv', 'bias') has PT grad norm 0.6659819483757019 and flax grad norm 0.6206995844841003.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '6', 'conv', 'kernel') has PT grad norm 4.915854454040527 and flax grad norm 4.5639190673828125.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '6', 'layer_norm', 'bias') has PT grad norm 4.91267728805542 and flax grad norm 4.279147624969482.\n",
      "❌ Layer ('encoder', 'feature_extractor', 'conv_layers', '6', 'layer_norm', 'scale') has PT grad norm 9.107418060302734 and flax grad norm 8.104247093200684.\n",
      "❌ Layer ('encoder', 'feature_projection', 'layer_norm', 'bias') has PT grad norm 0.746501088142395 and flax grad norm 0.6391669511795044.\n",
      "❌ Layer ('encoder', 'feature_projection', 'layer_norm', 'scale') has PT grad norm 1.1390292644500732 and flax grad norm 1.0014723539352417.\n",
      "❌ Layer ('encoder', 'feature_projection', 'projection', 'bias') has PT grad norm 0.22121499478816986 and flax grad norm 0.1966480016708374.\n",
      "❌ Layer ('encoder', 'feature_projection', 'projection', 'kernel') has PT grad norm 7.47062349319458 and flax grad norm 6.552497863769531.\n",
      "❌ Layer ('encoder', 'masked_spec_embed') has PT grad norm 0.0 and flax grad norm 0.0.\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------Checking rel gradients match--------------------------\")\n",
    "\n",
    "if len(results_fail_rel) == 0:\n",
    "    print(\"✅ All rel grads pass\")\n",
    "else:\n",
    "    print(\"\\n\".join(results_fail_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee9b5b-a35c-44e8-9c08-7feb0e4e1775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
