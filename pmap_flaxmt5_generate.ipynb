{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61070b96-4d96-401d-85c6-228010cc1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bde01f0-c3a5-459b-8124-28eb266771f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-18 13:02:18.689099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-18 13:02:19.581885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-01-18 13:02:19.582000: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-01-18 13:02:19.582010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, FlaxMT5ForConditionalGeneration\n",
    "import jax\n",
    "from flax import jax_utils\n",
    "from flax.training.common_utils import shard\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20505094-c032-457e-ba5a-c773f3032daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# only using 1 GPU, hence only 1 JAX device available; on a TPU v3-8 you should expect this to return 8 (since we have 8 TPU cores)\n",
    "print(jax.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2034a12-12af-4f14-9066-5074202d5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, params = FlaxMT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\", _do_init=False)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47f27ef-8d94-413b-98e3-1d545a018f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_context = [\"test string\" for i in range(8)]  # change to the prompt(s) you want to test\n",
    "tokenized_context = tokenizer(input_context, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7791528-ef43-488a-91c0-723b2afb135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shard our data across accelerator devices (i.e. data parallelism, [seq_1, seq_2, ..., seq_8] -> TPU_1: [seq_1], TPU_2: [seq_2], ... , TPU_8: [seq_8])\n",
    "input_ids = shard(tokenized_context.input_ids)\n",
    "attention_mask = shard(tokenized_context.attention_mask)\n",
    "\n",
    "# only using 1 GPU, so effectively no data parallelisation across devices; on TPU the data will be sharded across your 8 devices so this should be shape `(8, 1, 3)`\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0691080-796d-4545-8b3b-6d31c783da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy (replicate) the params across your TPU devices\n",
    "params = jax_utils.replicate(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1d451b-d987-4d7f-87f6-b10a66e02503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmap generate (like jit, but replicated across our JAX devices)\n",
    "pmap_generate = jax.pmap(model.generate, \"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b63b0f2-ce13-4dd2-b6d9-1f9339ab834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pmap_generate(input_ids, attention_mask=attention_mask, params=params).sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8a6db47-2017-48b7-b706-1f06634be82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on TPU the returned sequences will be sharded across your 8 devices, so this should be shape `(8, 1, 20)`\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a57f4a6-533b-43e7-a512-fa4d04c70596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to 'unshard' our returned sequences\n",
    "unshard = lambda x: einops.rearrange(x, 'd b ... -> (d b) ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80a2067d-7f5b-4b60-b763-5a09528dc4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = unshard(outputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15a9f578-52c2-4871-84ac-eadd322a5373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<extra_id_0>',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_0>',\n",
       " '<extra_id_0>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we get garbage outputs, but this is understandable since mT5 is not fine-tuned on a downstream task (see https://github.com/huggingface/transformers/issues/8704), replace with your fine-tuned model for applicable results\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c93a8c-7afd-41b3-9c51-c27ae1777519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
