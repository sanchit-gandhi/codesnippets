{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c24147-be90-4d03-9780-f5bee0098637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa727189-798b-474b-a02f-f73c6d4bedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import FlaxWhisperForConditionalGeneration, WhisperProcessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0d1edb8-37f4-4461-b6b4-bdff7e609cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, params = FlaxWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\", _do_init=False)\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33dadc17-5f45-4954-ba1a-aabca879cce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/Users/sanchitgandhi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
      "Loading cached processed dataset at /Users/sanchitgandhi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b/cache-4935437da9c0dd4b.arrow\n"
     ]
    }
   ],
   "source": [
    "librispeech = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "def preprocess(batch):\n",
    "    batch[\"input_features\"] = processor(\n",
    "        batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"np\"\n",
    "    ).input_features[0]\n",
    "    return batch\n",
    "\n",
    "dataset_processed = librispeech.map(preprocess, remove_columns=librispeech.column_names)\n",
    "\n",
    "eval_dataloader = dataset_processed.with_format(\"numpy\").iter(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7168f194-39dd-4574-93f9-5bf9eca9d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(eval_dataloader))\n",
    "decoder_input_ids = np.ones((batch[\"input_features\"].shape[0], 1)) * model.config.decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0322e28e-e4db-4590-ba83-eb620d9c555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward pass\n",
    "logits = model(batch[\"input_features\"], decoder_input_ids=decoder_input_ids, params=params).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e65b349d-b855-446a-bca2-fe3dd1049803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.',\n",
       " \" Nor is Mr. Quilter's manner less interesting than his matter.\",\n",
       " ' He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind.',\n",
       " \" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of rocky Ithaca.\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test generate\n",
    "pred_ids = model.generate(batch[\"input_features\"], params=params, max_new_tokens=64)\n",
    "pred_str = processor.batch_decode(pred_ids.sequences, skip_special_tokens=True)\n",
    "pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "624bf1f2-68e2-45cb-8f4a-3164775afb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FlaxWhisperForConditionalGeneration as FlaxScanRematWhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77fe35c8-186d-49ff-9e0b-0fb039735d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, params = FlaxScanRematWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\", _do_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf0cce24-077b-452b-ab87-d29a1782d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.',\n",
       " \" Nor is Mr. Quilter's manner less interesting than his matter.\",\n",
       " ' He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind.',\n",
       " \" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of rocky Ithaca.\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model structure is entirely equivalent to the original model -> check we get the same outputs\n",
    "# test forward pass\n",
    "new_logits = model(batch[\"input_features\"], decoder_input_ids=decoder_input_ids, params=params).logits\n",
    "print(\"Max diff in logits: \", np.max(np.abs(new_logits - logits)))\n",
    "\n",
    "# test generate\n",
    "pred_ids = model.generate(batch[\"input_features\"], params=params, max_new_tokens=64)\n",
    "pred_str = processor.batch_decode(pred_ids.sequences, skip_special_tokens=True)\n",
    "pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad3c6fc9-c5c5-4368-9ade-3b2f6787616b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff in logits:  1.5258789e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.',\n",
       " \" Nor is Mr. Quilter's manner less interesting than his matter.\",\n",
       " ' He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind.',\n",
       " \" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of rocky Ithaca.\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable gradient checkpointing -> check we get the same outputs\n",
    "model.enable_gradient_checkpointing()\n",
    "\n",
    "# test forward pass\n",
    "new_logits = model(batch[\"input_features\"], decoder_input_ids=decoder_input_ids, params=params).logits\n",
    "print(\"Max diff in logits: \", np.max(np.abs(new_logits - logits)))\n",
    "\n",
    "# test generate\n",
    "pred_ids = model.generate(batch[\"input_features\"], params=params, max_new_tokens=64)\n",
    "pred_str = processor.batch_decode(pred_ids.sequences, skip_special_tokens=True)\n",
    "pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de3b5ad5-d66e-4eff-a068-90403af668fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff in logits:  1.6212463e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.',\n",
       " \" Nor is Mr. Quilter's manner less interesting than his matter.\",\n",
       " ' He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind.',\n",
       " \" He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can discover in it but little of rocky Ithaca.\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable scan -> check we get the same outputs\n",
    "model.enable_scan()  # to enable scan in the nn.Module\n",
    "params = model.convert_unroll_to_scan(params) # to convert the unrolled params to scan\n",
    "\n",
    "# test forward pass\n",
    "new_logits = model(batch[\"input_features\"], decoder_input_ids=decoder_input_ids, params=params).logits\n",
    "print(\"Max diff in logits: \", np.max(np.abs(new_logits - logits)))\n",
    "\n",
    "# test generate\n",
    "pred_ids = model.generate(batch[\"input_features\"], params=params, max_new_tokens=64)\n",
    "pred_str = processor.batch_decode(pred_ids.sequences, skip_special_tokens=True)\n",
    "pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95d389-0020-41d6-a24f-e4e691e37e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
