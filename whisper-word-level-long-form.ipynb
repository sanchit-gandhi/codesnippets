{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415f5d87-59d8-4199-a653-15bcde7b5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanchitgandhi/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, set_seed\n",
    "from datasets import load_dataset, Audio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be633b65-a27e-49d5-bf8b-04f3c2e9c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77fbdcb-7e85-4ba9-87ef-f0c15a038b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and pre-process a long-form audio input\n",
    "sample = dataset[0][\"audio\"]\n",
    "input_features = processor(\n",
    "    sample[\"array\"], \n",
    "    sampling_rate=sample[\"sampling_rate\"],\n",
    "    padding=True,\n",
    "    truncation=False,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ").input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51f7b79-2b00-4953-935e-e5810715f4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel. Nor is Mr. Quilter's manner less interesting than his matter. He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similes drawn from eating and its results occur most readily to the mind. He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can discover in it but little of rocky Ithaca. Linnell's pictures are a sort of up-gards and atom paintings, and Mason's exquisite idles are as national as a jingo poem. Mr. Burkett Foster's landscapes smile at one much in the same way that Mr. Carker used to flash his teeth. Mr. John Collier gives his sitter a cheerful slap in the back. before he says like a shampooer and a Turkish bath. Next man.\"]\n"
     ]
    }
   ],
   "source": [
    "# long-form generation (note we don't use fallback here for simplicity)\n",
    "generate_outputs = model.generate(\n",
    "    input_features,\n",
    "    return_timestamps=True,\n",
    "    return_token_timestamps=True,\n",
    "    output_scores=True,\n",
    "    return_segments=True,\n",
    ")\n",
    "\n",
    "# decode text without timestamps\n",
    "pred_text = processor.batch_decode(generate_outputs[\"sequences\"], skip_special_tokens=True)\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0e0a62-02bb-4bf8-8d8b-1ed710de8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for segment in generate_outputs[\"segments\"]:\n",
    "    print(len(segment[0][\"result\"][\"scores\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb75b08c-bd04-46e1-942c-a521d8b72e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 51864])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for segment in generate_outputs[\"segments\"][0]:\n",
    "    for x in segment[\"result\"][\"scores\"]:\n",
    "        scores.append(x.unsqueeze(0))\n",
    "scores = torch.cat(scores, dim=0)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8abb76e5-0b6c-4ee8-9ee0-4cd5ac00ceb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51864])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a73d191-ee8a-4b0c-9b0f-86c4bc908639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_probabilities(generate_outputs):\n",
    "    # Strip off the BOS token; we have no scores for this token\n",
    "    predicted_ids = generate_outputs[\"sequences\"][:, 1:]\n",
    "\n",
    "    # Get the probability for each predicted token\n",
    "    scores = []\n",
    "    for segment in generate_outputs[\"segments\"]:\n",
    "        scores.extend([x.unsqueeze(0) for x in segment[0][\"result\"][\"scores\"]])\n",
    "    scores = torch.cat(scores, dim=0)\n",
    "    scores = scores.permute([1, 0, 2])\n",
    "    probabilities = scores.softmax(dim=-1)\n",
    "    token_probs = torch.gather(probabilities, 2, predicted_ids.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "    # There is no score for the first token, so set this to 1.0\n",
    "    ones = torch.ones((predicted_ids.shape[0], 1))\n",
    "    token_probs = torch.cat([ones, token_probs], dim=-1)\n",
    "    return token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c07e09b-7c64-4fb8-8dfe-6048ed8153d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m token_probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m token_probabilities\n",
      "Cell \u001b[0;32mIn[46], line 10\u001b[0m, in \u001b[0;36mget_token_probabilities\u001b[0;34m(generate_outputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     scores\u001b[38;5;241m.\u001b[39mextend([x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m segment[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      9\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m token_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(probabilities, \u001b[38;5;241m2\u001b[39m, predicted_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "token_probabilities = get_token_probabilities(generate_outputs)\n",
    "token_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77134c14-32e0-4748-89f7-6968bfb77000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(outputs, skip_special_tokens=False):\n",
    "    combined = []\n",
    "    for batch_idx in range(len(outputs[\"sequences\"])):\n",
    "        combined.append([\n",
    "            (\n",
    "                word,\n",
    "                token_id, \n",
    "                timestamp.item(),\n",
    "                probability.item(),\n",
    "            )\n",
    "            for (word, token_id, timestamp, probability) in zip(\n",
    "                processor.batch_decode(outputs[\"sequences\"][batch_idx].squeeze().tolist()), \n",
    "                outputs[\"sequences\"][batch_idx].tolist(), \n",
    "                outputs[\"token_timestamps\"][batch_idx], \n",
    "                get_token_probabilities(outputs)[batch_idx],\n",
    "            )\n",
    "            if not skip_special_tokens or token_id < model.config.eos_token_id\n",
    "        ])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b79311-1128-4b3d-a5b5-5e7841ad26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = combine_results(generate_outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19475f11-bdf8-4a53-853b-2382e3612d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(' Mr', 1770, 0.0, 0.9268808364868164),\n",
       "  ('.', 13, 0.8600000143051147, 0.9680864214897156),\n",
       "  (' Qu', 2264, 1.0199999809265137, 0.7532929182052612),\n",
       "  ('il', 346, 1.0199999809265137, 0.9181373715400696),\n",
       "  ('ter', 353, 1.0800000429153442, 0.9928463101387024),\n",
       "  (' is', 318, 1.2400000095367432, 0.991145670413971),\n",
       "  (' the', 262, 1.4800000190734863, 0.9941967725753784),\n",
       "  (' apostle', 46329, 1.6799999475479126, 0.8014128804206848),\n",
       "  (' of', 286, 2.0799999237060547, 0.9975734353065491),\n",
       "  (' the', 262, 2.359999895095825, 0.9960063099861145),\n",
       "  (' middle', 3504, 2.5, 0.7688519954681396),\n",
       "  (' classes', 6097, 2.700000047683716, 0.9324130415916443),\n",
       "  (',', 11, 3.200000047683716, 0.5493204593658447),\n",
       "  (' and', 290, 3.4000000953674316, 0.9934113621711731),\n",
       "  (' we', 356, 3.559999942779541, 0.9978528022766113),\n",
       "  (' are', 389, 3.700000047683716, 0.7633094191551208),\n",
       "  (' glad', 9675, 3.819999933242798, 0.9978243112564087),\n",
       "  (' to', 284, 4.099999904632568, 0.9964914917945862),\n",
       "  (' welcome', 7062, 4.320000171661377, 0.998943030834198),\n",
       "  (' his', 465, 4.639999866485596, 0.9734256863594055),\n",
       "  (' gospel', 21443, 4.940000057220459, 0.6287229061126709),\n",
       "  ('.', 13, 5.420000076293945, 0.8823179602622986)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str, token, start, prob\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c711226-6443-4929-bb13-5b0c5d25322a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
