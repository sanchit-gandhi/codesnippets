{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386082c8-3e1b-4252-92f7-893e47e0e830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import WhisperConfig, WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import subprocess as sp\n",
    "import os\n",
    "import sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa3df3e-c845-43bd-bca1-e2f583b0fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_id = \"openai/whisper-tiny.en\"\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843647b0-3de8-452a-a966-48335cce75e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/sanchit_huggingface_co/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b/cache-471c43302a97ea10.arrow\n"
     ]
    }
   ],
   "source": [
    "librispeech = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "def preprocess(batch):    \n",
    "    batch[\"input_features\"] = processor(batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features[0]\n",
    "    return batch\n",
    "\n",
    "dataset_processed = librispeech.map(preprocess, remove_columns=librispeech.column_names)\n",
    "\n",
    "dataloader = DataLoader(dataset_processed.with_format(\"torch\"), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0e2afa-9c41-4faa-83cd-f718524b48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Python equivalent of nvidia-smi, copied from https://stackoverflow.com/a/67722676\n",
    "    and verified as being equivalent ✅\n",
    "    \"\"\"\n",
    "    output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "    \n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "    \n",
    "    try:\n",
    "        memory_use_info = output_to_list(sp.check_output(COMMAND.split(),stderr=sp.STDOUT))[1:]\n",
    "    \n",
    "    except sp.CalledProcessError as e:\n",
    "        raise RuntimeError(\"command '{}' return with error (code {}): {}\".format(e.cmd, e.returncode, e.output))\n",
    "    \n",
    "    memory_use_values = [int(x.split()[0]) for i, x in enumerate(memory_use_info)]\n",
    "    return memory_use_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd6ec496-69cd-4db1-9aa1-bd06735c64fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 73/73 [00:08<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  8.990428924560547\n",
      "VRAM:  1381\n"
     ]
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_id, use_cache=True)\n",
    "model.to(\"cuda\")\n",
    "model.half()\n",
    "\n",
    "start = time.time()\n",
    "for batch in tqdm(dataloader):\n",
    "    predicted_ids = model.generate(batch[\"input_features\"].to(\"cuda\").half(), max_new_tokens=128, use_cache=True)\n",
    "runtime = time.time() - start\n",
    "\n",
    "print(\"Runtime: \", runtime)\n",
    "print(\"VRAM: \", get_gpu_memory()[0])\n",
    "\n",
    "# if we don't delete and re-load the model the GPU use is lower the second time round: warm-up effects?\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eef05a31-4938-41a1-b5a3-446ea9bbe2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 73/73 [00:11<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime:  11.993675231933594\n",
      "VRAM:  1381\n"
     ]
    }
   ],
   "source": [
    "# repeat experiment - this time disable cache during generation\n",
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_id, use_cache=False)\n",
    "model.to(\"cuda\")\n",
    "model.half()\n",
    "\n",
    "start = time.time()\n",
    "for batch in tqdm(dataloader):\n",
    "    predicted_ids = model.generate(batch[\"input_features\"].to(\"cuda\").half(), max_new_tokens=128, use_cache=False)\n",
    "runtime = time.time() - start\n",
    "\n",
    "print(\"Runtime: \", runtime)\n",
    "print(\"VRAM: \", get_gpu_memory()[0])\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ac7af-a119-46a3-9897-285b2e8d9341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
