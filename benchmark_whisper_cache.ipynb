{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386082c8-3e1b-4252-92f7-893e47e0e830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import WhisperConfig, WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import subprocess as sp\n",
    "import os\n",
    "import sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0be0c10-666c-43f2-bc51-9fd105530048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/sanchit_huggingface_co/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "librispeech = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843647b0-3de8-452a-a966-48335cce75e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sanchit_huggingface_co/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b/cache-471c43302a97ea10.arrow\n"
     ]
    }
   ],
   "source": [
    "# processors/tokenizers are the same for all models\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "def preprocess(batch):    \n",
    "    batch[\"input_features\"] = processor(batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features[0]\n",
    "    return batch\n",
    "\n",
    "dataset_processed = librispeech.map(preprocess, remove_columns=librispeech.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb8095d-9407-4039-bf73-d75ee2c7c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    \"\"\"\n",
    "\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "901f4798-7b99-47a3-8875-7bd75ec7d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8b1391-ccb8-4496-9fdd-52dff1a17b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset_processed.with_format(\"torch\"), batch_size=1, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a0e2afa-9c41-4faa-83cd-f718524b48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"Python equivalent of nvidia-smi, copied from https://stackoverflow.com/a/67722676\"\"\"\n",
    "    output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "    \n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "    \n",
    "    try:\n",
    "        memory_use_info = output_to_list(sp.check_output(COMMAND.split(),stderr=sp.STDOUT))[1:]\n",
    "    \n",
    "    except sp.CalledProcessError as e:\n",
    "        raise RuntimeError(\"command '{}' return with error (code {}): {}\".format(e.cmd, e.returncode, e.output))\n",
    "    \n",
    "    memory_use_values = [int(x.split()[0]) for i, x in enumerate(memory_use_info)]\n",
    "    return memory_use_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0ac8af-0870-422b-b76f-838824bef166",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_checkpoints = [\"tiny.en\", \"base.en\"]\n",
    "runtime_results = {checkpoint: [] for checkpoint in whisper_checkpoints}\n",
    "vram_results = {checkpoint: [] for checkpoint in whisper_checkpoints}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6ec496-69cd-4db1-9aa1-bd06735c64fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== tiny.en ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 73/73 [00:09<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== base.en ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 73/73 [00:11<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for checkpoint in whisper_checkpoints:\n",
    "    print(50*\"=\", checkpoint, 50*\"=\")\n",
    "    checkpoint_id = f\"openai/whisper-{checkpoint}\"\n",
    "\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(checkpoint_id)\n",
    "    model.to(\"cuda\")\n",
    "    model.half()\n",
    "\n",
    "    start = time.time()\n",
    "    for batch in tqdm(dataloader):\n",
    "        predicted_ids = model.generate(batch[\"input_features\"].to(\"cuda\").half(), max_new_tokens=128, use_cache=True)\n",
    "    runtime = time.time() - start\n",
    "\n",
    "    runtime_results[checkpoint] = runtime\n",
    "    vram_results[checkpoint] = get_gpu_memory()[0]\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1790debc-834d-4272-9c23-9fe6223afb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny.en': 9.957613468170166, 'base.en': 11.280346632003784}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f781bd-3b73-4418-a285-c699f8d92a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny.en': 2760, 'base.en': 2902}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vram_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eef05a31-4938-41a1-b5a3-446ea9bbe2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== tiny.en ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 73/73 [00:12<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== base.en ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 73/73 [00:18<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# repeat experiment - this time disable cache during generation\n",
    "for checkpoint in whisper_checkpoints:\n",
    "    print(50*\"=\", checkpoint, 50*\"=\")\n",
    "    checkpoint_id = f\"openai/whisper-{checkpoint}\"\n",
    "\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(checkpoint_id)\n",
    "    model.to(\"cuda\")\n",
    "    model.half()\n",
    "\n",
    "    start = time.time()\n",
    "    for batch in tqdm(dataloader):\n",
    "        predicted_ids = model.generate(batch[\"input_features\"].to(\"cuda\").half(), max_new_tokens=128, use_cache=False)\n",
    "    runtime = time.time() - start\n",
    "\n",
    "    runtime_results[checkpoint] = runtime\n",
    "    vram_results[checkpoint] = get_gpu_memory()[0]\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86f02193-8897-42fe-a1a6-50163f131dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny.en': 12.600297212600708, 'base.en': 18.400490760803223}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed9eab4-3ad7-4e1b-ab8b-eb5e621abcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny.en': 2760, 'base.en': 2902}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vram_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b503667-3139-4764-8148-4865d5e5b03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
