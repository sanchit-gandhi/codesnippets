{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper JAX âš¡ï¸\n",
    "\n",
    "This Kaggle notebook demonstratese how to run Whisper JAX on a TPU v3-8. Whisper JAX is a highly optimised JAX implementation of the Whisper model by OpenAI, largely built on the ðŸ¤— Hugging Face Transformers Whisper implementation. Compared to OpenAI's PyTorch code, Whisper JAX runs over **70x faster**, making it the fastest Whisper implementation available.\n",
    "\n",
    "The Whisper JAX model is also running as a [demo](https://huggingface.co/spaces/sanchit-gandhi/whisper-jax) on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install --quiet git+https://github.com/sanchit-gandhi/whisper-jax.git datasets soundfile librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's connect to a TPU! Open up the settings menu in the Notebook editor, and select â€˜TPU v3-8â€™ in the Accelerator menu. Refer to the guide [Introducing TPUs to Kaggle](https://www.kaggle.com/product-feedback/129828) for more information on choosing a TPU in Kaggle.\n",
    "\n",
    "We then need to some pre set-up to register JAX our TPU. Note that this step is note required for Cloud TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from jax.config import config\n",
    "    \n",
    "if \"TPU_NAME\" in os.environ and \"KAGGLE_DATA_PROXY_TOKEN\" in os.environ:\n",
    "    use_tpu = True\n",
    "\n",
    "    if \"TPU_DRIVER_MODE\" not in globals():\n",
    "        url = \"http:\" + os.environ[\"TPU_NAME\"].split(\":\")[1] + \":8475/requestversion/tpu_driver_nightly\"\n",
    "        resp = requests.post(url)\n",
    "        TPU_DRIVER_MODE = 1\n",
    "\n",
    "    config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
    "    config.FLAGS.jax_backend_target = os.environ[\"TPU_NAME\"]\n",
    "\n",
    "    # Enforce bfloat16 multiplication\n",
    "    config.update(\"jax_default_matmul_precision\", \"bfloat16\")\n",
    "    print(\"Registered (Kaggle) TPU:\", config.FLAGS.jax_backend_target)\n",
    "else:\n",
    "    use_tpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we've been assigned a TPU. Run the following to see the TPU devices we have available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've got 8 TPU devices packaged into one overall accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pipeline\n",
    "\n",
    "The recommended way of running Whisper JAX is through the [`FlaxWhisperPipline`](https://github.com/sanchit-gandhi/whisper-jax/blob/main/whisper_jax/pipeline.py#L57) class. This class handles all the necessary pre- and post-processing for the model, as well as wrapping the generate method for data parallelism across all available accelerator devices.\n",
    "\n",
    "Whisper JAX makes use of JAX's [`pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html) function for data parallelism across GPU/TPU devices. This function is Just In Time (JIT) compiled the first time it is called. Thereafter, the function will be cached, enabling it to be run in super-fast time.\n",
    "\n",
    "\n",
    "Let's load the large-v2 model in bfloat16 (half-precision). Using half-precision will speed-up the computation quite considerably by storing intermediate tensors in half-precision. There is no change to the precision of the model weights.\n",
    "\n",
    "We'll also make use of _batching_ for single audio inputs: the audio is first chunked into 30 second segments, and then chunks dispatched to the model to be transcribed in parallel. By batching an audio input and transcribing it in parallel, we get a ~10x speed-up compared to transcribing the audio samples sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tcmalloc: large alloc 6173270016 bytes == 0x10c2c2000 @  0x7fd11ee7f680 0x7fd11eea0824 0x5fa131 0x649f21 0x5c4f26 0x4f30be 0x64ec18 0x5050d3 0x56bbdf 0x569cea 0x50b2b0 0x56cbd1 0x569cea 0x5f6a13 0x59c757 0x5f6fbf 0x5715a2 0x569cea 0x68e7b7 0x601174 0x5c52f0 0x56b9fd 0x500a78 0x56d3fd 0x500a78 0x56d3fd 0x500a78 0x5047d6 0x56bbdf 0x5f6836 0x56b9fd\n"
     ]
    }
   ],
   "source": [
    "from whisper_jax import FlaxWhisperPipline\n",
    "import jax.numpy as jnp\n",
    "\n",
    "pipeline = FlaxWhisperPipline(\"openai/whisper-large-v2\", dtype=jnp.bfloat16, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then initialise a compilation cache, which will speed-up the compilation time if we close our kernel and want to compile the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized persistent compilation cache at ./jax_cache\n"
     ]
    }
   ],
   "source": [
    "from jax.experimental.compilation_cache import compilation_cache as cc\n",
    "\n",
    "cc.initialize_cache(\"./jax_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¶ Load an audio file\n",
    "\n",
    "Let's load up a long audio file for our tests. We provide 5 and 30 mins audio files created by contatenating consecutive sample of the [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr) corpus, which we can load in one line through Hugging Face Datastes' [`load_dataset`](https://huggingface.co/docs/datasets/loading#load) function. Note that you can also pass in any `.mp3`, `.wav` or `.flac` audio file directly to the Whisper JAX pipeline, and it will take care of loading the audio file for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/sanchitgandhi/.cache/huggingface/datasets/sanchit-gandhi___parquet/sanchit-gandhi--whisper-jax-test-files-95479fe55e88baac/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset(\"sanchit-gandhi/whisper-jax-test-files\", split=\"train\")\n",
    "audio = test_dataset[0][\"audio\"]  # load the first sample (5 mins) and get the audio array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a listen to the audio file that we've loaded, you'll see that it's approximately 5 mins long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(audio[\"array\"], rate=audio[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "Now we're ready to transcribe! We'll need to compile the `pmap` function the first time we use it. You can expect compilation to take ~2 minutes on a TPU v3-8 with a batch size of 16. Enough time to grab a coffee â˜•ï¸\n",
    "\n",
    "Thereafter, we can use our cached `pmap` function, which you'll see is amazingly fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 7min 53s, total: 12min 12s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "# JIT compile the forward call - slow, but we only do once\n",
    "%time text = pipeline(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 39.5 s, total: 52.2 s\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "# used cached function thereafter - super fast!\n",
    "%time text = pipeline(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" Chapter 16. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came, I to agree to whatever Marguerite wished, Marguerite to be unable to live apart from me. It was the day after the evening when she came to see me that I sent her Manon Lescate. From that time, seeing that I could not change my mistress's life, I changed my own. I wished above all not to leave myself time to think over the position I had accepted, for, in spite of myself, it was a great distress to me. Thus my life, generally so calm, assumed all at once an appearance of noise and disorder. Never believe, however disinterested the love of a kept woman may be, that it will cost one nothing. Nothing is so expensive as their caprices, flowers, boxes at the theatre, suppers, days in the country, which one can never refuse to one's mistress. As I have told you, I had little money. My father was, and still is, Receiver General at sea. He has a great reputation there for loyalty, thanks to which he was able to find the security which he needed in order to attain this position. I came to Paris, studied law, was called to the bar, and, like many other young men, put my diploma in my pocket, and let myself drift, as one so easily does in Paris. My expenses were very moderate, only I used up my year's income in eight months, and spent the four summer months with my father, which practically gave me twelve thousand francs a year, and, in addition, the reputation of a good son. For the rest, not a penny of debt. This, then, was my position when I made the acquaintance of Marguerite. You can well understand that, in spite of myself, my expenses soon increased. Marguerite's nature was very capricious, and, like so many women, she never regarded as a serious expense those thousand and one distractions which made up her life. So, wishing to spend as much time with me as possible, she would write to me in the morning that she would dine with me, not at home, but at some restaurant in Paris, or in the country. I would call for her, and we would dine and go on to the theatre, often having supper as well. Forgive me if I give you all these details, but you will see that they were the cause of what was to follow. What I tell you is a true and simple story, and I leave to it all the naivete of its details, and all the simplicity of its developments. I realized then that as nothing in the world would make me forget my mistress, it was needful for me to find some way of meeting the expenses into which she drew me. Then, too, my love for her had so disturbing an influence upon me, that every moment I spent away from Marguerite was like a year, and that I felt the need of consuming these moments in the fire of some sort of passion, as not to know that I was living them. I began by borrowing five or six thousand francs on my little capital, and with this I took to gambling. Since gambling-houses were destroyed, gambling goes on everywhere. Formerly, when one went to Frascati, one had the chance of making a fortune, one played against money, and if one lost, there was always the consolation of saying that one might have gained. Whereas now, except in the clubs, where there is still a certain rigour in regard to payments, one is almost certain, the moment one gains a considerable sum, not to receive it. You will readily understand why. Gambling is only likely to be carried on by young people very much in need of money, and not possessing the fortune necessary for supporting the life they lead. They gamble, then, and with this result, or else they gain, and then those who lose serve to pay for their horses and mistresses, which is very disagreeable. Debts are contracted, acquaintances begun about a green table, and by quarrels in which life or honour comes to grief, and though one may be an honest man, one finds oneself ruined by very honest men, whose only defect is that they have not two hundred thousand francs a year. I need not tell you of those who cheat at play. I flung myself into this rapid, noisy, and volcanic life, which had formerly terrified me when I thought of it, and which had become for me the necessary complement of my love for Marguerite. What else could I have done? The nights that I did not spend in the Rue d'Antin, if I had spent them alone in my own room, I could not have slept. Jealousy would have kept me awake, and inflamed my blood and my thoughts.\"}\n"
     ]
    }
   ],
   "source": [
    "# let's check our transcription - looks spot on!\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's step it up a notch. Let's try transcribing 30 minutes of audio from the LibriSpeech dataset. We'll first load up and listen to the second sample from our dataset, which corresponds to the 30 min audio file. We'll then pass the audio to the model for transcription, again timing how long the foward pass takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = test_dataset[1][\"audio\"]  # load the second sample (30 mins) and get the audio array\n",
    "\n",
    "Audio(audio[\"array\"], rate=audio[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 3min 56s, total: 5min 8s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "# transcribe using cached function\n",
    "%time text = pipeline(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just 14s to transcribe for 30 mins of audio! That means you could transcribe an entire 2 hour movie in under 1 minute ðŸ¤¯ By increasing the batch size, we could also reduce the transcription time for long audio files further: increasing the batch size by 2x roughly decreases the transcription time by 2x, provided the overall batch size is less than the total audio time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â° Timestamps and more\n",
    "\n",
    "We can also get timestamps from the model by passing `return_timestamps=True`, but this will require a recompilation since we change the signature of the forward pass. \n",
    "\n",
    "The timestamps compilation takes longer than the non-timestamps one. Luckily, because we initialised our compilation cache above, we're not starting from scratch in compiling this time. This is the last compilation we need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 34s, sys: 10min 38s, total: 16min 12s\n",
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext = pipeline(audio, return_timestamps=True)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[43moutputs\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]  \u001B[38;5;66;03m# transcription\u001B[39;00m\n\u001B[1;32m      3\u001B[0m chunks \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunks\u001B[39m\u001B[38;5;124m\"\u001B[39m]  \u001B[38;5;66;03m# transcription + timestamps\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "%time outputs = pipeline(audio, return_timestamps=True)\n",
    "text = outputs[\"text\"]  # transcription\n",
    "chunks = outputs[\"chunks\"]  # transcription + timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown how you can transcibe an audio file in English. The pipeline is also compatible with two further arguments that you can use to control the generation process. It's perfectly fine to omit these if you want speech transcription and the Whisper model to automatically detect which language the audio is in. Otherwise, you can change them depending on your task/language:\n",
    "\n",
    "\n",
    "* `task`: task to use for generation, either `\"transcribe\"` or `\"translate\"`. Defaults to `\"transcribe\"`.\n",
    "* `language`: language token to use for generation, can be either in the form of `\"<|en|>\"`, `\"en\"` or `\"english\"`. Defaults to `None`, meaning the language is automatically inferred from the audio input. Optional, and only relevant if the source audio language is known a-priori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
