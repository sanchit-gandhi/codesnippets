{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Whisper JAX ‚ö°Ô∏è\n\nThis Kaggle notebook demonstratese how to run Whisper JAX on a TPU v3-8. Whisper JAX is a highly optimised JAX implementation of the Whisper model by OpenAI, largely built on the ü§ó Hugging Face Transformers Whisper implementation. Compared to OpenAI's PyTorch code, Whisper JAX runs over **70x faster**, making it the fastest Whisper implementation available.\n\nThe Whisper JAX model is also running as a [demo](https://huggingface.co/spaces/sanchit-gandhi/whisper-jax) on the Hugging Face Hub.","metadata":{}},{"cell_type":"markdown","source":"We'll start by installing the required Python packages:","metadata":{}},{"cell_type":"code","source":"!pip install --quiet jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n!pip install --quiet git+https://github.com/sanchit-gandhi/whisper-jax.git datasets soundfile librosa","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's verify that we've been assigned a TPU. Run the following to see the TPU devices we have available:","metadata":{}},{"cell_type":"code","source":"import jax\njax.devices()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cool! We've got 8 TPU devices packaged into one overall accelerator.","metadata":{}},{"cell_type":"markdown","source":"## Loading the Pipeline\n\nThe recommended way of running Whisper JAX is through the [`FlaxWhisperPipline`](https://github.com/sanchit-gandhi/whisper-jax/blob/main/whisper_jax/pipeline.py#L57) class. This class handles all the necessary pre- and post-processing for the model, as well as wrapping the generate method for data parallelism across all available accelerator devices.\n\nWhisper JAX makes use of JAX's [`pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html) function for data parallelism across GPU/TPU devices. This function is Just In Time (JIT) compiled the first time it is called. Thereafter, the function will be cached, enabling it to be run in super-fast time.\n\n\nLet's load the large-v2 model in bfloat16 (half-precision). Using half-precision will speed-up the computation quite considerably by storing intermediate tensors in half-precision. There is no change to the precision of the model weights.\n\nWe'll also make use of _batching_ for single audio inputs: the audio is first chunked into 30 second segments, and then chunks dispatched to the model to be transcribed in parallel. By batching an audio input and transcribing it in parallel, we get a ~10x speed-up compared to transcribing the audio samples sequentially.","metadata":{}},{"cell_type":"code","source":"from whisper_jax import FlaxWhisperPipline\nimport jax.numpy as jnp\n\n\n# TODO(SG): update to large-v2 checkpoint\npipeline = FlaxWhisperPipline(\"openai/whisper-tiny\", dtype=jnp.bfloat16, batch_size=16)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load an audio file\n\nLet's load up a long audio file for our tests. We provide 5 and 30 mins audio files created by contatenating consecutive sample of the [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr) corpus, which we can load in one line through Hugging Face Datastes' [`load_dataset`](https://huggingface.co/docs/datasets/loading#load) function. Note that you can also pass in any `.mp3`, `.wav` or `.flac` audio file directly to the Whisper JAX pipeline, and it will take care of loading the audio file for you.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ntest_dataset = load_dataset(\"sanchit-gandhi/whisper-jax-test-file\")\naudio = test_dataset[0][\"audio\"]  # load the first sample (5 mins) and get the audio array","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can take a listen to the audio file that we've loaded, you'll see that it's approximately 5 mins long:","metadata":{}},{"cell_type":"code","source":"from IPython.display import Audio\n\nAudio(audio[\"array\"], rate=audio[\"sampling_rate\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run the model\n\nNow we're ready to transcribe! We'll need to compile the `pmap` function the first time we use it. You can expect compilation to take ~2 minutes on a TPU v3-8 with a batch size of 16.\n\nThereafter, we can use our cached `pmap` function, which you'll see is amazingly fast.","metadata":{}},{"cell_type":"code","source":"# JIT compile the forward call - slow, but we only do once\n%time text = pipeline(audio)\n\n# used cached function thereafter - super fast!!\n%time text = pipeline(audio)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's step it up a notch. Let's try transcribing 30 minutes of audio from the LibriSpeech dataset:","metadata":{}},{"cell_type":"code","source":"audio = test_dataset[1][\"audio\"]  # load the second sample (30 mins) and get the audio array\nAudio(audio[\"array\"], rate=audio[\"sampling_rate\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transcribe using cached function\n%time text = pipeline(audio)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just X mins to transcribe for 30 mins of audio! We can also get timestamps from the model by passing `return_timestamps=True`, but this will require a recompilation:","metadata":{}},{"cell_type":"code","source":"text = pipeline(audio, return_timestamps=True)\ntext = outputs[\"text\"]  # transcription\nchunks = outputs[\"chunks\"]  # transcription + timestamps","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've shown how you can transcibe an audio file in English. The pipeline is also compatible with two further arguments that you can use to control the generation process:\n* `task`: Task to use for generation, either `\"transcribe\"` or `\"translate\"`. Defaults to `\"transcribe\"`.\n* `language`: Language token to use for generation, can be either in the form of `\"<|en|>\"`, `\"en\"` or `\"english\"`. Defaults to `None`, meaning the language is automatically inferred from the audio input. Optional, and only relevant if the source audio language is known a-priori.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}