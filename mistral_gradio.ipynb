{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0fa350-9797-4f52-a46d-d5624917d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchitgandhi/hf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import jax_utils\n",
    "from flax.training.common_utils import shard\n",
    "\n",
    "from transformers import FlaxMistralForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2fba06-0a31-4519-8f41-a0ac07492ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"./Mistral-7B-Instruct-v0.2\"\n",
    "max_input_tokens = 256\n",
    "max_new_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56988603-c4be-4090-8dda-ede4b463c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 9942974464 bytes == 0x12aeaa000 @  0x7faf9fcdd680 0x7faf9fcfe824 0x5d93d1 0x634ae1 0x5a23da 0x4c8bce 0x63afe8 0x4db8d3 0x547447 0x4e1a5e 0x54c8a9 0x54552a 0x4e1bd0 0x5483b6 0x54552a 0x684327 0x5e1514 0x5a27d0 0x547265 0x4d71f8 0x548c6b 0x4d71f8 0x548c6b 0x4d71f8 0x4daf8a 0x547447 0x5d5846 0x547265 0x5d5846 0x547447 0x54552a\n",
      "tcmalloc: large alloc 4540514304 bytes == 0x12aeaa000 @  0x7faf9fcdd680 0x7faf9fcfe824 0x5d93d1 0x634ae1 0x5a23da 0x4c8bce 0x63afe8 0x4db8d3 0x547447 0x4e1a5e 0x54c8a9 0x54552a 0x4e1bd0 0x5483b6 0x54552a 0x684327 0x5e1514 0x5a27d0 0x547265 0x4d71f8 0x548c6b 0x4d71f8 0x548c6b 0x4d71f8 0x4daf8a 0x547447 0x5d5846 0x547265 0x5d5846 0x547447 0x54552a\n",
      "Some of the weights of FlaxMistralForCausalLM were initialized in bfloat16 precision from the model checkpoint at ./Mistral-7B-Instruct-v0.2:\n",
      "[('lm_head', 'kernel'), ('model', 'embed_tokens', 'embedding'), ('model', 'layers', '0', 'input_layernorm', 'weight'), ('model', 'layers', '0', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '0', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '0', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '0', 'post_attention_layernorm', 'weight'), ('model', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '1', 'input_layernorm', 'weight'), ('model', 'layers', '1', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '1', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '1', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '1', 'post_attention_layernorm', 'weight'), ('model', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '10', 'input_layernorm', 'weight'), ('model', 'layers', '10', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '10', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '10', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '10', 'post_attention_layernorm', 'weight'), ('model', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '11', 'input_layernorm', 'weight'), ('model', 'layers', '11', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '11', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '11', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '11', 'post_attention_layernorm', 'weight'), ('model', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '12', 'input_layernorm', 'weight'), ('model', 'layers', '12', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '12', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '12', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '12', 'post_attention_layernorm', 'weight'), ('model', 'layers', '12', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '13', 'input_layernorm', 'weight'), ('model', 'layers', '13', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '13', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '13', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '13', 'post_attention_layernorm', 'weight'), ('model', 'layers', '13', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '14', 'input_layernorm', 'weight'), ('model', 'layers', '14', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '14', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '14', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '14', 'post_attention_layernorm', 'weight'), ('model', 'layers', '14', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '15', 'input_layernorm', 'weight'), ('model', 'layers', '15', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '15', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '15', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '15', 'post_attention_layernorm', 'weight'), ('model', 'layers', '15', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '16', 'input_layernorm', 'weight'), ('model', 'layers', '16', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '16', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '16', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '16', 'post_attention_layernorm', 'weight'), ('model', 'layers', '16', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '17', 'input_layernorm', 'weight'), ('model', 'layers', '17', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '17', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '17', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '17', 'post_attention_layernorm', 'weight'), ('model', 'layers', '17', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '18', 'input_layernorm', 'weight'), ('model', 'layers', '18', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '18', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '18', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '18', 'post_attention_layernorm', 'weight'), ('model', 'layers', '18', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '18', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '18', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '18', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '19', 'input_layernorm', 'weight'), ('model', 'layers', '19', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '19', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '19', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '19', 'post_attention_layernorm', 'weight'), ('model', 'layers', '19', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '19', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '19', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '19', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '2', 'input_layernorm', 'weight'), ('model', 'layers', '2', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '2', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '2', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '2', 'post_attention_layernorm', 'weight'), ('model', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '20', 'input_layernorm', 'weight'), ('model', 'layers', '20', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '20', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '20', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '20', 'post_attention_layernorm', 'weight'), ('model', 'layers', '20', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '20', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '20', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '20', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '21', 'input_layernorm', 'weight'), ('model', 'layers', '21', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '21', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '21', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '21', 'post_attention_layernorm', 'weight'), ('model', 'layers', '21', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '21', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '21', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '21', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '22', 'input_layernorm', 'weight'), ('model', 'layers', '22', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '22', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '22', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '22', 'post_attention_layernorm', 'weight'), ('model', 'layers', '22', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '22', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '22', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '22', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '23', 'input_layernorm', 'weight'), ('model', 'layers', '23', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '23', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '23', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '23', 'post_attention_layernorm', 'weight'), ('model', 'layers', '23', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '23', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '23', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '23', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '24', 'input_layernorm', 'weight'), ('model', 'layers', '24', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '24', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '24', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '24', 'post_attention_layernorm', 'weight'), ('model', 'layers', '24', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '24', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '24', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '24', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '25', 'input_layernorm', 'weight'), ('model', 'layers', '25', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '25', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '25', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '25', 'post_attention_layernorm', 'weight'), ('model', 'layers', '25', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '25', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '25', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '25', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '26', 'input_layernorm', 'weight'), ('model', 'layers', '26', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '26', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '26', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '26', 'post_attention_layernorm', 'weight'), ('model', 'layers', '26', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '26', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '26', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '26', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '27', 'input_layernorm', 'weight'), ('model', 'layers', '27', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '27', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '27', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '27', 'post_attention_layernorm', 'weight'), ('model', 'layers', '27', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '27', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '27', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '27', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '28', 'input_layernorm', 'weight'), ('model', 'layers', '28', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '28', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '28', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '28', 'post_attention_layernorm', 'weight'), ('model', 'layers', '28', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '28', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '28', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '28', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '29', 'input_layernorm', 'weight'), ('model', 'layers', '29', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '29', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '29', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '29', 'post_attention_layernorm', 'weight'), ('model', 'layers', '29', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '29', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '29', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '29', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '3', 'input_layernorm', 'weight'), ('model', 'layers', '3', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '3', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '3', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '3', 'post_attention_layernorm', 'weight'), ('model', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '30', 'input_layernorm', 'weight'), ('model', 'layers', '30', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '30', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '30', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '30', 'post_attention_layernorm', 'weight'), ('model', 'layers', '30', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '30', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '30', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '30', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '31', 'input_layernorm', 'weight'), ('model', 'layers', '31', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '31', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '31', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '31', 'post_attention_layernorm', 'weight'), ('model', 'layers', '31', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '31', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '31', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '31', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '4', 'input_layernorm', 'weight'), ('model', 'layers', '4', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '4', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '4', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '4', 'post_attention_layernorm', 'weight'), ('model', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '5', 'input_layernorm', 'weight'), ('model', 'layers', '5', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '5', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '5', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '5', 'post_attention_layernorm', 'weight'), ('model', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '6', 'input_layernorm', 'weight'), ('model', 'layers', '6', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '6', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '6', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '6', 'post_attention_layernorm', 'weight'), ('model', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '7', 'input_layernorm', 'weight'), ('model', 'layers', '7', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '7', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '7', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '7', 'post_attention_layernorm', 'weight'), ('model', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '8', 'input_layernorm', 'weight'), ('model', 'layers', '8', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '8', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '8', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '8', 'post_attention_layernorm', 'weight'), ('model', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '9', 'input_layernorm', 'weight'), ('model', 'layers', '9', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '9', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '9', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '9', 'post_attention_layernorm', 'weight'), ('model', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('model', 'norm', 'weight')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model, params = FlaxMistralForCausalLM.from_pretrained(model_id, _do_init=False, dtype=jnp.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feee92bb-493d-4716-83f4-22fedc293c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e2fc3f-dcf1-4b20-9bff-4fb423f70ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 4 * [\"The capital of France is\"]\n",
    "inputs = tokenizer(input_text, return_tensors=\"np\", return_attention_mask=True, padding=\"max_length\", max_length=max_input_tokens)\n",
    "\n",
    "params = jax_utils.replicate(params)\n",
    "inputs = shard(inputs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c20c2f-0535-4ed5-8d04-b6b71ef305d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [{\"role\": \"user\", \"content\": \"The capital of France is\"}]\n",
    "input_text = tokenizer.apply_chat_template(input_text, tokenize=False)\n",
    "inputs = tokenizer(4 * [input_text], return_tensors=\"np\", return_attention_mask=True, padding=\"max_length\", max_length=max_input_tokens)\n",
    "inputs = shard(inputs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42446a96-eef7-4a74-aea4-15d2be0b6591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sanchitgandhi/hf/lib/python3.8/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=bfloat16 to dtype=float32. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "/home/sanchitgandhi/hf/lib/python3.8/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=bfloat16 to dtype=float32. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n"
     ]
    }
   ],
   "source": [
    "def generate(inputs, params, max_new_tokens):\n",
    "    generated_ids = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], params=params, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "    return generated_ids.sequences\n",
    "\n",
    "p_generate = jax.pmap(generate, \"inputs\", in_axes=(0, 0, None,), out_axes=0, static_broadcasted_argnums=(2,))\n",
    "gen_ids = p_generate(inputs, params, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7ad674-6b4e-460b-9cc5-9d13231f4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tok_per_s(input_ids, generated_ids, runtime):\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    input_ids = input_ids[input_ids != tokenizer.pad_token_id]\n",
    "    total_inputs = np.prod(input_ids.shape)\n",
    "    \n",
    "    generated_ids = np.asarray(generated_ids)\n",
    "    generated_ids = generated_ids[generated_ids != tokenizer.pad_token_id]\n",
    "    total_outputs = np.prod(generated_ids.shape)\n",
    "    \n",
    "    tokens_generated = total_outputs - total_inputs\n",
    "    tokens_per_s = tokens_generated / runtime\n",
    "    return tokens_per_s\n",
    "\n",
    "def chat_function(message, chat_history):\n",
    "    conversation = []\n",
    "    for user, assistant in chat_history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"np\")\n",
    "    # in-case our inputs exceed the maximum length, we might need to cut them\n",
    "    if input_ids.shape[1] > max_input_tokens:\n",
    "        input_ids = input_ids[:, -max_input_tokens:]\n",
    "        gr.Warning(f\"Trimmed input from conversation as it was longer than {max_input_tokens} tokens.\")\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    inputs = tokenizer(4 * [input_text], return_tensors=\"np\", return_attention_mask=True, padding=\"max_length\", max_length=max_input_tokens)\n",
    "    inputs = shard(inputs.data)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    start = time.time()\n",
    "    pred_ids = p_generate(inputs, params, max_new_tokens)\n",
    "    runtime = time.time() - start\n",
    "    \n",
    "    pred_ids = jax.device_get(pred_ids.reshape(-1, pred_ids.shape[-1]))\n",
    "    pred_text = tokenizer.decode(np.array(pred_ids[0])[input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    tok_per_s = compute_tok_per_s(input_ids, pred_ids, runtime)\n",
    "    gr.Info(f\"Tok/s: {round(tok_per_s, 2)}\")\n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84ad1d6-4200-45d8-b861-a838f6dbf35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://29f71c69eb6de75b87.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://29f71c69eb6de75b87.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_interface = gr.ChatInterface(chat_function)\n",
    "chat_interface.queue().launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6727f13-ac11-48fa-8817-339821af8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_function():\n",
    "    conversation = []\n",
    "    while True:\n",
    "        message = input()\n",
    "        conversation.append({\"role\": \"user\", \"content\": message})\n",
    "        #input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"np\")\n",
    "        # in-case our inputs exceed the maximum length, we might need to cut them\n",
    "        #if input_ids.shape[1] > max_input_tokens:\n",
    "        #    input_ids = input_ids[:, -max_input_tokens:]\n",
    "        #    gr.Warning(f\"Trimmed input from conversation as it was longer than {max_input_tokens} tokens.\")\n",
    "    \n",
    "        input_text = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "        inputs = tokenizer(4 * [input_text], return_tensors=\"np\", return_attention_mask=True, padding=\"max_length\", max_length=max_input_tokens)\n",
    "        inputs = shard(inputs.data)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "        start = time.time()\n",
    "        pred_ids = p_generate(inputs, params, max_new_tokens)\n",
    "        runtime = time.time() - start\n",
    "        \n",
    "        pred_ids = jax.device_get(pred_ids.reshape(-1, pred_ids.shape[-1]))\n",
    "        pred_text = tokenizer.decode(np.array(pred_ids[0])[input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        # tok_per_s = compute_tok_per_s(input_ids, pred_ids, runtime)\n",
    "        # print(f\"Tok/s: {round(tok_per_s, 2)}\")\n",
    "        print(\"Response:\", pred_text)\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": pred_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d5396f-e282-4c34-8e3e-05e0406445b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hello!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello! How can I help you today? If you have any questions or need assistance with a particular topic, feel free to ask! I'll do my best to provide you with accurate and helpful information. Additionally, if you'd just like to chat or share some thoughts, I'd be happy to listen! So, how can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Sounds great\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I'm glad you think so! I'm here to help answer any questions you might have, or to provide assistance with any topics you'd like to explore. Additionally, if you'd just like to chat or share some thoughts, I'd be happy to listen! So, how can I help you today? Let me know if you have any specific questions or topics in mind, and I'll do my best to provide you with accurate and helpful information! If you don't have any specific questions or topics in mind, that's perfectly fine too! Feel free to share any thoughts or ideas you might have, or just chat with me about whatever topic you'd like! I'm here to help and engage with you in a positive and productive way! So, how can I help you today? Let me know if you have any specific questions or topics in mind, and I'll do my best to provide you with accurate and helpful information! If you don't have any specific questions or topics in mind, that's perfectly fine too! Feel free to share any thoughts or ideas you might have, or just chat with me about whatever topic you'd like! I'm here to help and engage with you in\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What is the hottest country in the world?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/sanchitgandhi/hf/lib/python3.8/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=bfloat16 to dtype=float32. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c830025-c2bb-4fc9-8185-e96110f0b6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
